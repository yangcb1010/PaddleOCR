var __index = {"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\: )\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"en/index.html","title":"Home","text":""},{"location":"en/index.html#introduction","title":"Introduction","text":"<p>PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice.</p>"},{"location":"en/index.html#community","title":"\ud83d\ude80 Community","text":"<p>PaddleOCR is being oversight by a PMC. Issues and PRs will be reviewed on a best-effort basis. For a complete overview of PaddlePaddle community, please visit community.</p> <p>\u26a0\ufe0f Note: The Issues module is only for reporting program \ud83d\udc1e bugs, for the rest of the questions, please move to the Discussions. Please note that if the Issue mentioned is not a bug, it will be moved to the Discussions module.</p>"},{"location":"en/index.html#recent-updates","title":"\ud83d\udce3 Recent updates","text":"<ul> <li> <p>\ud83d\udd252023.8.7 Release PaddleOCRrelease/2.7</p> <ul> <li> <p>Release PP-OCRv4, support mobile version and server version</p> <ul> <li>PP-OCRv4-mobile\uff1aWhen the speed is comparable, the effect of the Chinese scene is improved by 4.5% compared with PP-OCRv3, the English scene is improved by 10%, and the average recognition accuracy of the 80-language multilingual model is increased by more than 8%.</li> <li>PP-OCRv4-server\uff1aRelease the OCR model with the highest accuracy at present, the detection model accuracy increased by 4.9% in the Chinese and English scenes, and the recognition model accuracy increased by 2% refer quickstart quick use by one line command, At the same time, the whole process of model training, reasoning, and high-performance deployment can also be completed with few code in the General OCR Industry Solution in PaddleX.</li> </ul> </li> <li> <p>ReleasePP-ChatOCR, a new scheme for extracting key information of general scenes using PP-OCR model and ERNIE LLM.</p> </li> </ul> </li> <li> <p>\ud83d\udd282022.11 Add implementation of 4 cutting-edge algorithms\uff1aText Detection DRRG,  Text Recognition RFL, Image Super-Resolution Text Telescope\uff0cHandwritten Mathematical Expression Recognition CAN</p> </li> <li> <p>2022.10 release optimized JS version PP-OCRv3 model with 4.3M model size, 8x faster inference time, and a ready-to-use web demo</p> <ul> <li>\ud83d\udca5 Live Playback: Introduction to PP-StructureV2 optimization strategy. Scan the QR code below using WeChat, follow the PaddlePaddle official account and fill out the questionnaire to join the WeChat group, get the live link and 20G OCR learning materials (including PDF2Word application, 10 models in vertical scenarios, etc.)</li> </ul> </li> <li> <p>\ud83d\udd252022.8.24 Release PaddleOCR release/2.6</p> <ul> <li>Release PP-StructureV2\uff0cwith functions and performance fully upgraded, adapted to Chinese scenes, and new support for Layout Recovery and one line command to convert PDF to Word;</li> <li>Layout Analysis optimization: model storage reduced by 95%, while speed increased by 11 times, and the average CPU time-cost is only 41ms;</li> <li>Table Recognition optimization: 3 optimization strategies are designed, and the model accuracy is improved by 6% under comparable time consumption;</li> <li>Key Information Extraction optimization\uff1aa visual-independent model structure is designed, the accuracy of semantic entity recognition is increased by 2.8%, and the accuracy of relation extraction is increased by 9.1%.</li> </ul> </li> <li> <p>\ud83d\udd252022.8 Release OCR scene application collection</p> <ul> <li>Release 9 vertical models such as digital tube, LCD screen, license plate, handwriting recognition model, high-precision SVTR model, etc, covering the main OCR vertical applications in general, manufacturing, finance, and transportation industries.</li> </ul> </li> <li> <p>2022.8 Add implementation of 8 cutting-edge algorithms</p> <ul> <li>Text Detection: FCENet, DB++</li> <li>Text Recognition: ViTSTR, ABINet, VisionLAN, SPIN, RobustScanner</li> <li>Table Recognition: TableMaster</li> <li> <p>2022.5.9 Release PaddleOCR release/2.5</p> </li> <li> <p>Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.</p> </li> <li>Release PPOCRLabelv2: Add the annotation function for table recognition task, key information extraction task and irregular text image.</li> <li>Release interactive e-book \"Dive into OCR\", covers the cutting-edge theory and code practice of OCR full stack technology.</li> <li>more</li> </ul> </li> </ul>"},{"location":"en/index.html#features","title":"\ud83c\udf1f Features","text":"<p>PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution PP-OCR\u3001PP-Structure and PP-ChatOCR on this basis, and get through the whole process of data production, model training, compression, inference and deployment.</p> <p></p> <p>It is recommended to start with the \u201cquick experience\u201d in the document tutorial</p>"},{"location":"en/index.html#technical-exchange-and-cooperation","title":"\ud83d\udcd6 Technical exchange and cooperation","text":"<ul> <li> <p>PaddleX provides a one-stop full-process high-efficiency development platform for flying paddle ecological model training, pressure, and push. Its mission is to help AI technology quickly land, and its vision is to make everyone an AI Developer!</p> <ul> <li>PaddleX currently covers areas such as image classification, object detection, image segmentation, 3D, OCR, and time series prediction, and has built-in 36 basic single models, such as RP-DETR, PP-YOLOE, PP-HGNet, PP-LCNet, PP- LiteSeg, etc.; integrated 12 practical industrial solutions, such as PP-OCRv4, PP-ChatOCR, PP-ShiTu, PP-TS, vehicle-mounted road waste detection, identification of prohibited wildlife products, etc.</li> <li>PaddleX provides two AI development modes: \"Toolbox\" and \"Developer\". The toolbox mode can tune key hyperparameters without code, and the developer mode can perform single-model training, push and multi-model serial inference with low code, and supports both cloud and local terminals.</li> <li>PaddleX also supports joint innovation and development, profit sharing! At present, PaddleX is rapidly iterating, and welcomes the participation of individual developers and enterprise developers to create a prosperous AI technology ecosystem!</li> </ul> </li> </ul>"},{"location":"en/index.html#guideline-for-new-language-requests","title":"\ud83c\uddfa\ud83c\uddf3 Guideline for New Language Requests","text":"<p>If you want to request a new language support, a PR with 1 following files are needed\uff1a</p> <ul> <li>In folder ppocr/utils/dict, it is necessary to submit the dict text to this path and name it with <code>{language}_dict.txt</code> that contains a list of all characters. Please see the format example from other files in that folder.</li> </ul> <p>If your language has unique elements, please tell me in advance within any way, such as useful links, wikipedia and so on.</p> <p>More details, please refer to Multilingual OCR Development Plan.</p>"},{"location":"en/index.html#visualization","title":"Visualization","text":""},{"location":"en/index.html#pp-ocrv3","title":"PP-OCRv3","text":""},{"location":"en/index.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/index.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/index.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/index.html#pp-structurev2","title":"PP-StructureV2","text":"<ul> <li>layout analysis + table recognition</li> </ul> <ul> <li>SER (Semantic entity recognition)</li> </ul> <ul> <li>RE (Relation Extraction)</li> </ul>"},{"location":"en/index.html#license","title":"\ud83d\udcc4 License","text":"<p>This project is released under Apache 2.0 license</p>"},{"location":"en/FAQ.html","title":"FAQ","text":"<ol> <li> <p>Prediction error: got an unexpected keyword argument 'gradient_clip' The installed version of paddle is incorrect. Currently, this project only supports Paddle 1.7, which will be adapted to 1.8 in the near future.</p> </li> <li> <p>Error when converting attention recognition model: KeyError: 'predict' Solved. Please update to the latest version of the code.</p> </li> <li> <p>About inference speed When there are many words in the picture, the prediction time will increase. You can use <code>--rec_batch_num</code> to set a smaller prediction batch num. The default value is 30, which can be changed to 10 or other values.</p> </li> <li> <p>Service deployment and mobile deployment It is expected that the service deployment based on Serving and the mobile deployment based on Paddle Lite will be released successively in mid-to-late June. Stay tuned for more updates.</p> </li> <li> <p>Release time of self-developed algorithm Baidu Self-developed algorithms such as SAST, SRN and end2end PSL will be released in June or July. Please be patient.</p> </li> <li> <p>How to run on Windows or Mac? PaddleOCR has completed the adaptation to Windows and MAC systems. Two points should be noted during operation:</p> <ol> <li>In Quick installation, if you do not want to install docker, you can skip the first step and start with the second step.</li> <li>When downloading the inference model, if wget is not installed, you can directly click the model link or copy the link address to the browser to download, then extract and place it in the corresponding directory.</li> </ol> </li> <li> <p>The difference between ultra-lightweight model and General OCR model At present, PaddleOCR has opensourced two Chinese models, namely 8.6M ultra-lightweight Chinese model and general Chinese OCR model. The comparison information between the two is as follows:</p> <ul> <li>Similarities: Both use the same algorithm and training data\uff1b</li> <li>Differences: The difference lies in backbone network and channel parameters, the ultra-lightweight model uses MobileNetV3 as the backbone network, the general model uses Resnet50_vd as the detection model backbone, and Resnet34_vd as the recognition model backbone. You can compare the two model training configuration files to see the differences in parameters.</li> </ul> </li> </ol> Model Backbone Detection configuration file Recognition configuration file 8.6M ultra-lightweight Chinese OCR model MobileNetV3+MobileNetV3 det_mv3_db.yml rec_chinese_lite_train.yml General Chinese OCR model Resnet50_vd+Resnet34_vd det_r50_vd_db.yml rec_chinese_common_train.yml <ol> <li> <p>Is there a plan to opensource a model that only recognizes numbers or only English + numbers? It is not planned to opensource numbers only, numbers + English only, or other vertical text models. PaddleOCR has opensourced a variety of detection and recognition algorithms for customized training. The two Chinese models are also based on the training output of the open-source algorithm library. You can prepare the data according to the tutorial, choose the appropriate configuration file, train yourselves, and we believe that you can get good result. If you have any questions during the training, you are welcome to open issues or ask in the communication group. We will answer them in time.</p> </li> <li> <p>What is the training data used by the open-source model? Can it be opensourced? At present, the open source model, dataset and magnitude are as follows:</p> <ul> <li>Detection: English dataset: ICDAR2015 Chinese dataset: LSVT street view dataset with 3w pictures</li> <li>Recognition: English dataset: MJSynth and SynthText synthetic dataset, the amount of data is tens of millions. Chinese dataset: LSVT street view dataset with cropped text area, a total of 30w images. In addition, the synthesized data based on LSVT corpus is 500w.</li> </ul> <p>Among them, the public datasets are opensourced, users can search and download by themselves, or refer to Chinese data set, synthetic data is not opensourced, users can use open-source synthesis tools to synthesize data themselves. Current available synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator, etc.</p> </li> <li> <p>Error in using the model with TPS module for prediction Error message: Input(X) dims[3] and Input(Grid) dims[2] should be equal, but received X dimension[3](108) != Grid dimension[2](100) Solution: TPS does not support variable shape. Please set --rec_image_shape='3,32,100' and --rec_char_type='en'</p> </li> <li> <p>Custom dictionary used during training, the recognition results show that words do not appear in the dictionary The used custom dictionary path is not set when making prediction. The solution is setting parameter <code>rec_char_dict_path</code> to the corresponding dictionary file.</p> </li> <li> <p>Results of cpp_infer and python_inference are very different Versions of exported inference model and inference library should be same. For example, on Windows platform, version of the inference library that PaddlePaddle provides is 1.8, but version of the inference model that PaddleOCR provides is 1.7, you should export model yourself(<code>tools/export_model.py</code>) on PaddlePaddle 1.8 and then use the exported model for inference.</p> </li> <li> <p>How to identify artistic fonts in signs or advertising images Recognizing artistic fonts in signs or advertising images is a very challenging task because the variation in individual characters is much greater compared to standard fonts. If the artistic font to be identified is within a dictionary list, each word in the dictionary can be treated as a template for recognition using a general image retrieval system. You can try using PaddleClas image recognition system.</p> </li> </ol>"},{"location":"en/quick_start.html","title":"Quick Start","text":"<ul> <li>Web online experience<ul> <li>PP-OCRv4 online experience\uff1ahttps://aistudio.baidu.com/aistudio/projectdetail/6611435</li> <li>PP-ChatOCR online experience\uff1ahttps://aistudio.baidu.com/aistudio/projectdetail/6488689</li> </ul> </li> <li>One line of code quick use: Quick Start\uff08Chinese/English/Multilingual/Document Analysis</li> <li>Full-process experience of training, inference, and high-performance deployment in the Paddle AI suite (PaddleX)\uff1a<ul> <li>PP-OCRv4\uff1ahttps://aistudio.baidu.com/aistudio/modelsdetail?modelId=286</li> <li>PP-ChatOCR\uff1ahttps://aistudio.baidu.com/aistudio/modelsdetail?modelId=332</li> </ul> </li> </ul>"},{"location":"en/update.html","title":"Recently Update","text":""},{"location":"en/update.html#recently-update","title":"Recently Update","text":""},{"location":"en/update.html#202259-release-paddleocr-v25-including","title":"2022.5.9 release PaddleOCR v2.5, including","text":"<ul> <li>PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.</li> <li>PPOCRLabelv2: Add the annotation function for table recognition task, key information extraction task and irregular text image.</li> <li>Interactive e-book \"Dive into OCR\", covers the cutting-edge theory and code practice of OCR full stack technology.</li> </ul>"},{"location":"en/update.html#202257-add-support-for-metric-and-model-logging-during-training-to-weights-biases","title":"2022.5.7 Add support for metric and model logging during training to Weights &amp; Biases","text":""},{"location":"en/update.html#20211221-ocr-open-source-online-course-starts-the-lesson-starts-at-830-every-night-and-lasts-for-ten-days-free-registration-httpsaistudiobaiducomaistudiocourseintroduce25207","title":"2021.12.21 OCR open source online course starts. The lesson starts at 8:30 every night and lasts for ten days. Free registration: https://aistudio.baidu.com/aistudio/course/introduce/25207","text":""},{"location":"en/update.html#20211221-release-paddleocr-v24-release-1-text-detection-algorithm-psenet-3-text-recognition-algorithms-nrtrseedsar-1-key-information-extraction-algorithm-sdmgr-and-3-docvqa-algorithms-layoutlmlayoutlmv2layoutxlm","title":"2021.12.21 release PaddleOCR v2.4, release 1 text detection algorithm (PSENet), 3 text recognition algorithms (NRTR\u3001SEED\u3001SAR), 1 key information extraction algorithm (SDMGR) and 3 DocVQA algorithms (LayoutLM\u3001LayoutLMv2\uff0cLayoutXLM)","text":""},{"location":"en/update.html#202197-release-paddleocr-v23-pp-ocrv2-is-proposed-the-cpu-inference-speed-of-pp-ocrv2-is-220-higher-than-that-of-pp-ocr-server-the-f-score-of-pp-ocrv2-is-7-higher-than-that-of-pp-ocr-mobile","title":"2021.9.7 release PaddleOCR v2.3, PP-OCRv2 is proposed. The CPU inference speed of PP-OCRv2 is 220% higher than that of PP-OCR server. The F-score of PP-OCRv2 is 7% higher than that of PP-OCR mobile","text":""},{"location":"en/update.html#202183-released-paddleocr-v22-add-a-new-structured-documents-analysis-toolkit-ie-pp-structure-support-layout-analysis-and-table-recognition-one-key-to-export-chart-images-to-excel-files","title":"2021.8.3 released PaddleOCR v2.2, add a new structured documents analysis toolkit, i.e., PP-Structure, support layout analysis and table recognition (One-key to export chart images to Excel files)","text":""},{"location":"en/update.html#202148-release-end-to-end-text-recognition-algorithm-pgnet-which-is-published-in-aaai-2021-find-tutorial-hererelease-multi-language-recognition-models-support-more-than-80-languages-recognition-especically-the-performance-of-english-recognition-model-is-optimized","title":"2021.4.8 release end-to-end text recognition algorithm PGNet which is published in AAAI 2021. Find tutorial here\uff1brelease multi language recognition models, support more than 80 languages recognition; especically, the performance of English recognition model is Optimized","text":""},{"location":"en/update.html#2021121-update-more-than-25-multilingual-recognition-models-models-list-includingenglish-chinese-german-french-japanesespanishportuguese-russia-arabic-and-so-on-models-for-more-languages-will-continue-to-be-updated-develop-plan","title":"2021.1.21 update more than 25+ multilingual recognition models models list, including\uff1aEnglish, Chinese, German, French, Japanese\uff0cSpanish\uff0cPortuguese Russia Arabic and so on.  Models for more languages will continue to be updated Develop Plan","text":""},{"location":"en/update.html#20201215-update-data-synthesis-tool-ie-style-texteasy-to-synthesize-a-large-number-of-images-which-are-similar-to-the-target-scene-image","title":"2020.12.15 update Data synthesis tool, i.e., Style-Text\uff0ceasy to synthesize a large number of images which are similar to the target scene image","text":""},{"location":"en/update.html#20201125-update-a-new-data-annotation-tool-ie-ppocrlabel-which-is-helpful-to-improve-the-labeling-efficiency-moreover-the-labeling-results-can-be-used-in-training-of-the-pp-ocr-system-directly","title":"2020.11.25 Update a new data annotation tool, i.e., PPOCRLabel, which is helpful to improve the labeling efficiency. Moreover, the labeling results can be used in training of the PP-OCR system directly","text":""},{"location":"en/update.html#2020922-update-the-pp-ocr-technical-article-httpsarxivorgabs200909941","title":"2020.9.22 Update the PP-OCR technical article, https://arxiv.org/abs/2009.09941","text":""},{"location":"en/update.html#2020919-update-the-ultra-lightweight-compressed-ppocr_mobile_slim-series-models-the-overall-model-size-is-35m-suitable-for-mobile-deployment","title":"2020.9.19 Update the ultra lightweight compressed ppocr_mobile_slim series models, the overall model size is 3.5M, suitable for mobile deployment","text":""},{"location":"en/update.html#2020917-update-english-recognition-model-and-multilingual-recognition-model-english-chinese-german-french-japanese-and-korean-have-been-supported-models-for-more-languages-will-continue-to-be-updated","title":"2020.9.17 update English recognition model and Multilingual recognition model, <code>English</code>, <code>Chinese</code>, <code>German</code>, <code>French</code>, <code>Japanese</code> and <code>Korean</code> have been supported. Models for more languages will continue to be updated","text":""},{"location":"en/update.html#2020824-support-the-use-of-paddleocr-through-whl-package-installationpelease-refer-paddleocr-package","title":"2020.8.24 Support the use of PaddleOCR through whl package installation\uff0cpelease refer  PaddleOCR Package","text":""},{"location":"en/update.html#2020816-release-text-detection-algorithm-sast-and-text-recognition-algorithm-srn","title":"2020.8.16 Release text detection algorithm SAST and text recognition algorithm SRN","text":""},{"location":"en/update.html#2020723-release-the-playback-and-ppt-of-live-class-on-bilibili-station-paddleocr-introduction-address","title":"2020.7.23, Release the playback and PPT of live class on BiliBili station, PaddleOCR Introduction, address","text":""},{"location":"en/update.html#2020715-add-mobile-app-demo-support-both-ios-and-android-based-on-easyedge-and-paddle-lite","title":"2020.7.15, Add mobile App demo , support both iOS and  Android  ( based on easyedge and Paddle Lite)","text":""},{"location":"en/update.html#2020715-improve-the-deployment-ability-add-the-c-inference-serving-deployment-in-addtion-the-benchmarks-of-the-ultra-lightweight-chinese-ocr-model-are-provided","title":"2020.7.15, Improve the  deployment ability, add the C + +  inference , serving deployment. In addtion, the benchmarks of the ultra-lightweight Chinese OCR model are provided","text":""},{"location":"en/update.html#2020715-add-several-related-datasets-data-annotation-and-synthesis-tools","title":"2020.7.15, Add several related datasets, data annotation and synthesis tools","text":""},{"location":"en/update.html#202079-add-a-new-model-to-support-recognize-the-character-space","title":"2020.7.9 Add a new model to support recognize the  character \"space\"","text":""},{"location":"en/update.html#202079-add-the-data-augument-and-learning-rate-decay-strategies-during-training","title":"2020.7.9 Add the data augument and learning rate decay strategies during training","text":""},{"location":"en/update.html#202068-add-datasets-and-keep-updating","title":"2020.6.8 Add datasets and keep updating","text":""},{"location":"en/update.html#202065-support-exporting-attention-model-to-inference_model","title":"2020.6.5 Support exporting <code>attention</code> model to <code>inference_model</code>","text":""},{"location":"en/update.html#202065-support-separate-prediction-and-recognition-output-result-score","title":"2020.6.5 Support separate prediction and recognition, output result score","text":""},{"location":"en/update.html#2020530-provide-lightweight-chinese-ocr-online-experience","title":"2020.5.30 Provide Lightweight Chinese OCR online experience","text":""},{"location":"en/update.html#2020530-model-prediction-and-training-support-on-windows-system","title":"2020.5.30 Model prediction and training support on Windows system","text":""},{"location":"en/update.html#2020530-open-source-general-chinese-ocr-model","title":"2020.5.30 Open source general Chinese OCR model","text":""},{"location":"en/update.html#2020514-release-paddleocr-open-class","title":"2020.5.14 Release PaddleOCR Open Class","text":""},{"location":"en/update.html#2020514-release-paddleocr-practice-notebook","title":"2020.5.14 Release PaddleOCR Practice Notebook","text":""},{"location":"en/update.html#2020514-open-source-86m-lightweight-chinese-ocr-model","title":"2020.5.14 Open source 8.6M lightweight Chinese OCR model","text":""},{"location":"en/algorithm/add_new_algorithm.html","title":"Add New Algorithm","text":"<p>PaddleOCR decomposes an algorithm into the following parts, and modularizes each part to make it more convenient to develop new algorithms.</p> <ul> <li>Data loading and processing</li> <li>Network</li> <li>Post-processing</li> <li>Loss</li> <li>Metric</li> <li>Optimizer</li> </ul> <p>The following will introduce each part separately, and introduce how to add the modules required for the new algorithm.</p>"},{"location":"en/algorithm/add_new_algorithm.html#data-loading-and-processing","title":"Data loading and processing","text":"<p>Data loading and processing are composed of different modules, which complete the image reading, data augment and label production. This part is under ppocr/data. The explanation of each file and folder are as follows:</p> <pre><code>ppocr/data/\n\u251c\u2500\u2500 imaug             # Scripts for image reading, data augment and label production\n\u2502   \u251c\u2500\u2500 label_ops.py  # Modules that transform the label\n\u2502   \u251c\u2500\u2500 operators.py  # Modules that transform the image\n\u2502   \u251c\u2500\u2500.....\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 lmdb_dataset.py   # The dataset that reads the lmdb\n\u2514\u2500\u2500 simple_dataset.py # Read the dataset saved in the form of `image_path\\tgt`\n</code></pre> <p>PaddleOCR has a large number of built-in image operation related modules. For modules that are not built-in, you can add them through the following steps:</p> <ol> <li>Create a new file under the ppocr/data/imaug folder, such as my_module.py.</li> <li> <p>Add code in the my_module.py file, the sample code is as follows:</p> <pre><code>class MyModule:\n    def __init__(self, *args, **kwargs):\n        # your init code\n        pass\n\n    def __call__(self, data):\n        img = data['image']\n        label = data['label']\n        # your process code\n\n        data['image'] = img\n        data['label'] = label\n        return data\n</code></pre> </li> <li> <p>Import the added module in the ppocr/data/imaug/_init_.py file.</p> </li> </ol> <p>All different modules of data processing are executed by sequence, combined and executed in the form of a list in the config file. Such as:</p> <pre><code># angle class data process\ntransforms:\n  - DecodeImage: # load image\n      img_mode: BGR\n      channel_first: False\n  - MyModule:\n      args1: args1\n      args2: args2\n  - KeepKeys:\n      keep_keys: [ 'image', 'label' ] # dataloader will return list in this order\n</code></pre>"},{"location":"en/algorithm/add_new_algorithm.html#network","title":"Network","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>PaddleOCR has built-in commonly used modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For modules that do not have built-in, you can add them through the following steps, the four parts are added in the same steps, take backbones as an example:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li> <p>Add code in the my_backbone.py file, the sample code is as follows:</p> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> </li> <li> <p>Import the added module in the ppocr/modeling/backbones/_init_.py file.</p> </li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>Architecture:\nmodel_type: rec\nalgorithm: CRNN\nTransform:\n    name: MyTransform\n    args1: args1\n    args2: args2\nBackbone:\n    name: MyBackbone\n    args1: args1\nNeck:\n    name: MyNeck\n    args1: args1\nHead:\n    name: MyHead\n    args1: args1\n</code></pre>"},{"location":"en/algorithm/add_new_algorithm.html#post-processing","title":"Post-processing","text":"<p>Post-processing realizes decoding network output to obtain text box or recognized text. This part is under ppocr/postprocess. PaddleOCR has built-in post-processing modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For components that are not built-in, they can be added through the following steps:</p> <ol> <li>Create a new file under the ppocr/postprocess folder, such as my_postprocess.py.</li> <li> <p>Add code in the my_postprocess.py file, the sample code is as follows:</p> <pre><code>import paddle\n\n\nclass MyPostProcess:\n    def __init__(self, *args, **kwargs):\n        # your init code\n        pass\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        # you preds decode code\n        preds = self.decode_preds(preds)\n        if label is None:\n            return preds\n        # you label decode code\n        label = self.decode_label(label)\n        return preds, label\n\n    def decode_preds(self, preds):\n        # you preds decode code\n        pass\n\n    def decode_label(self, preds):\n        # you label decode code\n        pass\n</code></pre> </li> <li> <p>Import the added module in the ppocr/postprocess/_init_.py file.</p> </li> </ol> <p>After the post-processing module is added, you only need to configure it in the configuration file to use, such as:</p> <pre><code>PostProcess:\nname: MyPostProcess\nargs1: args1\nargs2: args2\n</code></pre>"},{"location":"en/algorithm/add_new_algorithm.html#loss","title":"Loss","text":"<p>The loss function is used to calculate the distance between the network output and the label. This part is under ppocr/losses. PaddleOCR has built-in loss function modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For modules that do not have built-in modules, you can add them through the following steps:</p> <ol> <li>Create a new file in the ppocr/losses folder, such as my_loss.py.</li> <li> <p>Add code in the my_loss.py file, the sample code is as follows:</p> <pre><code>import paddle\nfrom paddle import nn\n\n\nclass MyLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(MyLoss, self).__init__()\n        # you init code\n        pass\n\n    def __call__(self, predicts, batch):\n        label = batch[1]\n        # your loss code\n        loss = self.loss(input=predicts, label=label)\n        return {'loss': loss}\n</code></pre> </li> <li> <p>Import the added module in the ppocr/losses/_init_.py file.</p> </li> </ol> <p>After the loss function module is added, you only need to configure it in the configuration file to use it, such as:</p> <pre><code>Loss:\n  name: MyLoss\n  args1: args1\n  args2: args2\n</code></pre>"},{"location":"en/algorithm/add_new_algorithm.html#metric","title":"Metric","text":"<p>Metric is used to calculate the performance of the network on the current batch. This part is under ppocr/metrics. PaddleOCR has built-in evaluation modules related to algorithms such as detection, classification and recognition. For modules that do not have built-in modules, you can add them through the following steps:</p> <ol> <li>Create a new file under the ppocr/metrics folder, such as my_metric.py.</li> <li> <p>Add code in the my_metric.py file, the sample code is as follows:</p> <pre><code>class MyMetric(object):\n    def __init__(self, main_indicator='acc', **kwargs):\n        # main_indicator is used for select best model\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, *args, **kwargs):\n        # preds is out of postprocess\n        # batch is out of dataloader\n        labels = batch[1]\n        cur_correct_num = 0\n        cur_all_num = 0\n        # you metric code\n        self.correct_num += cur_correct_num\n        self.all_num += cur_all_num\n        return {'acc': cur_correct_num / cur_all_num, }\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                'acc': 0,\n                'norm_edit_dis': 0,\n            }\n        \"\"\"\n        acc = self.correct_num / self.all_num\n        self.reset()\n        return {'acc': acc}\n\n    def reset(self):\n        # reset metric\n        self.correct_num = 0\n        self.all_num = 0\n</code></pre> </li> <li> <p>Import the added module in the ppocr/metrics/_init_.py file.</p> </li> </ol> <p>After the metric module is added, you only need to configure it in the configuration file to use it, such as:</p> <pre><code>Metric:\n  name: MyMetric\n  main_indicator: acc\n</code></pre>"},{"location":"en/algorithm/add_new_algorithm.html#optimizer","title":"Optimizer","text":"<p>The optimizer is used to train the network. The optimizer also contains network regularization and learning rate decay modules. This part is under ppocr/optimizer. PaddleOCR has built-in Commonly used optimizer modules such as <code>Momentum</code>, <code>Adam</code> and <code>RMSProp</code>, common regularization modules such as <code>Linear</code>, <code>Cosine</code>, <code>Step</code> and <code>Piecewise</code>, and common learning rate decay modules such as <code>L1Decay</code> and <code>L2Decay</code>. Modules without built-in can be added through the following steps, take <code>optimizer</code> as an example:</p> <ol> <li> <p>Create your own optimizer in the ppocr/optimizer/optimizer.py file, the sample code is as follows:</p> <pre><code>from paddle import optimizer as optim\n\n\nclass MyOptim(object):\n    def __init__(self, learning_rate=0.001, *args, **kwargs):\n        self.learning_rate = learning_rate\n\n    def __call__(self, parameters):\n        # It is recommended to wrap the built-in optimizer of paddle\n        opt = optim.XXX(\n            learning_rate=self.learning_rate,\n            parameters=parameters)\n        return opt\n</code></pre> </li> </ol> <p>After the optimizer module is added, you only need to configure it in the configuration file to use, such as:</p> <pre><code>Optimizer:\n  name: MyOptim\n  args1: args1\n  args2: args2\n  lr:\n    name: Cosine\n    learning_rate: 0.001\n  regularizer:\n    name: 'L2'\n    factor: 0\n</code></pre>"},{"location":"en/algorithm/overview.html","title":"Algorithms","text":"<p>This tutorial lists the OCR algorithms supported by PaddleOCR, as well as the models and metrics of each algorithm on English public datasets. It is mainly used for algorithm introduction and algorithm performance comparison. For more models on other datasets including Chinese, please refer to PP-OCRv3 models list.</p> <p>Developers are welcome to contribute more algorithms! Please refer to add new algorithm guideline.</p>"},{"location":"en/algorithm/overview.html#1-two-stage-ocr-algorithms","title":"1. Two-stage OCR Algorithms","text":""},{"location":"en/algorithm/overview.html#11-text-detection-algorithms","title":"1.1 Text Detection Algorithms","text":"<p>Supported text detection algorithms (Click the link to get the tutorial):</p> <ul> <li> DB &amp;&amp; DB++</li> <li> EAST</li> <li> SAST</li> <li> PSENet</li> <li> FCENet</li> <li> DRRG</li> <li> CT</li> </ul> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link EAST ResNet50_vd 88.71% 81.36% 84.88% trained model EAST MobileNetV3 78.20% 79.10% 78.65% trained model DB ResNet50_vd 86.41% 78.72% 82.38% trained model DB MobileNetV3 77.29% 73.08% 75.12% trained model SAST ResNet50_vd 91.39% 83.77% 87.42% trained model PSE ResNet50_vd 85.81% 79.53% 82.55% trained model PSE MobileNetV3 82.20% 70.48% 75.89% trained model DB++ ResNet50 90.89% 82.66% 86.58% pretrained model/trained model <p>On Total-Text dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link SAST ResNet50_vd 89.63% 78.44% 83.66% trained model CT ResNet18_vd 88.68% 81.70% 85.05% trained model <p>On CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link FCE ResNet50_dcn 88.39% 82.18% 85.27% trained model DRRG ResNet50_vd 89.92% 80.91% 85.18% trained model <p>Note\uff1a Additional data, like icdar2013, icdar2017, COCO-Text, ArT, was added to the model training of SAST. Download English public dataset in organized format used by PaddleOCR from:</p> <ul> <li>Baidu Drive (download code: 2bpi).</li> <li>Google Drive</li> </ul>"},{"location":"en/algorithm/overview.html#12-text-recognition-algorithms","title":"1.2 Text Recognition Algorithms","text":"<p>Supported text recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> CRNN</li> <li> Rosetta</li> <li> STAR-Net</li> <li> RARE</li> <li> SRN</li> <li> NRTR</li> <li> SAR</li> <li> SEED</li> <li> SVTR</li> <li> ViTSTR</li> <li> ABINet</li> <li> VisionLAN</li> <li> SPIN</li> <li> RobustScanner</li> <li> RFL</li> <li> ParseQ</li> <li> CPPD</li> <li> SATRN</li> </ul> <p>Refer to DTRB, the training and evaluation result of these above text recognition (using MJSynth and SynthText for training, evaluate on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE) is as follow:</p> Model Backbone Avg Accuracy Module combination Download link Rosetta Resnet34_vd 79.11% rec_r34_vd_none_none_ctc trained model Rosetta MobileNetV3 75.80% rec_mv3_none_none_ctc trained model CRNN Resnet34_vd 81.04% rec_r34_vd_none_bilstm_ctc trained model CRNN MobileNetV3 77.95% rec_mv3_none_bilstm_ctc trained model StarNet Resnet34_vd 82.85% rec_r34_vd_tps_bilstm_ctc trained model StarNet MobileNetV3 79.28% rec_mv3_tps_bilstm_ctc trained model RARE Resnet34_vd 83.98% rec_r34_vd_tps_bilstm_att trained model RARE MobileNetV3 81.76% rec_mv3_tps_bilstm_att trained model SRN Resnet50_vd_fpn 86.31% rec_r50fpn_vd_none_srn trained model NRTR NRTR_MTB 84.21% rec_mtb_nrtr trained model SAR Resnet31 87.20% rec_r31_sar trained model SEED Aster_Resnet 85.35% rec_resnet_stn_bilstm_att trained model SVTR SVTR-Tiny 89.25% rec_svtr_tiny_none_ctc_en trained model ViTSTR ViTSTR 79.82% rec_vitstr_none_ce trained model ABINet Resnet45 90.75% rec_r45_abinet trained model VisionLAN Resnet45 90.30% rec_r45_visionlan trained model SPIN ResNet32 90.00% rec_r32_gaspin_bilstm_att trained model RobustScanner ResNet31 87.77% rec_r31_robustscanner trained model RFL ResNetRFL 88.63% rec_resnet_rfl_att trained model ParseQ VIT 91.24% rec_vit_parseq_synth trained model CPPD SVTR-Base 93.8% rec_svtrnet_cppd_base_en trained model SATRN ShallowCNN 88.05% rec_satrn trained model"},{"location":"en/algorithm/overview.html#13-text-super-resolution-algorithms","title":"1.3 Text Super-Resolution Algorithms","text":"<p>Supported text super-resolution algorithms (Click the link to get the tutorial):</p> <ul> <li> Text Gestalt</li> <li> Text Telescope</li> </ul> <p>On the TextZoom public dataset, the effect of the algorithm is as follows:</p> Model Backbone PSNR_Avg SSIM_Avg Config Download link Text Gestalt tsrn 19.28 0.6560 configs/sr/sr_tsrn_transformer_strock.yml trained model Text Telescope tbsrn 21.56 0.7411 configs/sr/sr_telescope.yml trained model"},{"location":"en/algorithm/overview.html#14-formula-recognition-algorithm","title":"1.4 Formula Recognition Algorithm","text":"<p>Supported formula recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> CAN</li> <li> LaTeX-OCR</li> </ul> <p>On the CROHME handwritten formula dataset, the effect of the algorithm is as follows:</p> Model Backbone Config ExpRate Download link CAN DenseNet rec_d28_can.yml 51.72% trained model"},{"location":"en/algorithm/overview.html#2-end-to-end-ocr-algorithms","title":"2. End-to-end OCR Algorithms","text":"<p>Supported end-to-end algorithms (Click the link to get the tutorial):</p> <ul> <li> PGNet</li> </ul>"},{"location":"en/algorithm/overview.html#3-table-recognition-algorithms","title":"3. Table Recognition Algorithms","text":"<p>Supported table recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> TableMaster</li> </ul> <p>On the PubTabNet dataset, the algorithm result is as follows:</p> Model Backbone Config Acc Download link TableMaster TableResNetExtra configs/table/table_master.yml 77.47% trained model / inference model"},{"location":"en/algorithm/overview.html#4-key-information-extraction-algorithms","title":"4. Key Information Extraction Algorithms","text":"<p>Supported KIE algorithms (Click the link to get the tutorial):</p> <ul> <li> VI-LayoutXLM</li> <li> LayoutLM</li> <li> LayoutLMv2</li> <li> LayoutXLM</li> <li> SDMGR</li> </ul> <p>On wildreceipt dataset, the algorithm result is as follows:</p> Model Backbone Config Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model <p>On XFUND_zh dataset, the algorithm result is as follows:</p> Model Backbone Task Config Hmean Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% trained model LayoutLM LayoutLM-base SER ser_layoutlm_xfund_zh.yml 77.31% trained model LayoutLMv2 LayoutLMv2-base SER ser_layoutlmv2_xfund_zh.yml 85.44% trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% trained model LayoutLMv2 LayoutLMv2-base RE re_layoutlmv2_xfund_zh.yml 67.77% trained model"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html","title":"PGNet","text":""},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#1-brief-introduction","title":"1. Brief Introduction","text":"<p>OCR algorithms can be divided into two categories: two-stage algorithm and end-to-end algorithm. The two-stage OCR algorithm is generally divided into two parts, text detection and text recognition algorithm. The text detection algorithm locates the box of the text line from the image, and then the recognition algorithm identifies the content of the text box. The end-to-end OCR algorithm combines text detection and recognition in one algorithm. Its basic idea is to design a model with both detection unit and recognition module, share the CNN features of both and train them together. Because one algorithm can complete character recognition, the end-to-end model is smaller and faster.</p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#introduction-of-pgnet-algorithm","title":"Introduction Of PGNet Algorithm","text":"<p>During the recent years, the end-to-end OCR algorithm has been well developed, including MaskTextSpotter series, TextSnake, TextDragon, PGNet series and so on. Among these algorithms, PGNet algorithm has some advantages over the other algorithms.</p> <ul> <li>PGNet loss is designed to guide training, and no character-level annotations is needed.</li> <li>NMS and ROI related operations are not needed. It can accelerate the prediction</li> <li>The reading order prediction module is proposed</li> <li>A graph based modification module (GRM) is proposed to further improve the performance of model recognition</li> <li>Higher accuracy and faster prediction speed</li> </ul> <p>For details of PGNet algorithm, please refer to paper. The schematic diagram of the algorithm is as follows:</p> <p></p> <p>After feature extraction, the input image is sent to four branches: TBO module for text edge offset prediction, TCL module for text center-line prediction, TDO module for text direction offset prediction, and TCC module for text character classification graph prediction. The output of TBO and TCL can get text detection results after post-processing, and TCL, TDO and TCC are responsible for text recognition.</p> <p>The results of detection and recognition are as follows:</p> <p></p> <p></p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#performance","title":"Performance","text":""},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#test-set-total-text","title":"Test set: Total Text","text":""},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#test-environment-nvidia-tesla-v100-sxm2-16gb","title":"Test environment: NVIDIA Tesla V100-SXM2-16GB","text":"PGNetA det_precision det_recall det_f_score e2e_precision e2e_recall e2e_f_score FPS download Paper 85.30 86.80 86.10 - - 61.70 38.20 (size=640) - Ours 87.03 82.48 84.69 61.71 58.43 60.03 48.73 (size=768) download link <p>note:PGNet in PaddleOCR optimizes the prediction speed, and can significantly improve the end-to-end prediction speed within the acceptable range of accuracy reduction</p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#2-environment-configuration","title":"2. Environment Configuration","text":"<p>Please refer to Operation Environment Preparation to configure PaddleOCR operating environment first, refer to Project Clone to clone the project</p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#3-quick-use","title":"3. Quick Use","text":""},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#inference-model-download","title":"Inference model download","text":"<p>This section takes the trained end-to-end model as an example to quickly use the model prediction. First, download the trained end-to-end inference model download address</p> <pre><code>mkdir inference &amp;&amp; cd inference\n# Download the English end-to-end model and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/e2e_server_pgnetA_infer.tar &amp;&amp; tar xf e2e_server_pgnetA_infer.tar\n</code></pre> <ul> <li>In Windows environment, if 'wget' is not installed, the link can be copied to the browser when downloading the model, and decompressed and placed in the corresponding directory</li> </ul> <p>After decompression, there should be the following file structure:</p> <pre><code>\u251c\u2500\u2500 e2e_server_pgnetA_infer\n\u2502   \u251c\u2500\u2500 inference.pdiparams\n\u2502   \u251c\u2500\u2500 inference.pdiparams.info\n\u2502   \u2514\u2500\u2500 inference.pdmodel\n</code></pre>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#single-image-or-image-set-prediction","title":"Single image or image set prediction","text":"<pre><code># Prediction single image specified by image_dir\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --e2e_pgnet_valid_set=\"totaltext\"\n\n# Prediction the collection of images specified by image_dir\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --e2e_pgnet_valid_set=\"totaltext\"\n\n# If you want to use CPU for prediction, you need to set use_gpu parameter is false\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --use_gpu=False --e2e_pgnet_valid_set=\"totaltext\"\n</code></pre>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#visualization-results","title":"Visualization results","text":"<p>The visualized end-to-end results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#4-model-trainingevaluation-and-inference","title":"4. Model Training,Evaluation And Inference","text":"<p>This section takes the totaltext dataset as an example to introduce the training, evaluation and testing of the end-to-end model in PaddleOCR.</p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#data-preparation","title":"Data Preparation","text":"<p>Download and unzip totaltext dataset to PaddleOCR/train_data/, dataset organization structure is as follow:</p> <pre><code>/PaddleOCR/train_data/total_text/train/\n  |- rgb/            # total_text training data of dataset\n      |- img11.png\n      | ...\n  |- train.txt       # total_text training annotation of dataset\n</code></pre> <p>total_text.txt: the format of dimension file is as follows\uff0cthe file name and annotation information are separated by \"\\t\":</p> <pre><code>\" Image file name             Image annotation information encoded by json.dumps\"\nrgb/img11.jpg    [{\"transcription\": \"ASRAMA\", \"points\": [[214.0, 325.0], [235.0, 308.0], [259.0, 296.0], [286.0, 291.0], [313.0, 295.0], [338.0, 305.0], [362.0, 320.0], [349.0, 347.0], [330.0, 337.0], [310.0, 329.0], [290.0, 324.0], [269.0, 328.0], [249.0, 336.0], [231.0, 346.0]]}, {...}]\n</code></pre> <p>The image annotation after json.dumps() encoding is a list containing multiple dictionaries.</p> <p>The <code>points</code> in the dictionary represent the coordinates (x, y) of the four points of the text box, arranged clockwise from the point at the upper left corner.</p> <p><code>transcription</code> represents the text of the current text box. When its content is \"###\" it means that the text box is invalid and will be skipped during training.</p> <p>If you want to train PaddleOCR on other datasets, please build the annotation file according to the above format.</p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#start-training","title":"Start Training","text":"<p>PGNet training is divided into two steps: Step 1: training on the synthetic data to get the pretrain_model, and the accuracy of the model is still low; step 2: loading the pretrain_model and training on the totaltext data set; for fast training, we directly provide the pre training model of step 1download link.</p> <pre><code>cd PaddleOCR/\n\n# download step1 pretrain_models\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/train_step1.tar\n\n# You can get the following file format\n./pretrain_models/train_step1/\n  \u2514\u2500 best_accuracy.pdopt\n  \u2514\u2500 best_accuracy.states\n  \u2514\u2500 best_accuracy.pdparams\n</code></pre> <p>If CPU version installed, please set the parameter <code>use_gpu</code> to <code>false</code> in the configuration.</p> <pre><code># single GPU training\npython3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./pretrain_models/train_step1/best_accuracy Global.load_static_weights=False\n# multi-GPU training\n# Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./pretrain_models/train_step1/best_accuracy  Global.load_static_weights=False\n</code></pre> <p>In the above instruction, use <code>-c</code> to select the training to use the <code>configs/e2e/e2e_r50_vd_pg.yml</code> configuration file. For a detailed explanation of the configuration file, please refer to config.</p> <p>You can also use <code>-o</code> to change the training parameters without modifying the yml file. For example, adjust the training learning rate to 0.0001</p> <pre><code>python3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Optimizer.base_lr=0.0001\n</code></pre>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#load-trained-model-and-continue-training","title":"Load trained model and continue training","text":"<p>If you would like to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <pre><code>python3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrain_weights</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrain_weights</code> will be loaded.</p> <p>PaddleOCR calculates three indicators for evaluating performance of OCR end-to-end task: Precision, Recall, and Hmean.</p> <p>Run the following code to calculate the evaluation indicators. The result will be saved in the test result file specified by <code>save_res_path</code> in the configuration file <code>e2e_r50_vd_pg.yml</code> When evaluating, set post-processing parameters <code>max_side_len=768</code>. If you use different datasets, different models for training. The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file.</p> <pre><code>python3 tools/eval.py -c configs/e2e/e2e_r50_vd_pg.yml  -o Global.checkpoints=\"{path/to/weights}/best_accuracy\"\n</code></pre>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#model-test","title":"Model Test","text":"<p>Test the end-to-end result on a single image:</p> <pre><code>python3 tools/infer_e2e.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/e2e_pgnet/best_accuracy\" Global.load_static_weights=false\n</code></pre> <p>Test the end-to-end result on all images in the folder:</p> <pre><code>python3 tools/infer_e2e.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.infer_img=\"./doc/imgs_en/\" Global.pretrained_model=\"./output/e2e_pgnet/best_accuracy\" Global.load_static_weights=false\n</code></pre>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#model-inference","title":"Model inference","text":""},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#1quadrangle-text-detection-model-icdar2015","title":"(1).Quadrangle text detection model (ICDAR2015)","text":"<p>First, convert the model saved in the PGNet end-to-end training process into an inference model. In the first stage of training based on composite dataset, the model of English data set training is taken as an examplemodel download link, you can use the following command to convert:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/en_server_pgnetA.tar &amp;&amp; tar xf en_server_pgnetA.tar\npython3 tools/export_model.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./en_server_pgnetA/best_accuracy Global.load_static_weights=False Global.save_inference_dir=./inference/e2e\n</code></pre> <p>For PGNet quadrangle end-to-end model inference, you need to set the parameter <code>--e2e_algorithm=\"PGNet\"</code> and <code>--e2e_pgnet_valid_set=\"partvgg\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img_10.jpg\" --e2e_model_dir=\"./inference/e2e/\" --e2e_pgnet_valid_set=\"partvgg\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/algorithm/end_to_end/algorithm_e2e_pgnet.html#2-curved-text-detection-model-total-text","title":"(2). Curved text detection model (Total-Text)","text":"<p>For the curved text example, we use the same model as the quadrilateral For PGNet end-to-end curved text detection model inference, you need to set the parameter <code>--e2e_algorithm=\"PGNet\"</code> and <code>--e2e_pgnet_valid_set=\"totaltext\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e/\" --e2e_pgnet_valid_set=\"totaltext\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html","title":"CAN","text":""},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, Xiang Bai ECCV, 2022</p> <p>Using CROHME handwrittem mathematical expression recognition datasets for training, and evaluating on its test sets, the algorithm reproduction effect is as follows:</p> Model Backbone config exprate Download link CAN DenseNet rec_d28_can.yml 51.72% trained model"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_d28_can.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_d28_can.yml\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_d28_can.yml -o Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_d28_can.yml -o Architecture.Head.attdecoder.is_train=False Global.infer_img='./doc/crohme_demo/hme_00.jpg' Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CAN handwritten mathematical expression recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_d28_can.yml -o Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams Global.save_inference_dir=./inference/rec_d28_can/ Architecture.Head.attdecoder.is_train=False\n\n# The default output max length of the model is 36. If you need to predict a longer sequence, please specify its output sequence as an appropriate value when exporting the model, as: Architecture.Head.max_ text_ length=72\n</code></pre> <p>For CAN handwritten mathematical expression recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/datasets/crohme_demo/hme_00.jpg\" --rec_algorithm=\"CAN\" --rec_batch_num=1 --rec_model_dir=\"./inference/rec_d28_can/\" --rec_char_dict_path=\"./ppocr/utils/dict/latex_symbol_dict.txt\"\n\n# If you need to predict on a picture with black characters on a white background, please set: -- rec_ image_ inverse=False\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/formula_recognition/algorithm_rec_can.html#citation","title":"Citation","text":"<pre><code>@misc{https://doi.org/10.48550/arxiv.2207.11463,\n  doi = {10.48550/ARXIV.2207.11463},\n  url = {https://arxiv.org/abs/2207.11463},\n  author = {Li, Bohan and Yuan, Ye and Liang, Dingkang and Liu, Xiao and Ji, Zhilong and Bai, Jinfeng and Liu, Wenyu and Bai, Xiang},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html","title":"LaTeX-OCR","text":""},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#1-introduction","title":"1. Introduction","text":"<p>Original Project:</p> <p>https://github.com/lukas-blecher/LaTeX-OCR</p> <p>Using LaTeX-OCR printed mathematical expression recognition datasets for training, and evaluating on its test sets, the algorithm reproduction effect is as follows:</p> Model Backbone config BLEU score normed edit distance ExpRate Download link LaTeX-OCR Hybrid ViT rec_latex_ocr.yml 0.8821 0.0823 40.01% trained model"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\" to clone the project code.</p> <p>Furthermore, additional dependencies need to be installed: <pre><code>pip install \"tokenizers==0.19.1\" \"imagesize\"\n</code></pre></p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p> <p>Pickle File Generation:</p> <p>Download formulae.zip and math.txt in Google Drive, and then use the following command to generate the pickle file.</p> <pre><code># Create a LaTeX-OCR dataset directory\nmkdir -p train_data/LaTeXOCR\n# Unzip formulae.zip and copy math.txt\nunzip -d train_data/LaTeXOCR path/formulae.zip\ncp path/math.txt train_data/LaTeXOCR\n# Convert the original .txt file to a .pkl file to group images of different scales\n# Training set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/train --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n# Validation set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/val --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n# Test set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/test --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n</code></pre> <p>Training:</p> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#Single GPU training (Default training method)\npython3 tools/train.py -c configs/rec/rec_latex_ocr.yml\n\n#Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_latex_ocr.yml\n</code></pre> <p>Evaluation:</p> <pre><code># GPU evaluation\n# Validation set evaluation\npython3 tools/eval.py -c configs/rec/rec_latex_ocr.yml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams Metric.cal_blue_score=True\n# Test set evaluation\npython3 tools/eval.py -c configs/rec/rec_latex_ocr.yml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams Metric.cal_blue_score=True Eval.dataset.data=./train_data/LaTeXOCR/latexocr_test.pkl\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_latex_ocr.yml  -o  Architecture.Backbone.is_predict=True Architecture.Backbone.is_export=True Architecture.Head.is_export=True Global.infer_img='./doc/datasets/pme_demo/0000013.png' Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the LaTeX-OCR printed mathematical expression recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_latex_ocr.yml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams Global.save_inference_dir=./inference/rec_latex_ocr_infer/ Architecture.Backbone.is_predict=True Architecture.Backbone.is_export=True Architecture.Head.is_export=True\n\n# The default output max length of the model is 512.\n</code></pre> <p>For LaTeX-OCR printed mathematical expression recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/datasets/pme_demo/0000295.png' --rec_algorithm=\"LaTeXOCR\" --rec_batch_num=1 --rec_model_dir=\"./inference/rec_latex_ocr_infer/\"  --rec_char_dict_path=\"./ppocr/utils/dict/latex_ocr_tokenizer.json\"\n</code></pre>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html","title":"KIE Algorithm - LayoutXLM","text":""},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</p> <p>Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei</p> <p>2021</p> <p>On XFUND_zh dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Task Cnnfig Hmean Download link LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% trained model/inference model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% trained model/inference model"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to KIE tutorial\u3002PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#ser","title":"SER","text":"<p>First, we need to export the trained model into inference model. Take LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/pplayout/ser_LayoutXLM_xfun_zh.tar\ntar -xf ser_LayoutXLM_xfun_zh.tar\npython3 tools/export_model.py -c configs/kie/layoutlm_series/ser_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./ser_LayoutXLM_xfun_zh Global.save_inference_dir=./inference/ser_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using LayoutXLM SER model:</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>The SER visualization results are saved in the <code>./output</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#re","title":"RE","text":"<p>First, we need to export the trained model into inference model. Take LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/pplayout/re_LayoutXLM_xfun_zh.tar\ntar -xf re_LayoutXLM_xfun_zh.tar\npython3 tools/export_model.py -c configs/kie/layoutlm_series/re_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./re_LayoutXLM_xfun_zh Global.save_inference_dir=./inference/re_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using LayoutXLM RE model:</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_layoutxlm_infer \\\n  --ser_model_dir=../inference/ser_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>The RE visualization results are saved in the <code>./output</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/kie/algorithm_kie_layoutxlm.html#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2104-08836,\n  author    = {Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Furu Wei},\n  title     = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n               Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2104.08836},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2104.08836},\n  eprinttype = {arXiv},\n  eprint    = {2104.08836},\n  timestamp = {Thu, 14 Oct 2021 09:17:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08836.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-1912-13318,\n  author    = {Yiheng Xu and\n               Minghao Li and\n               Lei Cui and\n               Shaohan Huang and\n               Furu Wei and\n               Ming Zhou},\n  title     = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1912.13318},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1912.13318},\n  eprinttype = {arXiv},\n  eprint    = {1912.13318},\n  timestamp = {Mon, 01 Jun 2020 16:20:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-13318.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-2012-14740,\n  author    = {Yang Xu and\n               Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Furu Wei and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei A. F. Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Wanxiang Che and\n               Min Zhang and\n               Lidong Zhou},\n  title     = {LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2012.14740},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2012.14740},\n  eprinttype = {arXiv},\n  eprint    = {2012.14740},\n  timestamp = {Tue, 27 Jul 2021 09:53:52 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-14740.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html","title":"KIE Algorithm - SDMGR","text":""},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Spatial Dual-Modality Graph Reasoning for Key Information Extraction</p> <p>Hongbin Sun and Zhanghui Kuang and Xiaoyu Yue and Chenhao Lin and Wayne Zhang</p> <p>2021</p> <p>On wildreceipt dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Cnnfig Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model/inference model(coming soon)"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>SDMGR is a key information extraction algorithm that classifies each detected textline into predefined categories, such as order ID, invoice number, amount, etc.</p> <p>The training and test data are collected in the wildreceipt dataset, use following command to downloaded the dataset.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/dataset/wildreceipt.tar &amp;&amp; tar xf wildreceipt.tar\n</code></pre> <p>Create dataset soft link to <code>PaddleOCR/train_data</code> directory.</p> <pre><code>cd PaddleOCR/ &amp;&amp; mkdir train_data &amp;&amp; cd train_data\nln -s ../../wildreceipt ./\n</code></pre>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#31-model-training","title":"3.1 Model training","text":"<p>The config file is <code>configs/kie/sdmgr/kie_unet_sdmgr.yml</code>\uff0c the default dataset path is <code>train_data/wildreceipt</code>.</p> <p>Use the following command to train the model.</p> <pre><code>python3 tools/train.py -c configs/kie/sdmgr/kie_unet_sdmgr.yml -o Global.save_model_dir=./output/kie/\n</code></pre>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#32-model-evaluation","title":"3.2 Model evaluation","text":"<p>Use the following command to evaluate the model:</p> <pre><code>python3 tools/eval.py -c configs/kie/sdmgr/kie_unet_sdmgr.yml -o Global.checkpoints=./output/kie/best_accuracy\n</code></pre> <p>An example of output information is shown below.</p> <pre><code>[2022/08/10 05:22:23] ppocr INFO: metric eval ***************\n[2022/08/10 05:22:23] ppocr INFO: hmean:0.8670120239257812\n[2022/08/10 05:22:23] ppocr INFO: fps:10.18816520530961\n</code></pre>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#33-model-prediction","title":"3.3 Model prediction","text":"<p>Use the following command to load the model and predict. During the prediction, the text file storing the image path and OCR information needs to be loaded in advance. Use <code>Global.infer_img</code> to assign.</p> <pre><code>python3 tools/infer_kie.py -c configs/kie/kie_unet_sdmgr.yml -o Global.checkpoints=kie_vgg16/best_accuracy  Global.infer_img=./train_data/wildreceipt/1.txt\n</code></pre> <p>The visualization results and texts are saved in the <code>./output/sdmgr_kie/</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/kie/algorithm_kie_sdmgr.html#citation","title":"Citation","text":"<pre><code>@misc{sun2021spatial,\n      title={Spatial Dual-Modality Graph Reasoning for Key Information Extraction},\n      author={Hongbin Sun and Zhanghui Kuang and Xiaoyu Yue and Chenhao Lin and Wayne Zhang},\n      year={2021},\n      eprint={2103.14470},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html","title":"KIE Algorithm - VI-LayoutXLM","text":""},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#1-introduction","title":"1. Introduction","text":"<p>VI-LayoutXLM is improved based on LayoutXLM. In the process of downstream finetuning, the visual backbone network module is removed, and the model infernce speed is further improved on the basis of almost lossless accuracy.</p> <p>On XFUND_zh dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Task Config Hmean Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% trained model/inference model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% trained model/inference model"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to KIE tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#ser","title":"SER","text":"<p>First, we need to export the trained model into inference model. Take VI-LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar\ntar -xf ser_vi_layoutxlm_xfund_pretrained.tar\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./ser_vi_layoutxlm_xfund_pretrained/best_accuracy Global.save_inference_dir=./inference/ser_vi_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using VI-LayoutXLM SER model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The SER visualization results are saved in the <code>./output</code> folder by default. The results are as follows.</p> <p></p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#re","title":"RE","text":"<p>First, we need to export the trained model into inference model. Take VI-LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar\ntar -xf re_vi_layoutxlm_xfund_pretrained.tar\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./re_vi_layoutxlm_xfund_pretrained/best_accuracy Global.save_inference_dir=./inference/re_vi_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using VI-LayoutXLM RE model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm_infer \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_infer \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The RE visualization results are saved in the <code>./output</code> folder by default. The results are as follows.</p> <p></p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/kie/algorithm_kie_vi_layoutxlm.html#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2104-08836,\n  author    = {Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Furu Wei},\n  title     = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n               Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2104.08836},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2104.08836},\n  eprinttype = {arXiv},\n  eprint    = {2104.08836},\n  timestamp = {Thu, 14 Oct 2021 09:17:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08836.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-1912-13318,\n  author    = {Yiheng Xu and\n               Minghao Li and\n               Lei Cui and\n               Shaohan Huang and\n               Furu Wei and\n               Ming Zhou},\n  title     = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1912.13318},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1912.13318},\n  eprinttype = {arXiv},\n  eprint    = {1912.13318},\n  timestamp = {Mon, 01 Jun 2020 16:20:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-13318.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-2012-14740,\n  author    = {Yang Xu and\n               Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Furu Wei and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei A. F. Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Wanxiang Che and\n               Min Zhang and\n               Lidong Zhou},\n  title     = {LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2012.14740},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2012.14740},\n  eprinttype = {arXiv},\n  eprint    = {2012.14740},\n  timestamp = {Tue, 27 Jul 2021 09:53:52 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-14740.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html","title":"Text Gestalt","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Text Gestalt: Stroke-Aware Scene Text Image Super-Resolution Chen, Jingye and Yu, Haiyang and Ma, Jianqi and Li, Bin and Xue, Xiangyang AAAI, 2022</p> <p>Referring to the FudanOCR data download instructions, the effect of the super-score algorithm on the TextZoom test set is as follows:</p> Model Backbone config Acc Download link Text Gestalt tsrn 19.28 0.6560 configs/sr/sr_tsrn_transformer_strock.yml"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different models only requires changing the configuration file.</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/sr/sr_tsrn_transformer_strock.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/sr/sr_tsrn_transformer_strock.yml\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\n\npython3 tools/infer_sr.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words_en/word_52.png\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.save_inference_dir=./inference/sr_out\n</code></pre> <p>For Text-Gestalt super-resolution model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_sr.py --sr_model_dir=./inference/sr_out --image_dir=doc/imgs_words_en/word_52.png --sr_image_shape=3,32,128\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_gestalt.html#citation","title":"Citation","text":"<pre><code>@inproceedings{chen2022text,\n  title={Text gestalt: Stroke-aware scene text image super-resolution},\n  author={Chen, Jingye and Yu, Haiyang and Ma, Jianqi and Li, Bin and Xue, Xiangyang},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={36},\n  number={1},\n  pages={285--293},\n  year={2022}\n}\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html","title":"Text Gestalt","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Scene Text Telescope: Text-Focused Scene Image Super-Resolution Chen, Jingye, Bin Li, and Xiangyang Xue CVPR, 2021</p> <p>Referring to the FudanOCR data download instructions, the effect of the super-score algorithm on the TextZoom test set is as follows:</p> Model Backbone config Acc Download link Text Gestalt tsrn 21.56 0.7411 configs/sr/sr_telescope.yml <p>The TextZoom dataset comes from two superfraction data sets, RealSR and SR-RAW, both of which contain LR-HR pairs. TextZoom has 17367 pairs of training data and 4373 pairs of test data.</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different models only requires changing the configuration file.</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/sr/sr_telescope.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/sr/sr_telescope.yml\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\n\npython3 tools/infer_sr.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words_en/word_52.png\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.save_inference_dir=./inference/sr_out\n</code></pre> <p>For Text-Telescope super-resolution model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_sr.py --sr_model_dir=./inference/sr_out --image_dir=doc/imgs_words_en/word_52.png --sr_image_shape=3,32,128\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/super_resolution/algorithm_sr_telescope.html#citation","title":"Citation","text":"<pre><code>@INPROCEEDINGS{9578891,\n  author={Chen, Jingye and Li, Bin and Xue, Xiangyang},\n  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  title={Scene Text Telescope: Text-Focused Scene Image Super-Resolution},\n  year={2021},\n  volume={},\n  number={},\n  pages={12021-12030},\n  doi={10.1109/CVPR46437.2021.01185}}\n</code></pre>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html","title":"Table Recognition Algorithm-TableMASTER","text":""},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>TableMaster: PINGAN-VCGROUP\u2019S SOLUTION FOR ICDAR 2021 COMPETITION ON SCIENTIFIC LITERATURE PARSING TASK B: TABLE RECOGNITION TO HTML Ye, Jiaquan and Qi, Xianbiao and He, Yelin and Chen, Yihao and Gu, Dengyi and Gao, Peng and Xiao, Rong 2021</p> <p>On the PubTabNet table recognition public data set, the algorithm reproduction acc is as follows:</p> Model Backbone Cnnfig Acc Download link TableMaster TableResNetExtra configs/table/table_master.yml 77.47% trained model/inference model"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above TableMaster model is trained using the PubTabNet table recognition public dataset. For the download of the dataset, please refer to table_datasets.</p> <p>After the data download is complete, please refer to Text Recognition Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the TableMaster table recognition training process into an inference model. Taking the model based on the TableResNetExtra backbone network and trained on the PubTabNet dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/table/table_master.yml -o Global.pretrained_model=output/table_master/best_accuracy Global.save_inference_dir=./inference/table_master\n</code></pre> <p>Note:</p> <ul> <li>If you trained the model on your own dataset and adjusted the dictionary file, please pay attention to whether the <code>character_dict_path</code> in the modified configuration file is the correct dictionary file</li> </ul> <p>Execute the following command for model inference:</p> <pre><code>cd ppstructure/\n# When predicting all images in a folder, you can modify image_dir to a folder, such as --image_dir='docs/table'.\npython3.7 table/predict_structure.py --table_model_dir=../output/table_master/table_structure_tablemaster_infer/ --table_algorithm=TableMaster --table_char_dict_path=../ppocr/utils/dict/table_master_structure_dict.txt --table_max_len=480 --image_dir=docs/table/table.jpg\n</code></pre> <p>After executing the command, the prediction results of the above image (structural information and the coordinates of each cell in the table) are printed to the screen, and the visualization of the cell coordinates is also saved. An example is as follows:</p> <p>result\uff1a</p> <pre><code>[2022/06/16 13:06:54] ppocr INFO: result: ['&lt;html&gt;', '&lt;body&gt;', '&lt;table&gt;', '&lt;thead&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/thead&gt;', '&lt;tbody&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/tbody&gt;', '&lt;/table&gt;', '&lt;/body&gt;', '&lt;/html&gt;'], [[72.17591094970703, 10.759100914001465, 60.29658508300781, 16.6805362701416], [161.85562133789062, 10.884308815002441, 14.9495210647583, 16.727018356323242], [277.79876708984375, 29.54340362548828, 31.490320205688477, 18.143272399902344],\n...\n[336.11724853515625, 280.3601989746094, 39.456939697265625, 18.121286392211914]]\n[2022/06/16 13:06:54] ppocr INFO: save vis result to ./output/table.jpg\n[2022/06/16 13:06:54] ppocr INFO: Predict time of docs/table/table.jpg: 17.36806297302246\n</code></pre> <p>Note:</p> <ul> <li>TableMaster is relatively slow during inference, and it is recommended to use GPU for use.</li> </ul>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the TableMaster does not support CPP inference.</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/table_recognition/algorithm_table_master.html#citation","title":"Citation","text":"<pre><code>@article{ye2021pingan,\n  title={PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML},\n  author={Ye, Jiaquan and Qi, Xianbiao and He, Yelin and Chen, Yihao and Gu, Dengyi and Gao, Peng and Xiao, Rong},\n  journal={arXiv preprint arXiv:2105.01848},\n  year={2021}\n}\n</code></pre>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html","title":"\u8868\u683c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-SLANet-LCNetV2","text":""},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p>PaddleOCR \u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e8c\u200b\uff1a\u200b\u901a\u7528\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u6392\u884c\u699c\u200b\u7b2c\u4e00\u200b\u7b97\u6cd5\u200b\u3002\u200b\u6838\u5fc3\u200b\u601d\u8def\u200b\uff1a</p> <ul> <li> <ol> <li>\u200b\u6539\u5584\u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u81f3\u200bEOS\u200b\u505c\u6b62\u200b\uff0c\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b3\u200b\u500d\u200b</li> </ol> </li> <li> <ol> <li>\u200b\u5347\u7ea7\u200bBackbone\u200b\u4e3a\u200bLCNetV2\uff08SSLD\u200b\u7248\u672c\u200b\uff09</li> </ol> </li> <li> <ol> <li>\u200b\u884c\u5217\u200b\u7279\u5f81\u200b\u589e\u5f3a\u200b\u6a21\u5757\u200b</li> </ol> </li> <li> <ol> <li>\u200b\u63d0\u5347\u200b\u5206\u8fa8\u7387\u200b488\u200b\u81f3\u200b512</li> </ol> </li> <li> <ol> <li>\u200b\u4e09\u200b\u9636\u6bb5\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b</li> </ol> </li> </ul> <p>\u200b\u5728\u200bPubTabNet\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u590d\u73b0\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b \u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b acc SLANet LCNetV2 configs/table/SLANet_lcnetv2.yml 76.67%"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":"<p>\u200b\u4e0a\u8ff0\u200bSLANet_LCNetv2\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPubTabNet\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u53ef\u200b\u53c2\u8003\u200b table_datasets\u3002</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#_1","title":"\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6559\u7a0b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002PaddleOCR\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># stage1\npython3 -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/table/SLANet_lcnetv2.yml\n# stage2 \u200b\u52a0\u8f7d\u200bstage1\u200b\u7684\u200bbest model\u200b\u4f5c\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u6574\u200b\u4e3a\u200b0.0001;\n# stage3 \u200b\u52a0\u8f7d\u200bstage2\u200b\u7684\u200bbest model\u200b\u4f5c\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0d\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u5c06\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u6240\u6709\u200b\u7684\u200b488\u200b\u4fee\u6539\u200b\u4e3a\u200b512.\n</code></pre>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#4","title":"4. \u200b\u63a8\u7406\u200b\u90e8\u7f72","text":""},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#41-python","title":"4.1 Python\u200b\u63a8\u7406","text":"<p>\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200bbest\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f6c\u6362\u6210\u200binference model\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/export_model.py -c configs/table/SLANet_lcnetv2.yml -o Global.pretrained_model=path/best_accuracy Global.save_inference_dir=./inference/slanet_lcnetv2_infer\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8c03\u6574\u200b\u4e86\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>character_dict_path</code>\u200b\u662f\u5426\u200b\u4e3a\u200b\u6240\u200b\u6b63\u786e\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>./inference/slanet_lcnetv2_infer/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\uff1a</p> <pre><code>cd ppstructure/\npython3.7 table/predict_structure.py --table_model_dir=../inference/slanet_lcnetv2_infer/ --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt --image_dir=docs/table/table.jpg --output=../output/table_slanet_lcnetv2 --use_gpu=False --benchmark=True --enable_mkldnn=True\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200bimage_dir\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b --image_dir='docs/table'\u3002\n</code></pre> <p>\u200b\u6267\u884c\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u4e0a\u9762\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\u548c\u200b\u8868\u683c\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u5355\u5143\u683c\u200b\u7684\u200b\u5750\u6807\u200b\uff09\u200b\u4f1a\u200b\u6253\u5370\u200b\u5230\u200b\u5c4f\u5e55\u200b\u4e0a\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5355\u5143\u683c\u200b\u5750\u6807\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u3002\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a \u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>[2022/06/16 13:06:54] ppocr INFO: result: ['&lt;html&gt;', '&lt;body&gt;', '&lt;table&gt;', '&lt;thead&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/thead&gt;', '&lt;tbody&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/tbody&gt;', '&lt;/table&gt;', '&lt;/body&gt;', '&lt;/html&gt;'], [[72.17591094970703, 10.759100914001465, 60.29658508300781, 16.6805362701416], [161.85562133789062, 10.884308815002441, 14.9495210647583, 16.727018356323242], [277.79876708984375, 29.54340362548828, 31.490320205688477, 18.143272399902344],\n...\n[336.11724853515625, 280.3601989746094, 39.456939697265625, 18.121286392211914]]\n[2022/06/16 13:06:54] ppocr INFO: save vis result to ./output/table.jpg\n[2022/06/16 13:06:54] ppocr INFO: Predict time of docs/table/table.jpg: 17.36806297302246\n</code></pre>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#42-c","title":"4.2 C++\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u7531\u4e8e\u200bC++\u200b\u9884\u5904\u7406\u200b\u540e\u5904\u7406\u200b\u8fd8\u200b\u672a\u200b\u652f\u6301\u200bSLANet</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#43-serving","title":"4.3 Serving\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#44","title":"4.4 \u200b\u66f4\u200b\u591a\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/algorithm/table_recognition/algorithm_table_slanet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_ct.html","title":"CT","text":""},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>CentripetalText: An Efficient Text Instance Representation for Scene Text Detection Tao Sheng, Jie Chen, Zhouhui Lian NeurIPS, 2021</p> <p>On the Total-Text dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download CT ResNet18_vd configs/det/det_r18_vd_ct.yml 88.68% 81.70% 85.05% trained model"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above CT model is trained using the Total-Text text detection public dataset. For the download of the dataset, please refer to Total-Text-Dataset. PaddleOCR format annotation download link train.txt, test.txt.</p> <p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the CT text detection training process into an inference model. Taking the model based on the Resnet18_vd backbone network and trained on the Total Text English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r18_vd_ct.yml -o Global.pretrained_model=./det_r18_ct_train/best_accuracy  Global.save_inference_dir=./inference/det_ct\n</code></pre> <p>CT text detection model inference, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_ct/\" --det_algorithm=\"CT\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>. Examples of results are as follows:</p> <p></p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_ct.html#citation","title":"Citation","text":"<pre><code>@inproceedings{sheng2021centripetaltext,\n    title={CentripetalText: An Efficient Text Instance Representation for Scene Text Detection},\n    author={Tao Sheng and Jie Chen and Zhouhui Lian},\n    booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},\n    year={2021}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html","title":"DB &amp;&amp; DB++","text":""},{"location":"en/algorithm/text_detection/algorithm_det_db.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Real-time Scene Text Detection with Differentiable Binarization Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang AAAI, 2020</p> <p>Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang TPAMI, 2022</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DB ResNet50_vd configs/det/det_r50_vd_db.yml 86.41% 78.72% 82.38% trained model DB MobileNetV3 configs/det/det_mv3_db.yml 77.29% 73.08% 75.12% trained model DB++ ResNet50 configs/det/det_r50_db++_ic15.yml 90.89% 82.66% 86.58% pretrained model/trained model <p>On the TD_TR dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DB++ ResNet50 configs/det/det_r50_db++_td_tr.yml 92.92% 86.48% 89.58% pretrained model/trained model"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_db.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the DB text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_db.yml -o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_db\n</code></pre> <p>DB text detection model inference, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_db/\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>. Examples of results are as follows:</p> <p></p> <p>Note: Since the ICDAR2015 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese text images.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for DB:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_detection/algorithm_det_db.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_db.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liao2020real,\n  title={Real-time scene text detection with differentiable binarization},\n  author={Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={34},\n  number={07},\n  pages={11474--11481},\n  year={2020}\n}\n\n@article{liao2022real,\n  title={Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion},\n  author={Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  year={2022},\n  publisher={IEEE}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html","title":"DRRG","text":""},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection Zhang, Shi-Xue and Zhu, Xiaobin and Hou, Jie-Bo and Liu, Chang and Yang, Chun and Wang, Hongfa and Yin, Xu-Cheng CVPR, 2020</p> <p>On the CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DRRG ResNet50_vd configs/det/det_r50_drrg_ctw.yml 89.92% 80.91% 85.18% trained model"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above DRRG model is trained using the CTW1500 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Since the model needs to be converted to Numpy data for many times in the forward, DRRG dynamic graph to static graph is not supported.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_drrg.html#citation","title":"Citation","text":"<pre><code>@inproceedings{zhang2020deep,\n  title={Deep relational reasoning graph network for arbitrary shape text detection},\n  author={Zhang, Shi-Xue and Zhu, Xiaobin and Hou, Jie-Bo and Liu, Chang and Yang, Chun and Wang, Hongfa and Yin, Xu-Cheng},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={9699--9708},\n  year={2020}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html","title":"EAST","text":""},{"location":"en/algorithm/text_detection/algorithm_det_east.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>EAST: An Efficient and Accurate Scene Text Detector Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang CVPR, 2017</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download EAST ResNet50_vd det_r50_vd_east.yml 88.71% 81.36% 84.88% model EAST MobileNetV3 det_mv3_east.yml 78.20% 79.10% 78.65% model"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above EAST model is trained using the ICDAR2015 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_east.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the EAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_east.yml -o Global.pretrained_model=./det_r50_vd_east_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_r50_east/\n</code></pre> <p>For EAST text detection model inference, you need to set the parameter --det_algorithm=\"EAST\", run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_r50_east/\" --det_algorithm=\"EAST\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>.</p> <p></p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the EAST text detection model does not support CPP inference.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_east.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_east.html#citation","title":"Citation","text":"<pre><code>@inproceedings{zhou2017east,\n  title={East: an efficient and accurate scene text detector},\n  author={Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},\n  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},\n  pages={5551--5560},\n  year={2017}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html","title":"FCENet","text":""},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Fourier Contour Embedding for Arbitrary-Shaped Text Detection Yiqin Zhu and Jianyong Chen and Lingyu Liang and Zhanghui Kuang and Lianwen Jin and Wayne Zhang CVPR, 2021</p> <p>On the CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download FCE ResNet50_dcn configs/det/det_r50_vd_dcn_fce_ctw.yml 88.39% 82.18% 85.27% trained model"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above FCE model is trained using the CTW1500 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the FCE text detection training process into an inference model. Taking the model based on the Resnet50_vd_dcn backbone network and trained on the CTW1500 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_dcn_fce_ctw.yml -o Global.pretrained_model=./det_r50_dcn_fce_ctw_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_fce\n</code></pre> <p>FCE text detection model inference, to perform non-curved text detection, you can run the following commands:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_fce/\" --det_algorithm=\"FCE\" --det_fce_box_type=quad\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>If you want to perform curved text detection, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_fce/\" --det_algorithm=\"FCE\" --det_fce_box_type=poly\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: Since the CTW1500 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese or curved text images.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the FCE text detection model does not support CPP inference.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_fcenet.html#citation","title":"Citation","text":"<pre><code>@InProceedings{zhu2021fourier,\n  title={Fourier Contour Embedding for Arbitrary-Shaped Text Detection},\n  author={Yiqin Zhu and Jianyong Chen and Lingyu Liang and Zhanghui Kuang and Lianwen Jin and Wayne Zhang},\n  year={2021},\n  booktitle = {CVPR}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html","title":"PSENet","text":""},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Shape robust text detection with progressive scale expansion network Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai CVPR, 2019</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download PSE ResNet50_vd configs/det/det_r50_vd_pse.yml 85.81% 79.53% 82.55% trained model PSE MobileNetV3 configs/det/det_mv3_pse.yml 82.20% 70.48% 75.89% trained model"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above PSE model is trained using the ICDAR2015 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the PSE text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_pse.yml -o Global.pretrained_model=./det_r50_vd_pse_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_pse\n</code></pre> <p>PSE text detection model inference, to perform non-curved text detection, you can run the following commands:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_pse/\" --det_algorithm=\"PSE\" --det_pse_box_type=quad\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>If you want to perform curved text detection, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_pse/\" --det_algorithm=\"PSE\" --det_pse_box_type=poly\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: Since the ICDAR2015 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese or curved text images.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the PSE text detection model does not support CPP inference.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_psenet.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2019shape,\n  title={Shape robust text detection with progressive scale expansion network},\n  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={9336--9345},\n  year={2019}\n}\n</code></pre>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html","title":"SAST","text":""},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Huang, Zuming and En, Mengyi and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming ACM MM, 2019</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download SAST ResNet50_vd configs/det/det_r50_vd_sast_icdar15.yml 91.39% 83.77% 87.42% trained model <p>On the Total-text dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download SAST ResNet50_vd configs/det/det_r50_vd_sast_totaltext.yml 89.63% 78.44% 83.66% trained model"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#1-quadrangle-text-detection-model-icdar2015","title":"(1). Quadrangle text detection model (ICDAR2015)","text":"<p>First, convert the model saved in the SAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as an example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_sast_icdar15.yml -o Global.pretrained_model=./det_r50_vd_sast_icdar15_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_sast_ic15\n</code></pre> <p>For SAST quadrangle text detection model inference, you need to set the parameter <code>--det_algorithm=\"SAST\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"SAST\" --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_sast_ic15/\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#2-curved-text-detection-model-total-text","title":"(2). Curved text detection model (Total-Text)","text":"<p>First, convert the model saved in the SAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the Total-Text English dataset as an example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_sast_totaltext.yml -o Global.pretrained_model=./det_r50_vd_sast_totaltext_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_sast_tt\n</code></pre> <p>For SAST curved text detection model inference, you need to set the parameter <code>--det_algorithm=\"SAST\"</code> and <code>--det_box_type=poly</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"SAST\" --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_sast_tt/\" --det_box_type='poly'\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: SAST post-processing locality aware NMS has two versions: Python and C++. The speed of C++ version is obviously faster than that of Python version. Due to the compilation version problem of NMS of C++ version, C++ version NMS will be called only in Python 3.5 environment, and python version NMS will be called in other cases.</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_detection/algorithm_det_sast.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2019single,\n  title={A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning},\n  author={Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Huang, Zuming and En, Mengyi and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},\n  pages={1277--1285},\n  year={2019}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html","title":"ABINet","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>ABINet: Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition Shancheng Fang and Hongtao Xie and Yuxin Wang and Zhendong Mao and Yongdong Zhang CVPR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link ABINet ResNet45 rec_r45_abinet.yml 90.75% pretrained &amp; trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r45_abinet.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r45_abinet.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r45_abinet.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r45_abinet.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_r45_abinet_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the ABINet text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r45_abinet.yml -o Global.pretrained_model=./rec_r45_abinet_train/best_accuracy  Global.save_inference_dir=./inference/rec_r45_abinet\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to ABINet in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_r45_abinet/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For ABINet text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_r45_abinet/' --rec_algorithm='ABINet' --rec_image_shape='3,32,128' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999995231628418)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#5-faq","title":"5. FAQ","text":"<ol> <li>Note that the MJSynth and SynthText datasets come from ABINet repo.</li> <li>We use the pre-trained model provided by the ABINet authors for finetune training.</li> </ol>"},{"location":"en/algorithm/text_recognition/algorithm_rec_abinet.html#citation","title":"Citation","text":"<pre><code>@article{Fang2021ABINet,\n  title     = {ABINet: Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition},\n  author    = {Shancheng Fang and Hongtao Xie and Yuxin Wang and Zhendong Mao and Yongdong Zhang},\n  booktitle = {CVPR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.06495},\n  pages     = {7098-7107}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html","title":"STAR-Net","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>STAR-Net: a spatial attention residue network for scene text recognition. Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong, Zhizhong Su and Junyu Han. BMVC, pages 43.1-43.13, 2016</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link --- --- --- --- --- StarNet Resnet34_vd 84.44% configs/rec/rec_r34_vd_tps_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b StarNet MobileNetV3 81.42% configs/rec/rec_mv3_tps_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_r34_vd_tps_bilstm_ctc.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the STAR-Net text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_starnet\n</code></pre> <p>For STAR-Net text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_starnet/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for STAR-Net:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_aster.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liu2016star,\n  title={STAR-Net: a spatial attention residue network for scene text recognition.},\n  author={Liu, Wei and Chen, Chaofeng and Wong, Kwan-Yee K and Su, Zhizhong and Han, Junyu},\n  booktitle={BMVC},\n  volume={2},\n  pages={7},\n  year={2016}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html","title":"CPPD","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Context Perception Parallel Decoder for Scene Text Recognition Yongkun Du and Zhineng Chen and Caiyan Jia and Xiaoting Yin and Chenxia Li and Yuning Du and Yu-Gang Jiang</p> <p>Scene text recognition models based on deep learning typically follow an Encoder-Decoder structure, where the decoder can be categorized into two types: (1) CTC and (2) Attention-based. Currently, most state-of-the-art (SOTA) models use an Attention-based decoder, which can be further divided into AR and PD types. In general, AR decoders achieve higher recognition accuracy than PD, while PD decoders are faster than AR. CPPD, with carefully designed CO and CC modules, achieves a balance between the accuracy of AR and the speed of PD.</p> <p>The accuracy (%) and model files of CPPD on the public dataset of scene text recognition are as follows:\uff1a</p> <ul> <li>English dataset from PARSeq.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg Download CPPD Tiny 97.1 94.4 96.6 86.6 88.5 90.3 92.25 en CPPD Base 98.2 95.5 97.6 87.9 90.0 92.7 93.80 en CPPD Base 48*160 97.5 95.5 97.7 87.7 92.4 93.7 94.10 en <ul> <li>Trained on Synth dataset(MJ+ST), Test on Union14M-L benchmark from U14m.</li> </ul> Model Curve Multi-Oriented Artistic Contextless Salient Multi-word General Avg Download CPPD Tiny 52.4 12.3 48.2 54.4 61.5 53.4 61.4 49.10 Same as the table above. CPPD Base 65.5 18.6 56.0 61.9 71.0 57.5 65.8 56.63 Same as the table above. CPPD Base 48*160 71.9 22.1 60.5 67.9 78.3 63.9 67.1 61.69 Same as the table above. <ul> <li>Trained on Union14M-L training dataset.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg Download CPPD Base 32*128 98.7 98.5 99.4 91.7 96.7 99.7 97.44 en Model Curve Multi-Oriented Artistic Contextless Salient Multi-word General Avg Download CPPD Base 32*128 87.5 70.7 78.2 82.9 85.5 85.4 84.3 82.08 Same as the table above. <ul> <li>Chinese dataset from Chinese Benckmark.</li> </ul> Model Scene Web Document Handwriting Avg Download CPPD Base 74.4 76.1 98.6 55.3 76.10 ch CPPD Base + STN 78.4 79.3 98.9 57.6 78.55 ch"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#dataset-preparation","title":"Dataset Preparation","text":"<p>English dataset download Union14M-Benchmark download Chinese dataset download</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_svtrnet_cppd_base_en.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_svtrnet_cppd_base_en.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#evaluation","title":"Evaluation","text":"<p>You can download the model files and configuration files provided by <code>CPPD</code>: download link, take <code>CPPD-B</code> as an example, using the following command to evaluate:</p> <pre><code># Download the tar archive containing the model files and configuration files of CPPD-B and extract it\nwget https://paddleocr.bj.bcebos.com/CCPD/rec_svtr_cppd_base_en_train.tar &amp;&amp; tar xf rec_svtr_cppd_base_en_train.tar\n# GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c ./rec_svtr_cppd_base_en_train/rec_svtrnet_cppd_base_en.yml -o Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#prediction","title":"Prediction","text":"<pre><code>python3 tools/infer_rec.py -c ./rec_svtr_cppd_base_en_train/rec_svtrnet_cppd_base_en.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CPPD text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code># export model\n# en\npython3 tools/export_model.py -c configs/rec/rec_svtrnet_cppd_base_en.yml -o Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model.pdparams Global.save_inference_dir=./rec_svtr_cppd_base_en_infer\n# ch\npython3 tools/export_model.py -c configs/rec/rec_svtrnet_cppd_base_ch.yml -o Global.pretrained_model=./rec_svtr_cppd_base_ch_train/best_model.pdparams Global.save_inference_dir=./rec_svtr_cppd_base_ch_infer\n\n# speed test\n# docker image https://hub.docker.com/r/paddlepaddle/paddle/tags/: sudo docker pull paddlepaddle/paddle:2.4.2-gpu-cuda11.2-cudnn8.2-trt8.0\n# install auto_log: pip install https://paddleocr.bj.bcebos.com/libs/auto_log-1.2.0-py3-none-any.whl\n# en\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_en_infer/' --rec_algorithm='CPPD' --rec_image_shape='3,32,100' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n# ch\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_ch_infer/' --rec_algorithm='CPPDPadding' --rec_image_shape='3,32,256' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n# stn_ch\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_stn_ch_infer/' --rec_algorithm='CPPD' --rec_image_shape='3,64,256' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n</code></pre> <p>Note: If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</p> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_svtr_cppd_base_en_infer/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_cppd.html#citation","title":"Citation","text":"<pre><code>@article{Du2023CPPD,\n  title     = {Context Perception Parallel Decoder for Scene Text Recognition},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {Arxiv},\n  year      = {2023},\n  url       = {https://arxiv.org/abs/2307.12270}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html","title":"CRNN","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</p> <p>Baoguang Shi, Xiang Bai, Cong Yao</p> <p>IEEE, 2015</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link --- --- --- --- --- CRNN Resnet34_vd 81.04% configs/rec/rec_r34_vd_none_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b CRNN MobileNetV3 77.95% configs/rec/rec_mv3_none_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CRNN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_none_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_crnn\n</code></pre> <p>For CRNN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_crnn/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for CRNN:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_crnn.html#citation","title":"Citation","text":"<pre><code>@ARTICLE{7801919,\n  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  title={An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},\n  year={2017},\n  volume={39},\n  number={11},\n  pages={2298-2304},\n  doi={10.1109/TPAMI.2016.2646371}}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html","title":"NRTR","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition Fenfen Sheng and Zhineng Chen and Bo Xu ICDAR, 2019</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link NRTR MTB rec_mtb_nrtr.yml 84.21% trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_mtb_nrtr.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_mtb_nrtr.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_mtb_nrtr.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_mtb_nrtr.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_mtb_nrtr_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the NRTR text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_mtb_nrtr.yml -o Global.pretrained_model=./rec_mtb_nrtr_train/best_accuracy  Global.save_inference_dir=./inference/rec_mtb_nrtr\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to NRTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_mtb_nrtr/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For NRTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_mtb_nrtr/' --rec_algorithm='NRTR' --rec_image_shape='1,32,100' --rec_char_dict_path='./ppocr/utils/EN_symbol_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9465042352676392)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#5-faq","title":"5. FAQ","text":"<ol> <li>In the <code>NRTR</code> paper, Beam search is used to decode characters, but the speed is slow. Beam search is not used by default here, and greedy search is used to decode characters.</li> </ol>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#6-release-note","title":"6. Release Note","text":"<ol> <li> <p>The release/2.6 version updates the NRTR code structure. The new version of NRTR can load the model parameters of the old version (release/2.5 and before), and you may use the following code to convert the old version model parameters to the new version model parameters:</p> <p> Click to expand <pre><code>params = paddle.load('path/' + '.pdparams') # the old version parameters\nstate_dict = model.state_dict() # the new version model parameters\nnew_state_dict = {}\n\nfor k1, v1 in state_dict.items():\n\n    k = k1\n    if 'encoder' in k and 'self_attn' in k and 'qkv' in k and 'weight' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')].transpose((1, 0, 2, 3))\n        k = params[k_para.replace('qkv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('qkv', 'conv3')].transpose((1, 0, 2, 3))\n\n        new_state_dict[k1] = np.concatenate([q[:, :, 0, 0], k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'encoder' in k and 'self_attn' in k and 'qkv' in k and 'bias' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')]\n        k = params[k_para.replace('qkv', 'conv2')]\n        v = params[k_para.replace('qkv', 'conv3')]\n\n        new_state_dict[k1] = np.concatenate([q, k, v], -1)\n\n    elif 'encoder' in k and 'self_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n    elif 'encoder' in k and 'norm3' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para.replace('norm3', 'norm2')]\n\n    elif 'encoder' in k and 'norm1' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n\n    elif 'decoder' in k and 'self_attn' in k and 'qkv' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')].transpose((1, 0, 2, 3))\n        k = params[k_para.replace('qkv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('qkv', 'conv3')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = np.concatenate([q[:, :, 0, 0], k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'decoder' in k and 'self_attn' in k and 'qkv' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')]\n        k = params[k_para.replace('qkv', 'conv2')]\n        v = params[k_para.replace('qkv', 'conv3')]\n        new_state_dict[k1] = np.concatenate([q, k, v], -1)\n\n    elif 'decoder' in k and 'self_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n    elif 'decoder' in k and 'cross_attn' in k and 'q' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        q = params[k_para.replace('q', 'conv1')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = q[:, :, 0, 0]\n\n    elif 'decoder' in k and 'cross_attn' in k and 'q' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        q = params[k_para.replace('q', 'conv1')]\n        new_state_dict[k1] = q\n\n    elif 'decoder' in k and 'cross_attn' in k and 'kv' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        k = params[k_para.replace('kv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('kv', 'conv3')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = np.concatenate([k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'decoder' in k and 'cross_attn' in k and 'kv' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        k = params[k_para.replace('kv', 'conv2')]\n        v = params[k_para.replace('kv', 'conv3')]\n        new_state_dict[k1] = np.concatenate([k, v], -1)\n\n    elif 'decoder' in k and 'cross_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        new_state_dict[k1] = params[k_para]\n    elif 'decoder' in k and 'norm' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n    elif 'mlp' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('fc', 'conv')\n        k_para = k_para.replace('mlp.', '')\n        w = params[k_para].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = w[:, :, 0, 0]\n    elif 'mlp' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('fc', 'conv')\n        k_para = k_para.replace('mlp.', '')\n        w = params[k_para]\n        new_state_dict[k1] = w\n\n    else:\n        new_state_dict[k1] = params[k1]\n\n    if list(new_state_dict[k1].shape) != list(v1.shape):\n        print(k1)\n\n\nfor k, v1 in state_dict.items():\n    if k not in new_state_dict.keys():\n        print(1, k)\n    elif list(new_state_dict[k].shape) != list(v1.shape):\n        print(2, k)\n\n\n\nmodel.set_state_dict(new_state_dict)\npaddle.save(model.state_dict(), 'nrtrnew_from_old_params.pdparams')\n</code></pre> <li> <p>The new version has a clean code structure and improved inference speed compared with the old version.</p> </li>"},{"location":"en/algorithm/text_recognition/algorithm_rec_nrtr.html#citation","title":"Citation","text":"<pre><code>@article{Sheng2019NRTR,\n  title     = {NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition},\n  author    = {Fenfen Sheng and Zhineng Chen and Bo Xu},\n  booktitle = {ICDAR},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1806.00926},\n  pages     = {781-786}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html","title":"PasreQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Scene Text Recognition with Permuted Autoregressive Sequence Models Darwin Bautista, Rowel Atienza ECCV, 2021</p> <p>Using real datasets (real) and synthetic datsets (synth) for training respectively\uff0cand evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets.</p> <ul> <li>The real datasets include COCO-Text, RCTW17, Uber-Text, ArT, LSVT, MLT19, ReCTS, TextOCR and OpenVINO datasets.</li> <li>The synthesis datasets include MJSynth and SynthText datasets.</li> </ul> <p>the algorithm reproduction effect is as follows:</p> Training Dataset Model Backbone config Acc Download link Synth ParseQ VIT rec_vit_parseq.yml 91.24% train model Real ParseQ VIT rec_vit_parseq.yml 94.74% train model"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_vit_parseq.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_vit_parseq.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SAR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model=./rec_vit_parseq_real/best_accuracy Global.save_inference_dir=./inference/rec_parseq\n</code></pre> <p>For SAR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_parseq/\" --rec_image_shape=\"3, 32, 128\" --rec_algorithm=\"ParseQ\" --rec_char_dict_path=\"ppocr/utils/dict/parseq_dict.txt\" --max_text_length=25 --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_parseq.html#citation","title":"Citation","text":"<pre><code>@InProceedings{bautista2022parseq,\n  title={Scene Text Recognition with Permuted Autoregressive Sequence Models},\n  author={Bautista, Darwin and Atienza, Rowel},\n  booktitle={European Conference on Computer Vision},\n  pages={178--196},\n  month={10},\n  year={2022},\n  publisher={Springer Nature Switzerland},\n  address={Cham},\n  doi={10.1007/978-3-031-19815-1_11},\n  url={https://doi.org/10.1007/978-3-031-19815-1_11}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html","title":"RARE","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>Robust Scene Text Recognition with Automatic Rectification Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai\u2217 CVPR, 2016</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Configuration Files Avg Accuracy Download Links RARE Resnet34_vd configs/rec/rec_r34_vd_tps_bilstm_att.yml 83.60% training model RARE MobileNetV3 configs/rec/rec_mv3_tps_bilstm_att.yml 82.50% trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#31-training","title":"3.1 Training","text":"<pre><code>#  Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml\n# Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#33-prediction","title":"3.3 Prediction","text":"<pre><code>python3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#4-inference","title":"4. Inference","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the RARE text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example (Model download address ), which can be converted using the following command:</p> <p>```bash linenums=\"1\" python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_att_v2.0_train/best_accuracy Global.save_inference_dir=./inference/rec_rare ````</p> <p>RARE text recognition model inference, you can execute the following commands:</p> <p>```bash linenums=\"1\" python3 tools/infer/predict_rec.py --image_dir=\"doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_rare/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path= \"./ppocr/utils/ic15_dict.txt\" ````</p> <p>The inference results are as follows:</p> <p></p> <p><code>text linenums=\"1\" Predicts of doc/imgs_words/en/word_1.png:('joint ', 0.9999969601631165)</code></p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not currently supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#43-serving","title":"4.3 Serving","text":"<p>Not currently supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#44-more","title":"4.4 More","text":"<p>The RARE model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rare.html#citation","title":"Citation","text":"<p><code>bibtex @inproceedings{2016Robust,   title={Robust Scene Text Recognition with Automatic Rectification},   author={ Shi, B. and Wang, X. and Lyu, P. and Cong, Y. and Xiang, B. },   booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2016}, }</code></p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html","title":"RFL","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition Hui Jiang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Wenqi Ren, Fei Wu, and Wenming Tan ICDAR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link RFL-CNT ResNetRFL rec_resnet_rfl_visual.yml 93.40% \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b RFL-Att ResNetRFL rec_resnet_rfl_att.yml 88.63% \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#step1:train the CNT branch\n# Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_rfl_visual.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_resnet_rfl_visual.yml\n\n#step2:joint training of CNT and Att branches\n# Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_resnet_rfl_att.yml  -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the RFL text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_resnet_rfl_att\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to NRTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_resnet_rfl_att/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For RFL text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_resnet_rfl_att/' --rec_algorithm='RFL' --rec_image_shape='1,32,100'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999927282333374)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rfl.html#citation","title":"Citation","text":"<pre><code>@article{2021Reciprocal,\n  title     = {Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition},\n  author    = {Jiang, H.  and  Xu, Y.  and  Cheng, Z.  and  Pu, S.  and  Niu, Y.  and  Ren, W.  and  Wu, F.  and  Tan, W. },\n  booktitle = {ICDAR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.06229}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html","title":"RobustScanner","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun, Wayne Zhang ECCV, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link RobustScanner ResNet31 rec_r31_robustscanner.yml 87.77% trained model <p>Note:In addition to using the two text recognition datasets MJSynth and SynthText, SynthAdd data (extraction code: 627x), and some real data are used in training, the specific data details can refer to the paper.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r31_robustscanner.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r31_robustscanner.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the RobustScanner text recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_r31_robustscanner\n</code></pre> <p>For RobustScanner text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_r31_robustscanner/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"RobustScanner\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_robustscanner.html#citation","title":"Citation","text":"<pre><code>@article{2020RobustScanner,\n  title={RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition},\n  author={Xiaoyu Yue and Zhanghui Kuang and Chenhao Lin and Hongbin Sun and Wayne Zhang},\n  journal={ECCV2020},\n  year={2020},\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html","title":"Rosetta","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>Rosetta: Large Scale System for Text Detection and Recognition in Images Borisyuk F , Gordo A , V Sivakumar KDD, 2018</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Configuration Files Avg Accuracy Download Links Rosetta Resnet34_vd configs/rec/rec_r34_vd_none_none_ctc.yml 79.11% training model Rosetta MobileNetV3 configs/rec/rec_mv3_none_none_ctc.yml 75.80% training model"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#31-training","title":"3.1 Training","text":"<pre><code># Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_none_none_ctc.yml\n# Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_r34_vd_none_none_ctc.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#33-prediction","title":"3.3 Prediction","text":"<pre><code>python3 tools/infer_rec.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the Rosetta text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example ( Model download address ), which can be converted using the following command:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model=./rec_r34_vd_none_none_ctc_v2.0_train/best_accuracy Global.save_inference_dir=./inference/rec_rosetta\n</code></pre> <p>Rosetta text recognition model inference, you can execute the following commands:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_rosetta/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path= \"./ppocr/utils/ic15_dict.txt\"\n</code></pre> <p>The inference results are as follows:</p> <p></p> <pre><code>Predicts of doc/imgs_words/en/word_1.png:('joint', 0.9999982714653015)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not currently supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#43-serving","title":"4.3 Serving","text":"<p>Not currently supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#44-more","title":"4.4 More","text":"<p>The Rosetta model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_rosetta.html#citation","title":"Citation","text":"<pre><code>@inproceedings{2018Rosetta,\n  title={Rosetta: Large Scale System for Text Detection and Recognition in Images},\n  author={ Borisyuk, Fedor and Gordo, Albert and Sivakumar, Viswanath },\n  booktitle={the 24th ACM SIGKDD International Conference},\n  year={2018},\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html","title":"SAR","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition Hui Li, Peng Wang, Chunhua Shen, Guyu Zhang AAAI, 2019</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SAR ResNet31 rec_r31_sar.yml 87.20% train model <p>Note:In addition to using the two text recognition datasets MJSynth and SynthText, SynthAdd data (extraction code: 627x), and some real data are used in training, the specific data details can refer to the paper.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r31_sar.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r31_sar.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SAR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model=./rec_r31_sar_train/best_accuracy  Global.save_inference_dir=./inference/rec_sar\n</code></pre> <p>For SAR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_sar/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"SAR\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --max_text_length=30 --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_sar.html#citation","title":"Citation","text":"<pre><code>@article{Li2019ShowAA,\n  title={Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition},\n  author={Hui Li and Peng Wang and Chunhua Shen and Guyu Zhang},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1811.00751}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html","title":"SATRN","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#1-introduction","title":"1. Introduction","text":"<p>\u200b\u8bba\u6587\u200b\u4fe1\u606f\u200b\uff1a</p> <p>On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention Junyeop Lee, Sungrae Park, Jeonghun Baek, Seong Joon Oh, Seonghyeon Kim, Hwalsuk Lee CVPR, 2020 Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SATRN ShallowCNN 88.05% configs/rec/rec_satrn.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_satrn.yml\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_satrn.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SATRN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model=./rec_satrn_train/best_accuracy  Global.save_inference_dir=./inference/rec_satrn\n</code></pre> <p>For SATRN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_satrn/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"SATRN\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --max_text_length=30 --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_satrn.html#citation","title":"Citation","text":"<pre><code>@article{lee2019recognizing,\n      title={On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention},\n      author={Junyeop Lee and Sungrae Park and Jeonghun Baek and Seong Joon Oh and Seonghyeon Kim and Hwalsuk Lee},\n      year={2019},\n      eprint={1910.04396},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html","title":"SEED","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</p> <p>Qiao, Zhi and Zhou, Yu and Yang, Dongbao and Zhou, Yucan and Wang, Weiping</p> <p>CVPR, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link SEED Aster_Resnet 85.20% configs/rec/rec_resnet_stn_bilstm_att.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#training","title":"Training","text":"<p>The SEED model needs to additionally load the language model trained by FastText, and install the fasttext dependencies:</p> <pre><code>python3 -m pip install fasttext==0.9.1\n</code></pre> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_stn_bilstm_att.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_resnet_stn_bilstm_att.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_resnet_stn_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_resnet_stn_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Not support</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not support</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#43-serving","title":"4.3 Serving","text":"<p>Not support</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#44-more","title":"4.4 More","text":"<p>Not support</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_seed.html#citation","title":"Citation","text":"<pre><code>@inproceedings{qiao2020seed,\n  title={Seed: Semantics enhanced encoder-decoder framework for scene text recognition},\n  author={Qiao, Zhi and Zhou, Yu and Yang, Dongbao and Zhou, Yucan and Wang, Weiping},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={13528--13537},\n  year={2020}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html","title":"SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou AAAI, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets. The algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SPIN ResNet32 rec_r32_gaspin_bilstm_att.yml 90.00% trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SPIN text recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_r32_gaspin_bilstm_att\n</code></pre> <p>For SPIN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_r32_gaspin_bilstm_att/\" --rec_image_shape=\"3, 32, 100\" --rec_algorithm=\"SPIN\" --rec_char_dict_path=\"/ppocr/utils/dict/spin_dict.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_spin.html#citation","title":"Citation","text":"<pre><code>@article{2020SPIN,\n  title={SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition},\n  author={Chengwei Zhang and Yunlu Xu and Zhanzhan Cheng and Shiliang Pu and Yi Niu and Fei Wu and Futai Zou},\n  journal={AAAI2020},\n  year={2020},\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html","title":"SRN","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Towards Accurate Scene Text Recognition with Semantic Reasoning Networks Deli Yu, Xuan Li, Chengquan Zhang, Junyu Han, Jingtuo Liu, Errui Ding CVPR,2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SRN Resnet50_vd_fpn rec_r50_fpn_srn.yml 86.31% train model"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r50_fpn_srn.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r50_fpn_srn.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SRN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model=./rec_r50_vd_srn_train/best_accuracy  Global.save_inference_dir=./inference/rec_srn\n</code></pre> <p>For SRN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_srn/\" --rec_image_shape=\"1,64,256\" --rec_char_type=\"ch\" --rec_algorithm=\"SRN\" --rec_char_dict_path=\"ppocr/utils/ic15_dict.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_srn.html#citation","title":"Citation","text":"<pre><code>@article{Yu2020TowardsAS,\n  title={Towards Accurate Scene Text Recognition With Semantic Reasoning Networks},\n  author={Deli Yu and Xuan Li and Chengquan Zhang and Junyu Han and Jingtuo Liu and Errui Ding},\n  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2020},\n  pages={12110-12119}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html","title":"STAR-Net","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>STAR-Net: a spatial attention residue network for scene text recognition. Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong, Zhizhong Su and Junyu Han. BMVC, pages 43.1-43.13, 2016</p> <p>Refer to DTRB text Recognition Training and Evaluation Process . Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Avg Accuracy Configuration Files Download Links StarNet Resnet34_vd 84.44% configs/rec/rec_r34_vd_tps_bilstm_ctc.yml trained model StarNet MobileNetV3 81.42% configs/rec/rec_mv3_tps_bilstm_ctc.yml trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#31-training","title":"3.1 Training","text":"<p>After the data preparation is complete, the training can be started. The training command is as follows:</p> <pre><code>#  Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml # Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_r34_vd_tps_bilstm_ctc.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#33-prediction","title":"3.3 Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#4-inference","title":"4. Inference","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the STAR-Net text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example Model download address , which can be converted using the following command:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_starnet\n</code></pre> <p>STAR-Net text recognition model inference, you can execute the following commands:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_starnet/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre> <p></p> <p>The inference results are as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_336.png:('super', 0.9999073)\n</code></pre> <p>Attention Since the above model refers to the DTRB text recognition training and evaluation process, it is different from the ultra-lightweight Chinese recognition model training in two aspects:</p> <ul> <li> <p>The image resolutions used during training are different. The image resolutions used for training the above models are [3, 32, 100], while for Chinese model training, in order to ensure the recognition effect of long texts, the image resolutions used during training are [ 3, 32, 320]. The default shape parameter of the predictive inference program is the image resolution used for training Chinese, i.e. [3, 32, 320]. Therefore, when inferring the above English model here, it is necessary to set the shape of the recognized image through the parameter rec_image_shape.</p> </li> <li> <p>Character list, the experiment in the DTRB paper is only for 26 lowercase English letters and 10 numbers, a total of 36 characters. All uppercase and lowercase characters are converted to lowercase characters, and characters not listed above are ignored and considered spaces. Therefore, there is no input character dictionary here, but a dictionary is generated by the following command. Therefore, the parameter rec_char_dict_path needs to be set during inference, which is specified as an English dictionary \"./ppocr/utils/ic15_dict.txt\".</p> </li> </ul> <pre><code>self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\ndict_character = list(self.character_str)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>After preparing the inference model, refer to the cpp infer tutorial to operate.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#43-serving","title":"4.3 Serving","text":"<p>After preparing the inference model, refer to the pdserving tutorial for Serving deployment, including two modes: Python Serving and C++ Serving.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#44-more","title":"4.4 More","text":"<p>The STAR-Net model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_starnet.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liu2016star,\n  title={STAR-Net: a spatial attention residue network for scene text recognition.},\n  author={Liu, Wei and Chen, Chaofeng and Wong, Kwan-Yee K and Su, Zhizhong and Han, Junyu},\n  booktitle={BMVC},\n  volume={2},\n  pages={7},\n  year={2016}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html","title":"SVTR","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SVTR: Scene Text Recognition with a Single Visual Model Yongkun Du and Zhineng Chen and Caiyan Jia Xiaoting Yin and Tianlun Zheng and Chenxia Li and Yuning Du and Yu-Gang Jiang IJCAI, 2022</p> <p>The accuracy (%) and model files of SVTR on the public dataset of scene text recognition are as follows:</p> <ul> <li>Chinese dataset from Chinese Benckmark , and the Chinese training evaluation strategy of SVTR follows the paper.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg_6 IC152077 IC131015 IC03867 IC03860 Avg_10 Chinesescene_test Download link SVTR Tiny 96.85 91.34 94.53 83.99 85.43 89.24 90.87 80.55 95.37 95.27 95.70 90.13 67.90 English  / Chinese SVTR Small 95.92 93.04 95.03 84.70 87.91 92.01 91.63 82.72 94.88 96.08 96.28 91.02 69.00 English / Chinese SVTR Base 97.08 91.50 96.03 85.20 89.92 91.67 92.33 83.73 95.66 95.62 95.81 91.61 71.40 English  /                                              - SVTR Large 97.20 91.65 96.30 86.58 88.37 95.14 92.82 84.54 96.35 96.54 96.74 92.24 72.10 English / Chinese"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#dataset-preparation","title":"Dataset Preparation","text":"<p>English dataset download Chinese dataset download</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_svtrnet.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_svtrnet.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#evaluation","title":"Evaluation","text":"<p>You can download the model files and configuration files provided by <code>SVTR</code>: download link, take <code>SVTR-T</code> as an example, using the following command to evaluate:</p> <pre><code># Download the tar archive containing the model files and configuration files of SVTR-T and extract it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/rec_svtr_tiny_none_ctc_en_train.tar &amp;&amp; tar xf rec_svtr_tiny_none_ctc_en_train.tar\n# GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c ./rec_svtr_tiny_none_ctc_en_train/rec_svtr_tiny_6local_6global_stn_en.yml -o Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#prediction","title":"Prediction","text":"<pre><code>python3 tools/infer_rec.py -c ./rec_svtr_tiny_none_ctc_en_train/rec_svtr_tiny_6local_6global_stn_en.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SVTR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_svtrnet.yml -o Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy  Global.save_inference_dir=./inference/rec_svtr_tiny_stn_en\n</code></pre> <p>Note: If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</p> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_svtr_tiny_stn_en/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For SVTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_svtr_tiny_stn_en/' --rec_algorithm='SVTR' --rec_image_shape='3,64,256' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999998807907104)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#5-faq","title":"5. FAQ","text":"<ul> <li> <ol> <li>Speed situation on CPU and GPU</li> </ol> </li> <li>Since most of the operators used by <code>SVTR</code> are matrix multiplication, in the GPU environment, the speed has an advantage, but in the environment where mkldnn is enabled on the CPU, <code>SVTR</code> has no advantage over the optimized convolutional network.</li> <li> <ol> <li>SVTR model convert to ONNX failed</li> </ol> </li> <li>Ensure <code>paddle2onnx</code> and <code>onnxruntime</code> versions are up to date, refer to SVTR model to onnx step-by-step example for the convert onnx command. 1271214273).</li> <li> <ol> <li>SVTR model convert to ONNX is successful but the inference result is incorrect</li> </ol> </li> <li>The possible reason is that the model parameter <code>out_char_num</code> is not set correctly, it should be set to W//4, W//8 or W//12, please refer to Section 3.3.3 of SVTR, a high-precision Chinese scene text recognition model projectdetail/5073182?contributionType=1).</li> <li> <ol> <li>Optimization of long text recognition</li> </ol> </li> <li>Refer to Section 3.3 of SVTR, a high-precision Chinese scene text recognition model.</li> <li> <ol> <li>Notes on the reproduction of the paper results</li> </ol> </li> <li>Dataset using provided by ABINet.</li> <li>By default, 4 cards of GPUs are used for training, the default Batchsize of a single card is 512, and the total Batchsize is 2048, corresponding to a learning rate of 0.0005. When modifying the Batchsize or changing the number of GPU cards, the learning rate should be modified in equal proportion.</li> <li> <ol> <li>Exploration Directions for further optimization</li> </ol> </li> <li>Learning rate adjustment: adjusting to twice the default to keep Batchsize unchanged; or reducing Batchsize to 1/2 the default to keep the learning rate unchanged.</li> <li>Data augmentation strategies: optionally <code>RecConAug</code> and <code>RecAug</code>.</li> <li>If STN is not used, <code>Local</code> of <code>mixer</code> can be replaced by <code>Conv</code> and <code>local_mixer</code> can all be modified to <code>[5, 5]</code>.</li> <li>Grid search for optimal <code>embed_dim</code>, <code>depth</code>, <code>num_heads</code> configurations.</li> <li>Use the <code>Post-Normalization strategy</code>, which is to modify the model configuration <code>prenorm</code> to <code>True</code>.</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtr.html#citation","title":"Citation","text":"<pre><code>@article{Du2022SVTR,\n  title     = {SVTR: Scene Text Recognition with a Single Visual Model},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Zheng, Tianlun and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {IJCAI},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2205.00159}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html","title":"\u573a\u666f\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-SVTRv2","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#svtrv2","title":"SVTRv2\u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p>PaddleOCR \u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e00\u200b\uff1aOCR \u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u6392\u884c\u699c\u200b\u7b2c\u4e00\u200b\u7b97\u6cd5\u200b\u3002\u200b\u4e3b\u8981\u200b\u601d\u8def\u200b\uff1a1\u3001\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200bBackbone\u200b\u5347\u7ea7\u200b\u4e3a\u200bRepSVTR\uff1b2\u3001\u200b\u8bc6\u522b\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u5347\u7ea7\u200b\u4e3a\u200bSVTRv2\uff0c\u200b\u53ef\u200b\u8bc6\u522b\u200b\u957f\u200b\u6587\u672c\u200b\u3002</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#31","title":"3.1 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>#\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff08\u200b\u8bad\u7ec3\u200b\u5468\u671f\u957f\u200b\uff0c\u200b\u4e0d\u200b\u5efa\u8bae\u200b\uff09\npython3 tools/train.py -c configs/rec/SVTRv2/rec_repsvtr_gtc.yml\n\n# \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u8fc7\u200b--gpus\u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\n# Rec \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/rec_repsvtr_gtc.yml\n# Rec \u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/rec_svtrv2_gtc.yml\n# Rec \u200b\u84b8\u998f\u200b\u8bad\u7ec3\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/rec_svtrv2_gtc_distill.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#32","title":"3.2 \u200b\u8bc4\u4f30","text":"<pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/SVTRv2/rec_repsvtr_gtc.yml -o Global.pretrained_model=output/rec_repsvtr_gtc/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#33","title":"3.3 \u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/infer_rec.py -c tools/eval.py -c configs/rec/SVTRv2/rec_repsvtr_gtc.yml -o Global.pretrained_model=output/rec_repsvtr_gtc/best_accuracy Global.infer_img='./doc/imgs_words_en/word_10.png'\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200binfer_img\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b Global.infer_img='./doc/imgs_words_en/'\u3002\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#4","title":"4. \u200b\u63a8\u7406\u200b\u90e8\u7f72","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#41-python","title":"4.1 Python\u200b\u63a8\u7406","text":"<p>\u200b\u9996\u5148\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200bbest\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f6c\u6362\u6210\u200binference model\uff0c\u200b\u4ee5\u200bRepSVTR\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/export_model.py -c configs/rec/SVTRv2/rec_repsvtr_gtc.yml -o Global.pretrained_model=output/rec_repsvtr_gtc/best_accuracy Global.save_inference_dir=./inference/rec_repsvtr_infer\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8c03\u6574\u200b\u4e86\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>character_dict_path</code>\u200b\u662f\u5426\u200b\u4e3a\u200b\u6240\u200b\u6b63\u786e\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>./inference/rec_repsvtr_infer/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\uff1a</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_repsvtr_infer/'\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200bimage_dir\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b --image_dir='./doc/imgs_words_en/'\u3002\n</code></pre> <p></p> <p>\u200b\u6267\u884c\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u4e0a\u9762\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u8bc6\u522b\u200b\u7684\u200b\u6587\u672c\u200b\u548c\u200b\u5f97\u5206\u200b\uff09\u200b\u4f1a\u200b\u6253\u5370\u200b\u5230\u200b\u5c4f\u5e55\u200b\u4e0a\u200b\uff0c\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a \u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999998807907104)\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ul> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u8c03\u6574\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u8f93\u5165\u200b\u5206\u8fa8\u7387\u200b\uff0c\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u53c2\u6570\u200b<code>rec_image_shape</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u60a8\u200b\u9700\u8981\u200b\u7684\u200b\u8bc6\u522b\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u3002</li> <li>\u200b\u5728\u200b\u63a8\u7406\u200b\u65f6\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u53c2\u6570\u200b<code>rec_char_dict_path</code>\u200b\u6307\u5b9a\u200b\u5b57\u5178\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u4fee\u6539\u200b\u4e86\u200b\u5b57\u5178\u200b\uff0c\u200b\u8bf7\u200b\u4fee\u6539\u200b\u8be5\u200b\u53c2\u6570\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u4fee\u6539\u200b\u4e86\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u9700\u200b\u4fee\u6539\u200b<code>tools/infer/predict_rec.py</code>\u200b\u4e2d\u200bSVTR\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\u3002</li> </ul>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#42-c","title":"4.2 C++\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u7531\u4e8e\u200bC++\u200b\u9884\u5904\u7406\u200b\u540e\u5904\u7406\u200b\u8fd8\u200b\u672a\u200b\u652f\u6301\u200bSVTRv2</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#43-serving","title":"4.3 Serving\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#44","title":"4.4 \u200b\u66f4\u200b\u591a\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#5-faq","title":"5. FAQ","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_svtrv2.html#_1","title":"\u5f15\u7528","text":"<pre><code>@article{Du2022SVTR,\n  title     = {SVTR: Scene Text Recognition with a Single Visual Model},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Zheng, Tianlun and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {IJCAI},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2205.00159}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html","title":"VisionLAN","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, Yongdong Zhang ICCV, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link VisionLAN ResNet45 rec_r45_visionlan.yml 90.30% model link"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r45_visionlan.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r45_visionlan.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 tools/eval.py -c configs/rec/rec_r45_visionlan.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r45_visionlan.yml -o Global.infer_img='./doc/imgs_words/en/word_2.png' Global.pretrained_model=./rec_r45_visionlan_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the VisionLAN text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r45_visionlan.yml -o Global.pretrained_model=./rec_r45_visionlan_train/best_accuracy Global.save_inference_dir=./inference/rec_r45_visionlan/\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to VisionLAN in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>./inference/rec_r45_visionlan/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For VisionLAN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words/en/word_2.png' --rec_model_dir='./inference/rec_r45_visionlan/' --rec_algorithm='VisionLAN' --rec_image_shape='3,64,256' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt' --use_space_char=False\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words/en/word_2.png:('yourself', 0.9999493)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#5-faq","title":"5. FAQ","text":"<ol> <li>Note that the MJSynth and SynthText datasets come from VisionLAN repo.</li> <li>We use the pre-trained model provided by the VisionLAN authors for finetune training. The dictionary for the pre-trained model is 'ppocr/utils/ic15_dict.txt'.</li> </ol>"},{"location":"en/algorithm/text_recognition/algorithm_rec_visionlan.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2021two,\n  title={From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network},\n  author={Wang, Yuxin and Xie, Hongtao and Fang, Shancheng and Wang, Jing and Zhu, Shenggao and Zhang, Yongdong},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={14194--14203},\n  year={2021}\n}\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html","title":"ViTSTR","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Vision Transformer for Fast and Efficient Scene Text Recognition Rowel Atienza ICDAR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link ViTSTR ViTSTR rec_vitstr_none_ce.yml 79.82% trained model"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_vitstr_none_ce.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_vitstr_none_ce.yml\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the ViTSTR text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy  Global.save_inference_dir=./inference/rec_vitstr\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to ViTSTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_vitstr/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For ViTSTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_vitstr/' --rec_algorithm='ViTSTR' --rec_image_shape='1,224,224' --rec_char_dict_path='./ppocr/utils/EN_symbol_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9998350143432617)\n</code></pre>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#5-faq","title":"5. FAQ","text":"<ol> <li>In the <code>ViTSTR</code> paper, using pre-trained weights on ImageNet1k for initial training, we did not use pre-trained weights in training, and the final accuracy did not change or even improved.</li> </ol>"},{"location":"en/algorithm/text_recognition/algorithm_rec_vitstr.html#citation","title":"Citation","text":"<pre><code>@article{Atienza2021ViTSTR,\n  title     = {Vision Transformer for Fast and Efficient Scene Text Recognition},\n  author    = {Rowel Atienza},\n  booktitle = {ICDAR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.08582}\n}\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html","title":"\u57fa\u4e8e\u200bPP-OCRv3\u200b\u7684\u200bPCB\u200b\u5b57\u7b26\u8bc6\u522b","text":""},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u4ecb\u7ecd","text":"<p>\u200b\u5370\u5237\u200b\u7535\u8def\u677f\u200b(PCB)\u200b\u662f\u200b\u7535\u5b50\u4ea7\u54c1\u200b\u4e2d\u200b\u7684\u200b\u6838\u5fc3\u200b\u5668\u4ef6\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u677f\u4ef6\u200b\u8d28\u91cf\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u4e0e\u200b\u76d1\u63a7\u200b\u662f\u200b\u751f\u4ea7\u200b\u4e2d\u200b\u5fc5\u4e0d\u53ef\u5c11\u200b\u7684\u200b\u73af\u8282\u200b\u3002\u200b\u5728\u200b\u4e00\u4e9b\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200bPCB\u200b\u4e2d\u200b\u4fe1\u53f7\u706f\u200b\u989c\u8272\u200b\u548c\u200b\u6587\u5b57\u200b\u7ec4\u5408\u200b\u53ef\u4ee5\u200b\u5b9a\u4f4d\u200bPCB\u200b\u5c40\u90e8\u200b\u6a21\u5757\u200b\u8d28\u91cf\u200b\u95ee\u9898\u200b\uff0cPCB\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u4e2d\u200b\u5b58\u5728\u200b\u5982\u4e0b\u200b\u96be\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u88c1\u526a\u200b\u51fa\u200b\u7684\u200bPCB\u200b\u56fe\u7247\u200b\u5bbd\u9ad8\u200b\u6bd4\u4f8b\u200b\u8f83\u200b\u5c0f\u200b</li> <li>\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u6574\u4f53\u200b\u9762\u79ef\u200b\u4e5f\u200b\u8f83\u200b\u5c0f\u200b</li> <li>\u200b\u5305\u542b\u200b\u5782\u76f4\u200b\u3001\u200b\u6c34\u5e73\u200b\u591a\u79cd\u200b\u65b9\u5411\u200b\u6587\u672c\u200b</li> </ul> <p>\u200b\u9488\u5bf9\u200b\u672c\u200b\u573a\u666f\u200b\uff0cPaddleOCR\u200b\u57fa\u4e8e\u200b\u5168\u65b0\u200b\u7684\u200bPP-OCRv3\u200b\u901a\u8fc7\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u3001\u200b\u5fae\u8c03\u200b\u4ee5\u53ca\u200b\u5176\u4ed6\u200b\u573a\u666f\u200b\u9002\u914d\u200b\u65b9\u6cd5\u200b\u5b8c\u6210\u200b\u5c0f\u200b\u5b57\u7b26\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6ee1\u8db3\u200b\u4f01\u4e1a\u200b\u4e0a\u7ebf\u200b\u8981\u6c42\u200b\u3002PCB\u200b\u68c0\u6d4b\u200b\u3001\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\u5982\u200b \u200b\u56fe\u200b1 \u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u6b22\u8fce\u200b\u5728\u200bAIStudio\u200b\u9886\u53d6\u200b\u514d\u8d39\u200b\u7b97\u529b\u200b\u4f53\u9a8c\u200b\u7ebf\u4e0a\u200b\u5b9e\u8bad\u200b\uff0c\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b: \u200b\u57fa\u4e8e\u200bPP-OCRv3\u200b\u5b9e\u73b0\u200bPCB\u200b\u5b57\u7b26\u8bc6\u522b\u200b</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u5b89\u88c5\u200b\u8bf4\u660e","text":"<p>\u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u6e90\u7801\u200b\uff0c\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u73af\u5883\u200b\u3002</p> <pre><code># \u200b\u5982\u200b\u4ecd\u200b\u9700\u200b\u5b89\u88c5\u200bor\u200b\u5b89\u88c5\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6267\u884c\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\ngit clone https://github.com/PaddlePaddle/PaddleOCR.git\n#  git clone https://gitee.com/PaddlePaddle/PaddleOCR\n</code></pre> <pre><code># \u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5305\u200b\npip install -r /home/aistudio/PaddleOCR/requirements.txt\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u56fe\u7247\u200b\u5408\u6210\u200b\u5de5\u5177\u200b\u751f\u6210\u200b \u200b\u56fe\u200b2 \u200b\u6240\u793a\u200b\u7684\u200bPCB\u200b\u56fe\u7247\u200b\uff0c\u200b\u6574\u56fe\u200b\u53ea\u6709\u200b\u9ad8\u200b25\u3001\u200b\u5bbd\u200b150\u200b\u5de6\u53f3\u200b\u3001\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u9ad8\u200b9\u3001\u200b\u5bbd\u200b45\u200b\u5de6\u53f3\u200b\uff0c\u200b\u5305\u542b\u200b\u5782\u76f4\u200b\u548c\u200b\u6c34\u5e73\u200b2\u200b\u79cd\u200b\u65b9\u5411\u200b\u7684\u200b\u6587\u672c\u200b\uff1a</p> <p></p> <p>\u200b\u6682\u65f6\u200b\u4e0d\u200b\u5f00\u6e90\u200b\u751f\u6210\u200b\u7684\u200bPCB\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4f46\u662f\u200b\u901a\u8fc7\u200b\u66f4\u6362\u200b\u80cc\u666f\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u751f\u6210\u200b\u6570\u636e\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>cd gen_data\npython3 gen.py --num_img=10\n</code></pre> <p>\u200b\u751f\u6210\u200b\u56fe\u7247\u200b\u53c2\u6570\u200b\u89e3\u91ca\u200b\uff1a</p> <pre><code>num_img\uff1a\u200b\u751f\u6210\u200b\u56fe\u7247\u200b\u6570\u91cf\u200b\nfont_min_size\u3001font_max_size\uff1a\u200b\u5b57\u4f53\u200b\u6700\u5927\u200b\u3001\u200b\u6700\u5c0f\u200b\u5c3a\u5bf8\u200b\nbg_path\uff1a\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u80cc\u666f\u200b\u5b58\u653e\u200b\u8def\u5f84\u200b\ndet_bg_path\uff1a\u200b\u6574\u56fe\u200b\u80cc\u666f\u200b\u5b58\u653e\u200b\u8def\u5f84\u200b\nfonts_path\uff1a\u200b\u5b57\u4f53\u200b\u8def\u5f84\u200b\ncorpus_path\uff1a\u200b\u8bed\u6599\u200b\u8def\u5f84\u200b\noutput_dir\uff1a\u200b\u751f\u6210\u200b\u56fe\u7247\u200b\u5b58\u50a8\u200b\u8def\u5f84\u200b\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u751f\u6210\u200b 100\u200b\u5f20\u200b \u200b\u76f8\u540c\u200b\u5c3a\u5bf8\u200b\u548c\u200b\u6587\u672c\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u5982\u200b  \u200b\u56fe\u200b3 \u200b\u6240\u793a\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u5927\u5bb6\u200b\u8dd1\u901a\u200b\u5b9e\u9a8c\u200b\u3002\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u89e3\u538b\u200b\u6570\u636e\u200b\u96c6\u200b\uff1a</p> <p></p> <pre><code>tar xf ./data/data148165/dataset.tar -C ./\n</code></pre> <p>\u200b\u5728\u200b\u751f\u6210\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u65f6\u200b\u9700\u8981\u200b\u751f\u6210\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u8bad\u7ec3\u200b\u9700\u6c42\u200b\u7684\u200b\u683c\u5f0f\u200b\uff1a</p> <ul> <li>\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b</li> </ul> <p>\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u7528\u200b'\\t'\u200b\u5206\u9694\u200b\uff1a</p> <pre><code>\" \u200b\u56fe\u50cf\u200b\u6587\u4ef6\u540d\u200b                    json.dumps\u200b\u7f16\u7801\u200b\u7684\u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\"\nch4_test_images/img_61.jpg    [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> <p>json.dumps\u200b\u7f16\u7801\u200b\u524d\u200b\u7684\u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\u662f\u200b\u5305\u542b\u200b\u591a\u4e2a\u200b\u5b57\u5178\u200b\u7684\u200blist\uff0c\u200b\u5b57\u5178\u200b\u4e2d\u200b\u7684\u200b <code>points</code> \u200b\u8868\u793a\u200b\u6587\u672c\u6846\u200b\u7684\u200b\u56db\u4e2a\u200b\u70b9\u200b\u7684\u200b\u5750\u6807\u200b(x, y)\uff0c\u200b\u4ece\u200b\u5de6\u4e0a\u89d2\u200b\u7684\u200b\u70b9\u200b\u5f00\u59cb\u200b\u987a\u65f6\u9488\u200b\u6392\u5217\u200b\u3002 <code>transcription</code> \u200b\u8868\u793a\u200b\u5f53\u524d\u200b\u6587\u672c\u6846\u200b\u7684\u200b\u6587\u5b57\u200b\uff0c\u200b\u5f53\u5176\u200b\u5185\u5bb9\u200b\u4e3a\u200b\u201c###\u201d\u200b\u65f6\u200b\uff0c\u200b\u8868\u793a\u200b\u8be5\u200b\u6587\u672c\u6846\u200b\u65e0\u6548\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u4f1a\u200b\u8df3\u8fc7\u200b\u3002</p> <ul> <li>\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b</li> </ul> <p>\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u7684\u200b\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff0c txt\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u9ed8\u8ba4\u200b\u8bf7\u200b\u5c06\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\u548c\u200b\u56fe\u7247\u200b\u6807\u7b7e\u200b\u7528\u200b'\\t'\u200b\u5206\u5272\u200b\uff0c\u200b\u5982\u7528\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u5206\u5272\u200b\u5c06\u200b\u9020\u6210\u200b\u8bad\u7ec3\u200b\u62a5\u9519\u200b\u3002</p> <pre><code>\" \u200b\u56fe\u50cf\u200b\u6587\u4ef6\u540d\u200b                 \u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b \"\n\ntrain_data/rec/train/word_001.jpg   \u200b\u7b80\u5355\u200b\u53ef\u200b\u4f9d\u8d56\u200b\ntrain_data/rec/train/word_002.jpg   \u200b\u7528\u200b\u79d1\u6280\u200b\u8ba9\u200b\u590d\u6742\u200b\u7684\u200b\u4e16\u754c\u200b\u66f4\u200b\u7b80\u5355\u200b\n...\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#4","title":"4. \u200b\u6587\u672c\u200b\u68c0\u6d4b","text":"<p>\u200b\u9009\u7528\u200b\u98de\u6868\u200bOCR\u200b\u5f00\u53d1\u200b\u5957\u4ef6\u200bPaddleOCR\u200b\u4e2d\u200b\u7684\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u3002\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5171\u8ba1\u200b9\u200b\u4e2a\u200b\u65b9\u9762\u200b\u7684\u200b\u5347\u7ea7\u200b\uff1a</p> <ul> <li> <p>PP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5bf9\u200bPP-OCRv2\u200b\u4e2d\u200b\u7684\u200bCML\u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\uff0c\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u6548\u679c\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u5728\u200b\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200bLK-PAN\u200b\u548c\u200b\u5f15\u5165\u200b\u4e86\u200bDML\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff1b\u200b\u5728\u200b\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200bRSE-FPN\u3002</p> </li> <li> <p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002PP-OCRv3\u200b\u901a\u8fc7\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200bSVTR_LCNet\u3001Attention\u200b\u635f\u5931\u200b\u6307\u5bfc\u200bCTC\u200b\u635f\u5931\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u3001\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200bTextConAug\u3001TextRotNet\u200b\u81ea\u200b\u76d1\u7763\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3001UDML\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\u3001UIM\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\uff0c6\u200b\u4e2a\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u52a0\u901f\u200b\u548c\u200b\u6548\u679c\u200b\u63d0\u5347\u200b\u3002</p> </li> </ul> <p>\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv3\u200b\u6280\u672f\u200b\u62a5\u544a\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b 3\u200b\u79cd\u200b\u65b9\u6848\u200b \u200b\u8fdb\u884c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\uff1a</p> <ul> <li>PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b</li> <li>PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b</li> <li>PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune</li> </ul>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#41","title":"4.1 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30","text":"<p>\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u901a\u8fc7\u200bPaddleOCR\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5982\u679c\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u80fd\u200b\u6ee1\u8db3\u200b\u6548\u679c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0d\u518d\u200b\u9700\u8981\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u6b65\u9aa4\u200b\u5982\u4e0b\u200b\uff1a</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#1_1","title":"1\uff09\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>PaddleOCR\u200b\u5df2\u7ecf\u200b\u63d0\u4f9b\u200b\u4e86\u200bPP-OCR\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\uff0c\u200b\u90e8\u5206\u200b\u6a21\u578b\u200b\u5c55\u793a\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u63a8\u8350\u200b\u573a\u666f\u200b \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b \u200b\u65b9\u5411\u200b\u5206\u7c7b\u5668\u200b \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\uff0816.2M\uff09 ch_PP-OCRv3_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\uff0813.4M\uff09 en_PP-OCRv3_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCRv2\u200b\u6a21\u578b\u200b\uff0813.0M\uff09 ch_PP-OCRv2_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCR mobile\u200b\u6a21\u578b\u200b\uff089.4M\uff09 ch_ppocr_mobile_v2.0_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u901a\u7528\u200bPP-OCR server\u200b\u6a21\u578b\u200b\uff08143.4M\uff09 ch_ppocr_server_v2.0_xx \u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u66f4\u200b\u591a\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5305\u62ec\u200b\u591a\u200b\u8bed\u8a00\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPP-OCR\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</p> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u5982\u679c\u200b\u66f4\u6362\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u65b0\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u548c\u200b\u89e3\u538b\u200b\u6307\u4ee4\u200b\u5c31\u200b\u53ef\u4ee5\u200b\ncd /home/aistudio/PaddleOCR\nmkdir pretrain_models\ncd pretrain_models\n# \u200b\u4e0b\u8f7d\u200b\u82f1\u6587\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar\ntar xf en_PP-OCRv3_det_distill_train.tar &amp;&amp; rm -rf en_PP-OCRv3_det_distill_train.tar\n%cd ..\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b</p> <p>\u200b\u9996\u5148\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b,'/home/aistudio/dataset'\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b,'/home/aistudio/dataset/det_gt_val.txt'\nEval.dataset.transforms.DetResizeForTest:  \u200b\u5c3a\u5bf8\u200b\n        limit_side_len: 48\n        limit_type: 'min'\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5177\u4f53\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>cd /home/aistudio/PaddleOCR\npython tools/eval.py \\\n    -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml  \\\n    -o Global.checkpoints=\"./pretrain_models/en_PP-OCRv3_det_distill_train/best_accuracy\"\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#42-padding","title":"4.2 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+\u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding\u200b\u76f4\u63a5\u200b\u8bc4\u4f30","text":"<p>\u200b\u8003\u8651\u200b\u5230\u200bPCB\u200b\u56fe\u7247\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\u53ea\u6709\u200b25\u200b\u5de6\u53f3\u200b\u3001\u200b\u9ad8\u5ea6\u200b\u53ea\u6709\u200b140-170\u200b\u5de6\u53f3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u539f\u56fe\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u884c\u200bpadding\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200b\u68c0\u6d4b\u200b\u8bc4\u4f30\u200b\uff0cpadding\u200b\u524d\u540e\u200b\u6548\u679c\u200b\u5bf9\u200b\u6bd4\u5982\u200b \u200b\u56fe\u200b4 \u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u5c06\u200b\u56fe\u7247\u200b\u90fd\u200bpadding\u200b\u5230\u200b300*300\u200b\u5927\u5c0f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5750\u6807\u200b\u4fe1\u606f\u200b\u53d1\u751f\u200b\u4e86\u200b\u53d8\u5316\u200b\uff0c\u200b\u6211\u4eec\u200b\u540c\u65f6\u200b\u8981\u200b\u4fee\u6539\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\uff0c\u200b\u5728\u200b<code>/home/aistudio/dataset</code>\u200b\u76ee\u5f55\u200b\u91cc\u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200bpadding\u200b\u4e4b\u540e\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u5927\u5bb6\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\uff1a</p> <p>\u200b\u540c\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b,'/home/aistudio/dataset'\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b,/home/aistudio/dataset/det_gt_padding_val.txt\nEval.dataset.transforms.DetResizeForTest:  \u200b\u5c3a\u5bf8\u200b\n        limit_side_len: 1100\n        limit_type: 'min'\n</code></pre> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</p> <pre><code>cd /home/aistudio/PaddleOCR\npython tools/eval.py \\\n    -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml  \\\n    -o Global.checkpoints=\"./pretrain_models/en_PP-OCRv3_det_distill_train/best_accuracy\"\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#43-fine-tune","title":"4.3 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune","text":"<p>\u200b\u57fa\u4e8e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u751f\u6210\u200b\u7684\u200b1500\u200b\u56fe\u7247\u200b\u4e0a\u200b\u8fdb\u884c\u200bfine-tune\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5176\u4e2d\u200btrain\u200b\u6570\u636e\u200b1200\u200b\u5f20\u200b\uff0cval\u200b\u6570\u636e\u200b300\u200b\u5f20\u200b\uff0c\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Global.epoch_num: \u200b\u8fd9\u91cc\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b1\uff0c\u200b\u65b9\u4fbf\u200b\u5feb\u901f\u200b\u8dd1\u901a\u200b\uff0c\u200b\u5b9e\u9645\u200b\u4e2d\u200b\u6839\u636e\u200b\u6570\u636e\u91cf\u200b\u8c03\u6574\u200b\u8be5\u503c\u200b\nGlobal.save_model_dir\uff1a\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\nGlobal.pretrained_model\uff1a\u200b\u6307\u5411\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c'./pretrain_models/en_PP-OCRv3_det_distill_train/student.pdparams'\nOptimizer.lr.learning_rate\uff1a\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u672c\u200b\u5b9e\u9a8c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0.0005\nTrain.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b,'/home/aistudio/dataset'\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b,'/home/aistudio/dataset/det_gt_train.txt'\nTrain.dataset.transforms.EastRandomCropData.size\uff1a\u200b\u8bad\u7ec3\u200b\u5c3a\u5bf8\u200b\u6539\u4e3a\u200b[480,64]\nEval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b,'/home/aistudio/dataset/'\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b,'/home/aistudio/dataset/det_gt_val.txt'\nEval.dataset.transforms.DetResizeForTest\uff1a\u200b\u8bc4\u4f30\u200b\u5c3a\u5bf8\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u5982\u4e0b\u200b\u53c2\u6570\u200b\n    limit_side_len: 64\n    limit_type:'min'\n</code></pre> <p>\u200b\u6267\u884c\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>cd /home/aistudio/PaddleOCR/\npython tools/train.py \\\n        -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b</p> <p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>Global.checkpoints</code>:</p> <pre><code>cd /home/aistudio/PaddleOCR/\npython3 tools/eval.py \\\n    -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml  \\\n    -o Global.checkpoints=\"./output/ch_PP-OCR_V3_det/latest\"\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u65b9\u6848\u200b hmean \u200b\u6548\u679c\u200b\u63d0\u5347\u200b \u200b\u5b9e\u9a8c\u200b\u5206\u6790\u200b 1 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 64.64% - \u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b 2 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding 72.13% +7.49% padding\u200b\u53ef\u4ee5\u200b\u63d0\u5347\u200b\u5c3a\u5bf8\u200b\u8f83\u200b\u5c0f\u200b\u56fe\u7247\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b 3 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  + fine-tune 100.00% +27.87% fine-tune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4e0a\u8ff0\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u5747\u200b\u662f\u200b\u5728\u200b1500\u200b\u5f20\u200b\u56fe\u7247\u200b\uff081200\u200b\u5f20\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c300\u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff09\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u7684\u200b\u5f97\u5230\u200b\uff0cAIstudio\u200b\u53ea\u200b\u63d0\u4f9b\u200b\u4e86\u200b100\u200b\u5f20\u200b\u6570\u636e\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6307\u6807\u200b\u6709\u6240\u200b\u5dee\u5f02\u200b\u5c5e\u4e8e\u200b\u6b63\u5e38\u200b\uff0c\u200b\u53ea\u8981\u200b\u7b56\u7565\u200b\u6709\u6548\u200b\u3001\u200b\u89c4\u5f8b\u200b\u76f8\u540c\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#5","title":"5. \u200b\u6587\u672c\u200b\u8bc6\u522b","text":"<p>\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b4\u200b\u79cd\u200b\u65b9\u6848\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\uff1a</p> <ul> <li>\u200b\u65b9\u6848\u200b1\uff1aPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b</li> <li>\u200b\u65b9\u6848\u200b2\uff1aPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune</li> <li>\u200b\u65b9\u6848\u200b3\uff1aPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b</li> <li>\u200b\u65b9\u6848\u200b4\uff1aPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b</li> </ul>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#51","title":"5.1 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30","text":"<p>\u200b\u540c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u63d0\u4f9b\u200b\u7684\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200bPCB\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u6b65\u9aa4\u200b\u5982\u4e0b\u200b\uff1a</p> <p>1\uff09\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u5982\u679c\u200b\u66f4\u6362\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u65b0\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u548c\u200b\u89e3\u538b\u200b\u6307\u4ee4\u200b\u5c31\u200b\u53ef\u4ee5\u200b\ncd /home/aistudio/PaddleOCR/pretrain_models/\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\ntar xf ch_PP-OCRv3_rec_train.tar &amp;&amp; rm -rf ch_PP-OCRv3_rec_train.tar\ncd ..\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b \u200b\u9996\u5148\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv3/ch_PP-OCRv2_rec_distillation.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Metric.ignore_space: True\uff1a\u200b\u5ffd\u7565\u200b\u7a7a\u683c\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b,'/home/aistudio/dataset'\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b,'/home/aistudio/dataset/rec_gt_val.txt'\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff1a</p> <pre><code>cd /home/aistudio/PaddleOCR\npython3 tools/eval.py \\\n    -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml \\\n    -o Global.checkpoints=pretrain_models/ch_PP-OCRv3_rec_train/best_accuracy\n</code></pre>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#52-fine-tune","title":"5.2 \u200b\u4e09\u79cd\u200bfine-tune\u200b\u65b9\u6848","text":"<p>\u200b\u65b9\u6848\u200b2\u30013\u30014\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u65b9\u5f0f\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u6211\u4eec\u200b\u4e86\u89e3\u200b\u6bcf\u4e2a\u200b\u6280\u672f\u200b\u65b9\u6848\u200b\u4e4b\u540e\u200b\uff0c\u200b\u518d\u200b\u5177\u4f53\u200b\u770b\u200b\u4fee\u6539\u200b\u54ea\u4e9b\u200b\u53c2\u6570\u200b\u662f\u200b\u76f8\u540c\u200b\uff0c\u200b\u54ea\u4e9b\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\u3002</p> <p>\u200b\u65b9\u6848\u200b\u4ecb\u7ecd\u200b\uff1a</p> <p>1\uff09 \u200b\u65b9\u6848\u200b2\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune</p> <ul> <li>\u200b\u5728\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u884c\u200bfine-tune\uff0c\u200b\u4f7f\u7528\u200b1500\u200b\u5f20\u200bPCB\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bad\u7ec3\u200b\u96c6\u200b1200\u200b\u5f20\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b300\u200b\u5f20\u200b\u3002</li> </ul> <p>2\uff09 \u200b\u65b9\u6848\u200b3\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b</p> <ul> <li>\u200b\u5f53\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u6bd4\u8f83\u200b\u5c11\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u5728\u200b\u65b9\u6848\u200b2\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5982\u200blsvt\u3001rctw\u200b\u7b49\u200b\u3002</li> </ul> <p>3\uff09\u200b\u65b9\u6848\u200b4\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b</p> <ul> <li>\u200b\u5982\u679c\u200b\u80fd\u591f\u200b\u83b7\u53d6\u200b\u8db3\u591f\u200b\u591a\u200b\u771f\u5b9e\u200b\u573a\u666f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u3002\u200b\u5728\u200b\u65b9\u6848\u200b2\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u589e\u52a0\u200bPCB\u200b\u7684\u200b\u6570\u91cf\u200b\u5230\u200b2W\u200b\u5f20\u200b\u5de6\u53f3\u200b\u3002</li> </ul> <p>\u200b\u53c2\u6570\u200b\u4fee\u6539\u200b\uff1a</p> <p>\u200b\u63a5\u7740\u200b\u6211\u4eec\u200b\u770b\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u4e0a\u200b\u65b9\u6848\u200b\u5747\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml</code>\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4fee\u6539\u200b\u4e00\u6b21\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>Global.pretrained_model\uff1a\u200b\u6307\u5411\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b,'pretrain_models/ch_PP-OCRv3_rec_train/best_accuracy'\nOptimizer.lr.values\uff1a\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u672c\u200b\u5b9e\u9a8c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0.0005\nTrain.loader.batch_size_per_card: batch size,\u200b\u9ed8\u8ba4\u200b128\uff0c\u200b\u56e0\u4e3a\u200b\u6570\u636e\u91cf\u200b\u5c0f\u4e8e\u200b128\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b8\uff0c\u200b\u6570\u636e\u91cf\u200b\u5927\u200b\u53ef\u4ee5\u200b\u6309\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u8bad\u7ec3\u200b\nEval.loader.batch_size_per_card: batch size,\u200b\u9ed8\u8ba4\u200b128\uff0c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b4\nMetric.ignore_space: \u200b\u5ffd\u7565\u200b\u7a7a\u683c\u200b\uff0c\u200b\u672c\u200b\u5b9e\u9a8c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bTrue\n</code></pre> <p>\u200b\u66f4\u6362\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u6848\u200b\u6bcf\u6b21\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u7684\u200b\u53c2\u6570\u200b\uff1a</p> <pre><code>Global.epoch_num: \u200b\u8fd9\u91cc\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b1\uff0c\u200b\u65b9\u4fbf\u200b\u5feb\u901f\u200b\u8dd1\u901a\u200b\uff0c\u200b\u5b9e\u9645\u200b\u4e2d\u200b\u6839\u636e\u200b\u6570\u636e\u91cf\u200b\u8c03\u6574\u200b\u8be5\u503c\u200b\nGlobal.save_model_dir\uff1a\u200b\u6307\u5411\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\nTrain.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u540c\u65f6\u200b\u65b9\u6848\u200b3\u200b\u4fee\u6539\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b</p> <pre><code>Eval.dataset.label_file_list\uff1a\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.ratio_list\uff1a\u200b\u6570\u636e\u200b\u548c\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u6bcf\u6b21\u200b\u91c7\u6837\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u6309\u200b\u5b9e\u9645\u200b\u4fee\u6539\u200b\u5373\u53ef\u200b\n</code></pre> <p>\u200b\u5982\u200b \u200b\u56fe\u200b5 \u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u63d0\u53d6\u200bStudent\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200bPCB\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200bfine-tune\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>import paddle\n# \u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nall_params = paddle.load(\"./pretrain_models/ch_PP-OCRv3_rec_train/best_accuracy.pdparams\")\n# \u200b\u67e5\u770b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(all_params.keys())\n# \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u63d0\u53d6\u200b\ns_params = {key[len(\"student_model.\"):]: all_params[key] for key in all_params if \"student_model.\" in key}\n# \u200b\u67e5\u770b\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(s_params.keys())\n# \u200b\u4fdd\u5b58\u200b\npaddle.save(s_params, \"./pretrain_models/ch_PP-OCRv3_rec_train/student.pdparams\")\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u53c2\u6570\u200b\u540e\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u65b9\u6848\u200b\u90fd\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>cd /home/aistudio/PaddleOCR/\npython3 tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>Global.checkpoints</code>\uff1a</p> <pre><code>cd /home/aistudio/PaddleOCR/\npython3 tools/eval.py \\\n    -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml \\\n    -o Global.checkpoints=./output/rec_ppocr_v3/latest\n</code></pre> <p>\u200b\u6240\u6709\u200b\u65b9\u6848\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u65b9\u6848\u200b acc \u200b\u6548\u679c\u200b\u63d0\u5347\u200b \u200b\u5b9e\u9a8c\u200b\u5206\u6790\u200b 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b 46.67% - \u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b 2 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune 42.02% -4.65% \u200b\u5728\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53cd\u800c\u200b\u6bd4\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u4f4e\u200b(\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u518d\u200b\u8bd5\u8bd5\u200b) 3 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b 77.00% +30.33% \u200b\u5728\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u8865\u5145\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b 4 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b 99.99% +22.99% \u200b\u5982\u679c\u200b\u80fd\u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u63d0\u5347\u200b\u6548\u679c\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4e0a\u8ff0\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u5747\u200b\u662f\u200b\u5728\u200b1500\u200b\u5f20\u200b\u56fe\u7247\u200b\uff081200\u200b\u5f20\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c300\u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff09\u30012W\u200b\u5f20\u200b\u56fe\u7247\u200b\u3001\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u7684\u200b\u5f97\u5230\u200b\uff0cAIstudio\u200b\u53ea\u200b\u63d0\u4f9b\u200b\u4e86\u200b100\u200b\u5f20\u200b\u6570\u636e\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6307\u6807\u200b\u6709\u6240\u200b\u5dee\u5f02\u200b\u5c5e\u4e8e\u200b\u6b63\u5e38\u200b\uff0c\u200b\u53ea\u8981\u200b\u7b56\u7565\u200b\u6709\u6548\u200b\u3001\u200b\u89c4\u5f8b\u200b\u76f8\u540c\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#6","title":"6. \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>inference \u200b\u6a21\u578b\u200b\uff08paddle.jit.save\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\uff09 \u200b\u4e00\u822c\u200b\u662f\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u628a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u548c\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4fdd\u5b58\u200b\u5728\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u56fa\u5316\u200b\u6a21\u578b\u200b\uff0c\u200b\u591a\u200b\u7528\u4e8e\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u573a\u666f\u200b\u3002 \u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200bcheckpoints\u200b\u6a21\u578b\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u7684\u200b\u53ea\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u591a\u200b\u7528\u4e8e\u200b\u6062\u590d\u200b\u8bad\u7ec3\u200b\u7b49\u200b\u3002 \u200b\u4e0e\u200bcheckpoints\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\uff0cinference \u200b\u6a21\u578b\u200b\u4f1a\u200b\u989d\u5916\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u3001\u200b\u52a0\u901f\u200b\u63a8\u7406\u200b\u4e0a\u200b\u6027\u80fd\u4f18\u8d8a\u200b\uff0c\u200b\u7075\u6d3b\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u9002\u5408\u200b\u4e8e\u200b\u5b9e\u9645\u200b\u7cfb\u7edf\u96c6\u6210\u200b\u3002</p> <pre><code># \u200b\u5bfc\u51fa\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\npython3 tools/export_model.py \\\n     -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml \\\n     -o Global.pretrained_model=\"./output/ch_PP-OCR_V3_det/latest\" \\\n     Global.save_inference_dir=\"./inference_model/ch_PP-OCR_V3_det/\"\n</code></pre> <p>\u200b\u56e0\u4e3a\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u53ea\u200b\u8bad\u7ec3\u200b\u4e86\u200b1\u200b\u4e2a\u200bepoch\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u6700\u4f18\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5b58\u50a8\u200b\u5728\u200b<code>/home/aistudio/best_models/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u89e3\u538b\u200b\u5373\u53ef\u200b</p> <pre><code>cd /home/aistudio/best_models/\nwget https://paddleocr.bj.bcebos.com/fanliku/PCB/det_ppocr_v3_en_infer_PCB.tar\ntar xf /home/aistudio/best_models/det_ppocr_v3_en_infer_PCB.tar -C /home/aistudio/PaddleOCR/pretrain_models/\n</code></pre> <pre><code># \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200binference\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\ncd /home/aistudio/PaddleOCR/\npython3 tools/infer/predict_det.py \\\n    --image_dir=\"/home/aistudio/dataset/imgs/0000.jpg\" \\\n    --det_algorithm=\"DB\" \\\n    --det_model_dir=\"./pretrain_models/det_ppocr_v3_en_infer_PCB/\" \\\n    --det_limit_side_len=48 \\\n    --det_limit_type='min' \\\n    --det_db_unclip_ratio=2.5 \\\n    --use_gpu=True\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5b58\u50a8\u200b\u5728\u200b<code>inference_results</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u540c\u7406\u200b\uff0c\u200b\u5bfc\u51fa\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5e76\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002</p> <pre><code># \u200b\u5bfc\u51fa\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\npython3 tools/export_model.py \\\n    -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml \\\n    -o Global.pretrained_model=\"./output/rec_ppocr_v3/latest\" \\\n    Global.save_inference_dir=\"./inference_model/rec_ppocr_v3/\"\n</code></pre> <p>\u200b\u540c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u4e5f\u200b\u53ea\u200b\u8bad\u7ec3\u200b\u4e86\u200b1\u200b\u4e2a\u200bepoch\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u6700\u4f18\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5b58\u50a8\u200b\u5728\u200b<code>/home/aistudio/best_models/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u89e3\u538b\u200b\u5373\u53ef\u200b</p> <pre><code>cd /home/aistudio/best_models/\nwget https://paddleocr.bj.bcebos.com/fanliku/PCB/rec_ppocr_v3_ch_infer_PCB.tar\ntar xf /home/aistudio/best_models/rec_ppocr_v3_ch_infer_PCB.tar -C /home/aistudio/PaddleOCR/pretrain_models/\n</code></pre> <pre><code># \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200binference\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\ncd /home/aistudio/PaddleOCR/\npython3 tools/infer/predict_rec.py \\\n    --image_dir=\"../test_imgs/0000_rec.jpg\" \\\n    --rec_model_dir=\"./pretrain_models/rec_ppocr_v3_ch_infer_PCB\" \\\n    --rec_image_shape=\"3, 48, 320\" \\\n    --use_space_char=False \\\n    --use_gpu=True\n</code></pre> <pre><code># \u200b\u68c0\u6d4b\u200b+\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200binference\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\ncd /home/aistudio/PaddleOCR/\npython3 tools/infer/predict_system.py  \\\n    --image_dir=\"../test_imgs/0000.jpg\" \\\n    --det_model_dir=\"./pretrain_models/det_ppocr_v3_en_infer_PCB\" \\\n    --det_limit_side_len=48 \\\n    --det_limit_type='min' \\\n    --det_db_unclip_ratio=2.5 \\\n    --rec_model_dir=\"./pretrain_models/rec_ppocr_v3_ch_infer_PCB\"  \\\n    --rec_image_shape=\"3, 48, 320\" \\\n    --draw_img_save_dir=./det_rec_infer/ \\\n    --use_space_char=False \\\n    --use_angle_cls=False \\\n    --use_gpu=True\n</code></pre> <p>\u200b\u7aef\u5230\u200b\u7aef\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5b58\u50a8\u200b\u5728\u200b<code>det_res_infer</code>\u200b\u6587\u4ef6\u5939\u200b\u5185\u200b\uff0c\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#7","title":"7. \u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u8bc4\u6d4b","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\u4ecb\u7ecd\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b+\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u6307\u6807\u200b\u8bc4\u4f30\u200b\u65b9\u5f0f\u200b\u3002\u200b\u4e3b\u8981\u200b\u5206\u4e3a\u200b\u4e09\u6b65\u200b\uff1a</p> <p>1\uff09\u200b\u9996\u5148\u200b\u8fd0\u884c\u200b<code>tools/infer/predict_system.py</code>\uff0c\u200b\u5c06\u200b<code>image_dir</code>\u200b\u6539\u4e3a\u200b\u9700\u8981\u200b\u8bc4\u4f30\u200b\u7684\u200b\u6570\u636e\u6587\u4ef6\u200b\u5bb6\u200b\uff0c\u200b\u5f97\u5230\u200b\u4fdd\u5b58\u200b\u7684\u200b\u7ed3\u679c\u200b:</p> <pre><code># \u200b\u68c0\u6d4b\u200b+\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200binference\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\npython3 tools/infer/predict_system.py  \\\n    --image_dir=\"../dataset/imgs/\" \\\n    --det_model_dir=\"./pretrain_models/det_ppocr_v3_en_infer_PCB\" \\\n    --det_limit_side_len=48 \\\n    --det_limit_type='min' \\\n    --det_db_unclip_ratio=2.5 \\\n    --rec_model_dir=\"./pretrain_models/rec_ppocr_v3_ch_infer_PCB\"  \\\n    --rec_image_shape=\"3, 48, 320\" \\\n    --draw_img_save_dir=./det_rec_infer/ \\\n    --use_space_char=False \\\n    --use_angle_cls=False \\\n    --use_gpu=True\n</code></pre> <p>\u200b\u5f97\u5230\u200b\u4fdd\u5b58\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u53ef\u89c6\u5316\u200b\u56fe\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>det_rec_infer/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>det_rec_infer/system_results.txt</code>\u200b\u4e2d\u200b\uff0c\u200b\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a<code>0018.jpg   [{\"transcription\": \"E295\", \"points\": [[88, 33], [137, 33], [137, 40], [88, 40]]}]</code></p> <p>2\uff09\u200b\u7136\u540e\u200b\u5c06\u200b\u6b65\u9aa4\u200b\u4e00\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u8bc4\u6d4b\u200b\u9700\u8981\u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\uff1a \u200b\u4fee\u6539\u200b\u00a0<code>tools/end2end/convert_ppocr_label.py</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0cconvert_label\u200b\u51fd\u6570\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u8f93\u5165\u200b\u6807\u7b7e\u200b\u8def\u5f84\u200b\uff0cMode\uff0c\u200b\u4fdd\u5b58\u200b\u6807\u7b7e\u200b\u8def\u5f84\u200b\u7b49\u200b\uff0c\u200b\u5bf9\u200b\u9884\u6d4b\u200b\u6570\u636e\u200b\u7684\u200bGTlabel\u200b\u548c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u7684\u200blabel\u200b\u683c\u5f0f\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\u3002</p> <pre><code>ppocr_label_gt =  \"/home/aistudio/dataset/det_gt_val.txt\"\nconvert_label(ppocr_label_gt, \"gt\", \"./save_gt_label/\")\n\nppocr_label_gt =  \"/home/aistudio/PaddleOCR/PCB_result/det_rec_infer/system_results.txt\"\nconvert_label(ppocr_label_gt, \"pred\", \"./save_PPOCRV2_infer/\")\n</code></pre> <p>\u200b\u8fd0\u884c\u200b<code>convert_ppocr_label.py</code>:</p> <pre><code>python3 tools/end2end/convert_ppocr_label.py\n</code></pre> <p>\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u7ed3\u679c\u200b\uff1a</p> <pre><code>\u251c\u2500\u2500 ./save_gt_label/\n\u251c\u2500\u2500 ./save_PPOCRV2_infer/\n</code></pre> <p>3\uff09 \u200b\u6700\u540e\u200b\uff0c\u200b\u6267\u884c\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u8bc4\u6d4b\u200b\uff0c\u200b\u8fd0\u884c\u200b<code>tools/end2end/eval_end2end.py</code>\u200b\u8ba1\u7b97\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u6307\u6807\u200b\uff0c\u200b\u8fd0\u884c\u200b\u65b9\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>pip install editdistance\npython3 tools/end2end/eval_end2end.py ./save_gt_label/ ./save_PPOCRV2_infer/\n</code></pre> <p>\u200b\u4f7f\u7528\u200b<code>\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune'\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b</code>\u3001<code>\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + 2W\u200b\u5f20\u200bPCB\u200b\u56fe\u7247\u200bfunetune</code>\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b300\u200b\u5f20\u200bPCB\u200b\u56fe\u7247\u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u7ed3\u679c\u200b\uff0cfmeasure\u200b\u4e3a\u200b\u4e3b\u8981\u200b\u5173\u6ce8\u200b\u7684\u200b\u6307\u6807\u200b:</p> <p></p> <p>\u200b\u6ce8\u200b: \u200b\u4f7f\u7528\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u4e0d\u80fd\u200b\u8dd1\u200b\u51fa\u8be5\u200b\u7ed3\u679c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0d\u200b\u76f8\u540c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u66f4\u6362\u200b\u4e3a\u200b\u81ea\u5df1\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6309\u200b\u4e0a\u8ff0\u200b\u6d41\u7a0b\u200b\u8fd0\u884c\u200b</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#8-jetson","title":"8. Jetson\u200b\u90e8\u7f72","text":"<p>\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5b8c\u6210\u200bJetson nano\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\uff0c\u200b\u7b80\u5355\u200b\u6613\u200b\u64cd\u4f5c\u200b\uff1a</p> <p>1\u3001\u200b\u5728\u200bJetson nano\u200b\u5f00\u53d1\u200b\u7248\u4e0a\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\uff1a</p> <ul> <li> <p>\u200b\u5b89\u88c5\u200bPaddlePaddle</p> </li> <li> <p>\u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u5e76\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b</p> </li> </ul> <p>2\u3001\u200b\u6267\u884c\u200b\u9884\u6d4b\u200b</p> <ul> <li> <p>\u200b\u5c06\u200b\u63a8\u7406\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5230\u200bjetson</p> </li> <li> <p>\u200b\u6267\u884c\u200b\u68c0\u6d4b\u200b\u3001\u200b\u8bc6\u522b\u200b\u3001\u200b\u4e32\u8054\u200b\u9884\u6d4b\u200b\u5373\u53ef\u200b</p> </li> </ul> <p>\u200b\u8be6\u7ec6\u200b\u53c2\u8003\u200b\u6d41\u7a0b\u200b\u3002</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#9","title":"9. \u200b\u603b\u7ed3","text":"<p>\u200b\u68c0\u6d4b\u200b\u5b9e\u9a8c\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200bPCB\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding\u3001 fine-tune 3\u200b\u79cd\u200b\u65b9\u6848\u200b\uff0c\u200b\u8bc6\u522b\u200b\u5b9e\u9a8c\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200bPCB\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u3001 fine-tune\u3001\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u7247\u200b\u6570\u91cf\u200b4\u200b\u79cd\u200b\u65b9\u6848\u200b\uff0c\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u68c0\u6d4b\u200b</li> </ul> \u200b\u5e8f\u53f7\u200b \u200b\u65b9\u6848\u200b hmean \u200b\u6548\u679c\u200b\u63d0\u5347\u200b \u200b\u5b9e\u9a8c\u200b\u5206\u6790\u200b 1 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b 64.64% - \u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b 2 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b 72.13% +7.49% padding\u200b\u53ef\u4ee5\u200b\u63d0\u5347\u200b\u5c3a\u5bf8\u200b\u8f83\u200b\u5c0f\u200b\u56fe\u7247\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b 3 PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  + fine-tune 100.00% +27.87% fine-tune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c\u200b <ul> <li>\u200b\u8bc6\u522b\u200b</li> </ul> \u200b\u5e8f\u53f7\u200b \u200b\u65b9\u6848\u200b acc \u200b\u6548\u679c\u200b\u63d0\u5347\u200b \u200b\u5b9e\u9a8c\u200b\u5206\u6790\u200b 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b 46.67% - \u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b 2 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune 42.02% -4.65% \u200b\u5728\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53cd\u800c\u200b\u6bd4\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u4f4e\u200b(\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u518d\u200b\u8bd5\u8bd5\u200b) 3 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b 77.00% +30.33% \u200b\u5728\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u8865\u5145\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b 4 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b 99.99% +22.99% \u200b\u5982\u679c\u200b\u80fd\u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u63d0\u5347\u200b\u6548\u679c\u200b <ul> <li>\u200b\u7aef\u5230\u200b\u7aef\u200b</li> </ul> det rec fmeasure PP-OCRv3\u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  + fine-tune PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + fine-tune + \u200b\u589e\u52a0\u200bPCB\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b 93.30% <p>\u200b\u7ed3\u8bba\u200b</p> <p>PP-OCRv3\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5728\u200b\u672a\u200b\u7ecf\u8fc7\u200bfine-tune\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5728\u200bPCB\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4e5f\u200b\u6709\u200b64.64%\u200b\u7684\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002\u200b\u9a8c\u8bc1\u200b\u96c6\u200bpadding\u200b\u4e4b\u540e\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b7.5%\uff0c\u200b\u5728\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bpadding\u200b\u7684\u200b\u65b9\u5f0f\u200b\u63d0\u5347\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002\u200b\u7ecf\u8fc7\u200b fine-tune \u200b\u540e\u200b\u80fd\u591f\u200b\u6781\u5927\u200b\u7684\u200b\u63d0\u5347\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fbe\u5230\u200b100%\u3002</p> <p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u65b9\u6848\u200b1\u200b\u548c\u200b\u65b9\u6848\u200b2\u200b\u5bf9\u6bd4\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5f53\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u53ef\u80fd\u200b\u6bd4\u200bfine-tune\u200b\u6548\u679c\u200b\u8fd8\u8981\u200b\u9ad8\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5148\u200b\u5c1d\u8bd5\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u3002\u200b\u5982\u679c\u200b\u5728\u200b\u6570\u636e\u91cf\u200b\u4e0d\u8db3\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u60f3\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6dfb\u52a0\u200b\u516c\u5f00\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\u63d0\u5347\u200b30%\uff0c\u200b\u975e\u5e38\u200b\u6709\u6548\u200b\u3002\u200b\u6700\u540e\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u91c7\u96c6\u200b\u8db3\u591f\u200b\u591a\u200b\u7684\u200b\u771f\u5b9e\u200b\u573a\u666f\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fbe\u5230\u200b99.99%\u3002</p>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_1","title":"\u66f4\u200b\u591a\u200b\u8d44\u6e90","text":"<ul> <li> <p>\u200b\u66f4\u200b\u591a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u77e5\u8bc6\u200b\u3001\u200b\u4ea7\u4e1a\u200b\u6848\u4f8b\u200b\u3001\u200b\u9762\u8bd5\u200b\u5b9d\u5178\u200b\u7b49\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aawesome-DeepLearning</p> </li> <li> <p>\u200b\u66f4\u200b\u591a\u200bPaddleOCR\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aPaddleOCR</p> </li> <li> <p>\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u76f8\u5173\u200b\u8d44\u6599\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1a\u200b\u98de\u6868\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e73\u53f0\u200b</p> </li> </ul>"},{"location":"en/applications/PCB%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_2","title":"\u53c2\u8003","text":"<ul> <li>\u200b\u6570\u636e\u200b\u751f\u6210\u200b\u4ee3\u7801\u200b\u5e93\u200b\uff1ahttps://github.com/zcswdt/Color_OCR_image_generator</li> </ul>"},{"location":"en/applications/overview.html","title":"\u573a\u666f\u200b\u5e94\u7528","text":"<p>PaddleOCR\u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u8986\u76d6\u200b\u901a\u7528\u200b\uff0c\u200b\u5236\u9020\u200b\u3001\u200b\u91d1\u878d\u200b\u3001\u200b\u4ea4\u901a\u200b\u884c\u4e1a\u200b\u7684\u200b\u4e3b\u8981\u200bOCR\u200b\u5782\u7c7b\u200b\u5e94\u7528\u200b\uff0c\u200b\u5728\u200bPP-OCR\u3001PP-Structure\u200b\u7684\u200b\u901a\u7528\u200b\u80fd\u529b\u200b\u57fa\u7840\u200b\u4e4b\u4e0a\u200b\uff0c\u200b\u4ee5\u200bnotebook\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5c55\u793a\u200b\u5229\u7528\u200b\u573a\u666f\u200b\u6570\u636e\u200b\u5fae\u8c03\u200b\u3001\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65b9\u6cd5\u200b\u3001\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b49\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4e3a\u200b\u5f00\u53d1\u8005\u200b\u5feb\u901f\u200b\u843d\u5730\u200bOCR\u200b\u5e94\u7528\u200b\u63d0\u4f9b\u200b\u793a\u8303\u200b\u4e0e\u200b\u542f\u53d1\u200b\u3002</p>"},{"location":"en/applications/overview.html#_2","title":"\u6559\u7a0b\u200b\u6587\u6863","text":""},{"location":"en/applications/overview.html#_3","title":"\u901a\u7528","text":"\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u6559\u7a0b\u200b \u200b\u793a\u4f8b\u200b\u56fe\u200b \u200b\u9ad8\u7cbe\u5ea6\u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSVTR \u200b\u6bd4\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u9ad8\u200b3%\uff0c\u200b\u53ef\u200b\u7528\u4e8e\u200b\u6570\u636e\u6316\u6398\u200b\u6216\u200b\u5bf9\u200b\u9884\u6d4b\u200b\u6548\u7387\u200b\u8981\u6c42\u200b\u4e0d\u9ad8\u200b\u7684\u200b\u573a\u666f\u200b\u3002 \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u624b\u5199\u4f53\u200b\u8bc6\u522b\u200b \u200b\u65b0\u589e\u200b\u5b57\u5f62\u200b\u652f\u6301\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English"},{"location":"en/applications/overview.html#_4","title":"\u5236\u9020","text":"\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u6559\u7a0b\u200b \u200b\u793a\u4f8b\u200b\u56fe\u200b \u200b\u6570\u7801\u7ba1\u200b\u8bc6\u522b\u200b \u200b\u6570\u7801\u7ba1\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u3001\u200b\u6f0f\u200b\u8bc6\u522b\u200b\u8c03\u4f18\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u6db2\u6676\u5c4f\u200b\u8bfb\u6570\u200b\u8bc6\u522b\u200b \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u84b8\u998f\u200b\u3001Serving\u200b\u90e8\u7f72\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b \u200b\u70b9\u9635\u200b\u5b57\u7b26\u200b\u5408\u6210\u200b\u3001\u200b\u8fc7\u200b\u66dd\u8fc7\u200b\u6697\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English PCB\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b \u200b\u5c0f\u200b\u5c3a\u5bf8\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u7535\u8868\u200b\u8bc6\u522b\u200b \u200b\u5927\u200b\u5206\u8fa8\u7387\u200b\u56fe\u50cf\u200b\u68c0\u6d4b\u200b\u8c03\u4f18\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u6db2\u6676\u5c4f\u200b\u7f3a\u9677\u200b\u68c0\u6d4b\u200b \u200b\u975e\u200b\u6587\u5b57\u200b\u5b57\u7b26\u8bc6\u522b"},{"location":"en/applications/overview.html#_5","title":"\u91d1\u878d","text":"\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u6559\u7a0b\u200b \u200b\u793a\u4f8b\u200b\u56fe\u200b \u200b\u8868\u5355\u200bVQA \u200b\u591a\u200b\u6a21\u6001\u200b\u901a\u7528\u200b\u8868\u5355\u200b\u7ed3\u6784\u5316\u200b\u63d0\u53d6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b \u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\uff0cSER\u3001RE\u200b\u4efb\u52a1\u200b\u8bad\u7ec3\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b \u200b\u7aef\u5230\u200b\u7aef\u200b\u5f2f\u66f2\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u901a\u7528\u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b \u200b\u901a\u7528\u200b\u7ed3\u6784\u5316\u200b\u63d0\u53d6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u8eab\u4efd\u8bc1\u200b\u8bc6\u522b\u200b \u200b\u7ed3\u6784\u5316\u200b\u63d0\u53d6\u200b\u3001\u200b\u56fe\u50cf\u200b\u9634\u5f71\u200b \u200b\u5408\u540c\u200b\u6bd4\u200b\u5bf9\u200b \u200b\u5bc6\u96c6\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u3001NLP\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English"},{"location":"en/applications/overview.html#_6","title":"\u4ea4\u901a","text":"\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u6559\u7a0b\u200b \u200b\u793a\u4f8b\u200b\u56fe\u200b \u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b \u200b\u591a\u89d2\u5ea6\u200b\u56fe\u50cf\u200b\u3001\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\u3001\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u4e2d\u6587\u200b/English \u200b\u9a7e\u9a76\u8bc1\u200b/\u200b\u884c\u9a76\u8bc1\u200b\u8bc6\u522b\u200b \u200b\u5c3d\u8bf7\u200b\u671f\u5f85\u200b \u200b\u5feb\u9012\u200b\u5355\u200b\u8bc6\u522b\u200b \u200b\u5c3d\u8bf7\u200b\u671f\u5f85"},{"location":"en/applications/overview.html#_7","title":"\u6a21\u578b\u200b\u4e0b\u8f7d","text":"<p>\u200b\u5982\u9700\u200b\u4e0b\u8f7d\u200b\u4e0a\u8ff0\u200b\u573a\u666f\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u5782\u7c7b\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u4f01\u4e1a\u200b\u5f00\u53d1\u8005\u200b\u4e14\u200b\u672a\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u573a\u666f\u200b\u4e2d\u200b\u627e\u5230\u200b\u5408\u9002\u200b\u7684\u200b\u65b9\u6848\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u586b\u5199\u200bOCR\u200b\u5e94\u7528\u200b\u5408\u4f5c\u200b\u8c03\u7814\u200b\u95ee\u5377\u200b\uff0c\u200b\u514d\u8d39\u200b\u4e0e\u200b\u5b98\u65b9\u200b\u56e2\u961f\u200b\u5c55\u5f00\u200b\u4e0d\u540c\u200b\u5c42\u6b21\u200b\u7684\u200b\u5408\u4f5c\u200b\uff0c\u200b\u5305\u62ec\u200b\u4f46\u200b\u4e0d\u200b\u9650\u4e8e\u200b\u95ee\u9898\u200b\u62bd\u8c61\u200b\u3001\u200b\u786e\u5b9a\u200b\u6280\u672f\u200b\u65b9\u6848\u200b\u3001\u200b\u9879\u76ee\u200b\u7b54\u7591\u200b\u3001\u200b\u5171\u540c\u200b\u7814\u53d1\u200b\u7b49\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u7ecf\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u843d\u5730\u200b\u9879\u76ee\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u586b\u5199\u200b\u6b64\u200b\u95ee\u5377\u200b\uff0c\u200b\u4e0e\u200b\u98de\u6868\u200b\u5e73\u53f0\u200b\u5171\u540c\u200b\u5ba3\u4f20\u200b\u63a8\u5e7f\u200b\uff0c\u200b\u63d0\u5347\u200b\u4f01\u4e1a\u200b\u6280\u672f\u200b\u54c1\u5ba3\u200b\u3002\u200b\u671f\u5f85\u200b\u60a8\u200b\u7684\u200b\u63d0\u4ea4\u200b\uff01</p>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html","title":"\u667a\u80fd\u200b\u8fd0\u8425\u200b\uff1a\u200b\u901a\u7528\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u80cc\u666f\u200b\u4ecb\u7ecd","text":"<p>\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u5728\u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u6709\u7740\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u5e94\u7528\u200b\uff0c\u200b\u5982\u200b\u4fdd\u9669\u200b\u7406\u8d54\u200b\u3001\u200b\u8d22\u62a5\u200b\u5206\u6790\u200b\u548c\u200b\u4fe1\u606f\u200b\u5f55\u5165\u200b\u7b49\u200b\u9886\u57df\u200b\u3002\u200b\u5f53\u524d\u200b\uff0c\u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u7684\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u4e3b\u8981\u200b\u4ee5\u200b\u624b\u52a8\u200b\u5f55\u5165\u200b\u4e3a\u4e3b\u200b\uff0c\u200b\u5f00\u53d1\u200b\u4e00\u79cd\u200b\u81ea\u52a8\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6210\u4e3a\u200b\u4e1e\u5f85\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p></p> <p>\u200b\u5728\u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u4e2d\u200b\uff0c\u200b\u8868\u683c\u200b\u56fe\u50cf\u200b\u4e3b\u8981\u200b\u6709\u200b\u6e05\u5355\u200b\u7c7b\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u5bc6\u96c6\u578b\u200b\u8868\u683c\u200b\uff0c\u200b\u7533\u8bf7\u8868\u200b\u7c7b\u200b\u7684\u200b\u5927\u200b\u5355\u5143\u683c\u200b\u8868\u683c\u200b\uff0c\u200b\u62cd\u7167\u200b\u8868\u683c\u200b\u548c\u200b\u503e\u659c\u200b\u8868\u683c\u200b\u56db\u79cd\u200b\u4e3b\u8981\u200b\u5f62\u5f0f\u200b\u3002</p> <p></p> <p></p> <p>\u200b\u5f53\u524d\u200b\u7684\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u4e0d\u80fd\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u5904\u7406\u200b\u8fd9\u4e9b\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u8868\u683c\u200b\u56fe\u50cf\u200b\u3002\u200b\u5728\u200b\u672c\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPP-StructureV2\u200b\u6700\u65b0\u200b\u53d1\u5e03\u200b\u7684\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSLANet\u200b\u6765\u200b\u6f14\u793a\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u662f\u200b\u8bc6\u522b\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u4f5c\u4e1a\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5bf9\u200b\u8868\u683c\u200b\u56fe\u50cf\u200b\u7684\u200b\u5c5e\u6027\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\uff0c\u200b\u5bf9\u200b\u8868\u683c\u200b\u7684\u200b\u96be\u6613\u200b\u7a0b\u5ea6\u200b\u8fdb\u884c\u200b\u5224\u65ad\u200b\uff0c\u200b\u52a0\u5feb\u200b\u4eba\u5de5\u200b\u8fdb\u884c\u200b\u6821\u5bf9\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200bAI Studio\u200b\u94fe\u63a5\u200b\uff1ahttps://aistudio.baidu.com/aistudio/projectdetail/4588067</p>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#21","title":"2.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<pre><code># \u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u4ee3\u7801\u200b\n! git clone -b dygraph https://gitee.com/paddlepaddle/PaddleOCR\n</code></pre> <pre><code># \u200b\u5b89\u88c5\u200bPaddleOCR\u200b\u73af\u5883\u200b\n! pip install -r PaddleOCR/requirements.txt --force-reinstall\n! pip install protobuf==3.19\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#22","title":"2.2 \u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u672c\u4f8b\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u91c7\u7528\u200b\u8868\u683c\u200b\u751f\u6210\u200b\u5de5\u5177\u200b\u5236\u4f5c\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u89e3\u538b\u200b\uff0c\u200b\u5e76\u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u96c6\u200b\u5927\u5c0f\u200b</p> <pre><code>! cd data/data165849 &amp;&amp; tar -xf table_gen_dataset.tar &amp;&amp; cd -\n! wc -l data/data165849/table_gen_dataset/gt.txt\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#221","title":"2.2.1 \u200b\u5212\u5206\u200b\u8bad\u7ec3\u200b\u6d4b\u8bd5\u200b\u96c6","text":"<p>\u200b\u4f7f\u7528\u200b\u4e0b\u8ff0\u200b\u547d\u4ee4\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u200b\u5212\u5206\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b, \u200b\u8fd9\u91cc\u200b\u5c06\u200b90%\u200b\u5212\u5206\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c10%\u200b\u5212\u5206\u200b\u4e3a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b</p> <pre><code>import random\nwith open('/home/aistudio/data/data165849/table_gen_dataset/gt.txt') as f:\n    lines = f.readlines()\nrandom.shuffle(lines)\ntrain_len = int(len(lines)*0.9)\ntrain_list = lines[:train_len]\nval_list = lines[train_len:]\n\n# \u200b\u4fdd\u5b58\u200b\u7ed3\u679c\u200b\nwith open('/home/aistudio/train.txt','w',encoding='utf-8') as f:\n    f.writelines(train_list)\nwith open('/home/aistudio/val.txt','w',encoding='utf-8') as f:\n    f.writelines(val_list)\n</code></pre> <p>\u200b\u5212\u5206\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u4fe1\u606f\u200b\u5982\u4e0b\u200b</p> \u200b\u7c7b\u578b\u200b \u200b\u6570\u91cf\u200b \u200b\u56fe\u7247\u200b\u5730\u5740\u200b \u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b \u200b\u8bad\u7ec3\u200b\u96c6\u200b 18000 /home/aistudio/data/data165849/table_gen_dataset /home/aistudio/train.txt \u200b\u6d4b\u8bd5\u200b\u96c6\u200b 2000 /home/aistudio/data/data165849/table_gen_dataset /home/aistudio/val.txt"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#222","title":"2.2.2 \u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u96c6","text":"<pre><code>import cv2\nimport os, json\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndef parse_line(data_dir, line):\n    data_line = line.strip(\"\\n\")\n    info = json.loads(data_line)\n    file_name = info['filename']\n    cells = info['html']['cells'].copy()\n    structure = info['html']['structure']['tokens'].copy()\n\n    img_path = os.path.join(data_dir, file_name)\n    if not os.path.exists(img_path):\n        print(img_path)\n        return None\n    data = {\n        'img_path': img_path,\n        'cells': cells,\n        'structure': structure,\n        'file_name': file_name\n    }\n    return data\n\ndef draw_bbox(img_path, points, color=(255, 0, 0), thickness=2):\n    if isinstance(img_path, str):\n        img_path = cv2.imread(img_path)\n    img_path = img_path.copy()\n    for point in points:\n        cv2.polylines(img_path, [point.astype(int)], True, color, thickness)\n    return img_path\n\n\ndef rebuild_html(data):\n    html_code = data['structure']\n    cells = data['cells']\n    to_insert = [i for i, tag in enumerate(html_code) if tag in ('&lt;td&gt;', '&gt;')]\n\n    for i, cell in zip(to_insert[::-1], cells[::-1]):\n        if cell['tokens']:\n            text = ''.join(cell['tokens'])\n            # skip empty text\n            sp_char_list = ['&lt;b&gt;', '&lt;/b&gt;', '\\u2028', ' ', '&lt;i&gt;', '&lt;/i&gt;']\n            text_remove_style = skip_char(text, sp_char_list)\n            if len(text_remove_style) == 0:\n                continue\n            html_code.insert(i + 1, text)\n\n    html_code = ''.join(html_code)\n    return html_code\n\n\ndef skip_char(text, sp_char_list):\n    \"\"\"\n    skip empty cell\n    @param text: text in cell\n    @param sp_char_list: style char and special code\n    @return:\n    \"\"\"\n    for sp_char in sp_char_list:\n        text = text.replace(sp_char, '')\n    return text\n\nsave_dir = '/home/aistudio/vis'\nos.makedirs(save_dir, exist_ok=True)\nimage_dir = '/home/aistudio/data/data165849/'\nhtml_str = '&lt;table border=\"1\"&gt;'\n\n# \u200b\u89e3\u6790\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\u5e76\u200b\u8fd8\u539f\u200bhtml\u200b\u8868\u683c\u200b\ndata = parse_line(image_dir, val_list[0])\n\nimg = cv2.imread(data['img_path'])\nimg_name = ''.join(os.path.basename(data['file_name']).split('.')[:-1])\nimg_save_name = os.path.join(save_dir, img_name)\nboxes = [np.array(x['bbox']) for x in data['cells']]\nshow_img = draw_bbox(data['img_path'], boxes)\ncv2.imwrite(img_save_name + '_show.jpg', show_img)\n\nhtml = rebuild_html(data)\nhtml_str += html\nhtml_str += '&lt;/table&gt;'\n\n# \u200b\u663e\u793a\u200b\u6807\u6ce8\u200b\u7684\u200bhtml\u200b\u5b57\u7b26\u4e32\u200b\nfrom IPython.core.display import display, HTML\ndisplay(HTML(html_str))\n# \u200b\u663e\u793a\u200b\u5355\u5143\u683c\u200b\u5750\u6807\u200b\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#23","title":"2.3 \u200b\u8bad\u7ec3","text":"<p>\u200b\u8fd9\u91cc\u200b\u9009\u7528\u200bPP-StructureV2\u200b\u4e2d\u200b\u7684\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSLANet</p> <p>SLANet\u200b\u662f\u200bPP-StructureV2\u200b\u5168\u65b0\u200b\u63a8\u51fa\u200b\u7684\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u76f8\u6bd4\u200bPP-StructureV1\u200b\u4e2d\u200bTableRec-RARE\uff0c\u200b\u5728\u200b\u901f\u5ea6\u200b\u4e0d\u53d8\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b4.7%\u3002TEDS\u200b\u63d0\u5347\u200b2%</p> \u200b\u7b97\u6cd5\u200b Acc TEDS(Tree-Edit-Distance-based Similarity) Speed EDD<sup>[2]</sup> x 88.30% x TableRec-RARE(ours) 71.73% 93.88% 779ms SLANet(ours) 76.31% 95.89% 766ms <p>\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\u5148\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> <pre><code># \u200b\u8fdb\u5165\u200bPaddleOCR\u200b\u5de5\u4f5c\u200b\u76ee\u5f55\u200b\nos.chdir('/home/aistudio/PaddleOCR')\n# \u200b\u4e0b\u8f7d\u200b\u82f1\u6587\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\n! wget  -nc -P  ./pretrain_models/  https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_train.tar --no-check-certificate\n! cd ./pretrain_models/ &amp;&amp; tar xf en_ppstructure_mobile_v2.0_SLANet_train.tar  &amp;&amp; cd ../\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5373\u53ef\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u7684\u200b\u914d\u7f6e\u200b\u6709\u200b</p> \u200b\u5b57\u200b\u6bb5\u200b \u200b\u4fee\u6539\u200b\u503c\u200b \u200b\u542b\u4e49\u200b Global.pretrained_model ./pretrain_models/en_ppstructure_mobile_v2.0_SLANet_train/best_accuracy.pdparams \u200b\u6307\u5411\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b Global.eval_batch_step 562 \u200b\u6a21\u578b\u200b\u591a\u5c11\u200bstep\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff0c\u200b\u4e00\u822c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u4e00\u4e2a\u200bepoch\u200b\u603b\u200b\u7684\u200bstep\u200b\u6570\u200b Optimizer.lr.name Const \u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u5668\u200b Optimizer.lr.learning_rate 0.0005 \u200b\u5b66\u4e60\u200b\u7387\u8bbe\u200b\u4e3a\u200b\u4e4b\u524d\u200b\u7684\u200b0.05\u200b\u500d\u200b Train.dataset.data_dir /home/aistudio/data/data165849 \u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b Train.dataset.label_file_list /home/aistudio/data/data165849/table_gen_dataset/train.txt \u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b Train.loader.batch_size_per_card 32 \u200b\u8bad\u7ec3\u200b\u65f6\u200b\u6bcf\u5f20\u200b\u5361\u200b\u7684\u200bbatch_size Train.loader.num_workers 1 \u200b\u8bad\u7ec3\u200b\u96c6\u591a\u200b\u8fdb\u7a0b\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u7684\u200b\u8fdb\u7a0b\u200b\u6570\u200b\uff0c\u200b\u5728\u200baistudio\u200b\u4e2d\u200b\u9700\u8981\u200b\u8bbe\u200b\u4e3a\u200b1 Eval.dataset.data_dir /home/aistudio/data/data165849 \u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b Eval.dataset.label_file_list /home/aistudio/data/data165849/table_gen_dataset/val.txt \u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b Eval.loader.batch_size_per_card 32 \u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u6bcf\u5f20\u200b\u5361\u200b\u7684\u200bbatch_size Eval.loader.num_workers 1 \u200b\u6d4b\u8bd5\u200b\u96c6\u591a\u200b\u8fdb\u7a0b\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u7684\u200b\u8fdb\u7a0b\u200b\u6570\u200b\uff0c\u200b\u5728\u200baistudio\u200b\u4e2d\u200b\u9700\u8981\u200b\u8bbe\u200b\u4e3a\u200b1 <p>\u200b\u5df2\u7ecf\u200b\u4fee\u6539\u200b\u597d\u200b\u7684\u200b\u914d\u7f6e\u200b\u5b58\u50a8\u200b\u5728\u200b <code>/home/aistudio/SLANet_ch.yml</code></p> <pre><code>import os\nos.chdir('/home/aistudio/PaddleOCR')\n! python3 tools/train.py -c /home/aistudio/SLANet_ch.yml\n</code></pre> <p>\u200b\u5927\u7ea6\u200b\u5728\u200b7\u200b\u4e2a\u200bepoch\u200b\u540e\u200b\u8fbe\u5230\u200b\u6700\u9ad8\u200b\u7cbe\u5ea6\u200b 97.49%</p>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#24","title":"2.4 \u200b\u9a8c\u8bc1","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b</p> <pre><code>! python3 tools/eval.py -c /home/aistudio/SLANet_ch.yml -o Global.checkpoints=/home/aistudio/PaddleOCR/output/SLANet_ch/best_accuracy.pdparams\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#25","title":"2.5 \u200b\u8bad\u7ec3\u200b\u5f15\u64ce\u200b\u63a8\u7406","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u5f15\u64ce\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b</p> <pre><code>import os;os.chdir('/home/aistudio/PaddleOCR')\n! python3 tools/infer_table.py -c /home/aistudio/SLANet_ch.yml -o Global.checkpoints=/home/aistudio/PaddleOCR/output/SLANet_ch/best_accuracy.pdparams Global.infer_img=/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg\n</code></pre> <pre><code>import cv2\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# \u200b\u663e\u793a\u200b\u539f\u56fe\u200b\nshow_img = cv2.imread('/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n\n# \u200b\u663e\u793a\u200b\u9884\u6d4b\u200b\u7684\u200b\u5355\u5143\u683c\u200b\nshow_img = cv2.imread('/home/aistudio/PaddleOCR/output/infer/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#26","title":"2.6 \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u200b\u5c06\u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e3a\u200binference\u200b\u6a21\u578b\u200b</p> <pre><code>! python3 tools/export_model.py -c /home/aistudio/SLANet_ch.yml -o Global.checkpoints=/home/aistudio/PaddleOCR/output/SLANet_ch/best_accuracy.pdparams Global.save_inference_dir=/home/aistudio/SLANet_ch/infer\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#27","title":"2.7 \u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u63a8\u7406","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u200b\u4f7f\u7528\u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b</p> <pre><code>os.chdir('/home/aistudio/PaddleOCR/ppstructure')\n! python3 table/predict_structure.py \\\n    --table_model_dir=/home/aistudio/SLANet_ch/infer \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --image_dir=/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg \\\n    --output=../output/inference\n</code></pre> <pre><code># \u200b\u663e\u793a\u200b\u539f\u56fe\u200b\nshow_img = cv2.imread('/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n\n# \u200b\u663e\u793a\u200b\u9884\u6d4b\u200b\u7684\u200b\u5355\u5143\u683c\u200b\nshow_img = cv2.imread('/home/aistudio/PaddleOCR/output/inference/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#28","title":"2.8 \u200b\u8868\u683c\u200b\u8bc6\u522b","text":"<p>\u200b\u5728\u200b\u8868\u683c\u200b\u7ed3\u6784\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u200b\u7ed3\u5408\u200bOCR\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5bf9\u200b\u8868\u683c\u200b\u5185\u5bb9\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</p> <pre><code># \u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5e76\u200b\u89e3\u538b\u200b\n! wget  -nc -P  ./inference/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_slim_infer.tar --no-check-certificate\n! wget  -nc -P  ./inference/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_slim_infer.tar --no-check-certificate\n! cd ./inference/ &amp;&amp; tar xf ch_PP-OCRv3_det_slim_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_rec_slim_infer.tar  &amp;&amp; cd ../\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b</p> <pre><code>import os;os.chdir('/home/aistudio/PaddleOCR/ppstructure')\n! python3 table/predict_table.py \\\n    --det_model_dir=inference/ch_PP-OCRv3_det_slim_infer \\\n    --rec_model_dir=inference/ch_PP-OCRv3_rec_slim_infer  \\\n    --table_model_dir=/home/aistudio/SLANet_ch/infer \\\n    --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --image_dir=/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg \\\n    --output=../output/table\n</code></pre> <pre><code># \u200b\u663e\u793a\u200b\u539f\u56fe\u200b\nshow_img = cv2.imread('/home/aistudio/data/data165849/table_gen_dataset/img/no_border_18298_G7XZH93DDCMATGJQ8RW2.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(show_img)\nplt.show()\n\n# \u200b\u663e\u793a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\nfrom IPython.core.display import display, HTML\ndisplay(HTML('&lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;tr&gt;&lt;td colspan=\"5\"&gt;alleadersh&lt;/td&gt;&lt;td rowspan=\"2\"&gt;\u200b\u4e0d\u200b\u8d30\u200b\u8fc7\u200b\uff0c\u200b\u63a8\u200b&lt;/td&gt;&lt;td rowspan=\"2\"&gt;\u200b\u4ece\u200b\u81ea\u5df1\u200b\u53c2\u4e0e\u200b\u6d59\u6c5f\u200b\u6570\u200b&lt;/td&gt;&lt;td rowspan=\"2\"&gt;\u3002\u200b\u53e6\u4e00\u65b9\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;AnSha&lt;/td&gt;&lt;td&gt;\u200b\u81ea\u5df1\u200b\u8d8a\u200b&lt;/td&gt;&lt;td&gt;\u200b\u5171\u5546\u200b\u5171\u5efa\u200b\u5de5\u4f5c\u200b\u534f\u5546\u200b&lt;/td&gt;&lt;td&gt;w.east &lt;/td&gt;&lt;td&gt;\u200b\u6293\u597d\u200b\u6539\u9769\u200b\u8bd5\u70b9\u200b\u4efb\u52a1\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Edime&lt;/td&gt;&lt;td&gt;ImisesElec&lt;/td&gt;&lt;td&gt;\u200b\u6000\u200b\u5929\u4e0b\u200b\u201d\u3002&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;22.26 &lt;/td&gt;&lt;td&gt;31.61&lt;/td&gt;&lt;td&gt;4.30 &lt;/td&gt;&lt;td&gt;794.94&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=\"2\"&gt;ip&lt;/td&gt;&lt;td&gt; Profundi&lt;/td&gt;&lt;td&gt;\uff1a2019\u200b\u5e74\u200b12\u200b\u6708\u200b1&lt;/td&gt;&lt;td&gt;Horspro&lt;/td&gt;&lt;td&gt;444.48&lt;/td&gt;&lt;td&gt;2.41 &lt;/td&gt;&lt;td&gt;87&lt;/td&gt;&lt;td&gt;679.98&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; iehaiTrain&lt;/td&gt;&lt;td&gt;\u200b\u7ec4\u957f\u200b\u848b\u854a\u200b&lt;/td&gt;&lt;td&gt;Toafterdec&lt;/td&gt;&lt;td&gt;203.43&lt;/td&gt;&lt;td&gt;23.54 &lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4266.62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Tyint &lt;/td&gt;&lt;td&gt; roudlyRol&lt;/td&gt;&lt;td&gt;\u200b\u8c22\u200b\u60a8\u200b\u7684\u200b\u597d\u610f\u200b\uff0c\u200b\u6211\u200b\u77e5\u9053\u200b&lt;/td&gt;&lt;td&gt;ErChows&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;48.90&lt;/td&gt;&lt;td&gt;1031&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;NaFlint&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;\u200b\u4e00\u8f88\u200b\u7684\u200b&lt;/td&gt;&lt;td&gt;aterreclam&lt;/td&gt;&lt;td&gt;7823.86&lt;/td&gt;&lt;td&gt;9829.23&lt;/td&gt;&lt;td&gt;7.96 &lt;/td&gt;&lt;td&gt; 3068&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u5bb6\u200b\u4e0a\u4e0b\u6e38\u200b\u4f01\u4e1a\u200b\uff0c5&lt;/td&gt;&lt;td&gt;Tr&lt;/td&gt;&lt;td&gt;\u200b\u666f\u8c61\u200b\u3002\u200b\u5f53\u200b\u5730\u7403\u200b\u4e0a\u200b\u7684\u200b\u6211\u4eec\u200b&lt;/td&gt;&lt;td&gt;Urelaw&lt;/td&gt;&lt;td&gt;799.62&lt;/td&gt;&lt;td&gt;354.96&lt;/td&gt;&lt;td&gt;12.98&lt;/td&gt;&lt;td&gt;33 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u8d5b\u4e8b\u200b\uff08&lt;/td&gt;&lt;td&gt; uestCh&lt;/td&gt;&lt;td&gt;\u200b\u590d\u5236\u200b\u7684\u200b\u4e1a\u52a1\u200b\u6a21\u5f0f\u200b\u5e76\u200b&lt;/td&gt;&lt;td&gt;Listicjust&lt;/td&gt;&lt;td&gt;9.23&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;td&gt;53.22&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; Ca&lt;/td&gt;&lt;td&gt; Iskole&lt;/td&gt;&lt;td&gt;\u200b\u6276\u8d2b\u200b\"\u200b\u4e4b\u540d\u200b\u5f15\u5bfc\u200b&lt;/td&gt;&lt;td&gt; Papua &lt;/td&gt;&lt;td&gt;7191.90&lt;/td&gt;&lt;td&gt;1.65&lt;/td&gt;&lt;td&gt;3.62&lt;/td&gt;&lt;td&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=\"2\"&gt;\u200b\u907f\u8bb3\u200b&lt;/td&gt;&lt;td&gt;ir&lt;/td&gt;&lt;td&gt;\u200b\u4f46\u200b\u7531\u4e8e\u200b&lt;/td&gt;&lt;td&gt;Fficeof&lt;/td&gt;&lt;td&gt;0.22&lt;/td&gt;&lt;td&gt;6.37&lt;/td&gt;&lt;td&gt;7.17&lt;/td&gt;&lt;td&gt;3397.75&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ndaTurk&lt;/td&gt;&lt;td&gt;\u200b\u767e\u5904\u200b\u9057\u5740\u200b&lt;/td&gt;&lt;td&gt;gMa&lt;/td&gt;&lt;td&gt;1288.34&lt;/td&gt;&lt;td&gt;2053.66&lt;/td&gt;&lt;td&gt;2.29&lt;/td&gt;&lt;td&gt;885.45&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;'))\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#31","title":"3.1 \u200b\u4ee3\u7801\u200b\u3001\u200b\u73af\u5883\u200b\u3001\u200b\u6570\u636e\u200b\u51c6\u5907","text":""},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#311","title":"3.1.1 \u200b\u4ee3\u7801\u200b\u51c6\u5907","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0cPaddleClas\u200b\u96c6\u6210\u200b\u4e86\u200bPULC\u200b\u65b9\u6848\u200b\uff0c\u200b\u8be5\u200b\u65b9\u6848\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u83b7\u5f97\u200b\u4e00\u4e2a\u200b\u5728\u200bCPU\u200b\u4e0a\u7528\u200b\u65f6\u200b2ms\u200b\u7684\u200b\u5c5e\u6027\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u3002PaddleClas\u200b\u4ee3\u7801\u200b\u53ef\u4ee5\u200bclone\u200b\u4e0b\u8f7d\u200b\u5f97\u5230\u200b\u3002\u200b\u83b7\u53d6\u200b\u65b9\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>! git clone -b develop https://gitee.com/paddlepaddle/PaddleClas\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#312","title":"3.1.2 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>\u200b\u5176\u6b21\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b\u8bad\u7ec3\u200bPaddleClas\u200b\u76f8\u5173\u200b\u7684\u200b\u4f9d\u8d56\u200b\u5305\u200b</p> <pre><code>! pip install -r PaddleClas/requirements.txt --force-reinstall\n! pip install protobuf==3.20.0\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#313","title":"3.1.3 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u6700\u540e\u200b\uff0c\u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u5171\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u8868\u683c\u200b\u7684\u200b6\u200b\u4e2a\u200b\u5c5e\u6027\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u8868\u683c\u200b\u6765\u6e90\u200b\u3001\u200b\u8868\u683c\u200b\u6570\u91cf\u200b\u3001\u200b\u8868\u683c\u200b\u989c\u8272\u200b\u3001\u200b\u8868\u683c\u200b\u6e05\u6670\u5ea6\u200b\u3001\u200b\u8868\u683c\u200b\u6709\u65e0\u200b\u5e72\u6270\u200b\u3001\u200b\u8868\u683c\u200b\u89d2\u5ea6\u200b\u3002\u200b\u5176\u200b\u53ef\u89c6\u5316\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u7684\u200bdemo\u200b\u5b50\u96c6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u8fed\u4ee3\u200b\u4f53\u9a8c\u200b\u3002\u200b\u4e0b\u8f7d\u65b9\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>%cd PaddleClas/dataset\n!wget https://paddleclas.bj.bcebos.com/data/PULC/table_attribute.tar\n!tar -xf table_attribute.tar\n%cd ../PaddleClas/dataset\n%cd ../\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#32","title":"3.2 \u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u8bc6\u522b\u200b\u8bad\u7ec3","text":"<p>\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u8bad\u7ec3\u200b\u6574\u4f53\u200bpipelinie\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>1.\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u56fe\u7247\u200b\u7ecf\u8fc7\u200b\u9884\u5904\u7406\u200b\u4e4b\u540e\u200b\uff0c\u200b\u9001\u5165\u200b\u5230\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u4e4b\u4e2d\u200b\uff0c\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u5c06\u200b\u62bd\u53d6\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\u7684\u200b\u7279\u5f81\u200b\uff0c\u200b\u6700\u7ec8\u200b\u8be5\u200b\u7279\u5f81\u200b\u8fde\u63a5\u200b\u8f93\u51fa\u200b\u7684\u200bFC\u200b\u5c42\u200b\uff0cFC\u200b\u5c42\u200b\u7ecf\u8fc7\u200bSigmoid\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u540e\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u505a\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f18\u5316\u200b\u5668\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u8be5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u505a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u6765\u200b\u66f4\u65b0\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u591a\u8f6e\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u4e3a\u6b62\u200b\u56fe\u7247\u200b\u505a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u9884\u6d4b\u200b\uff1b</p> <p>2.\u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u56fe\u7247\u200b\u7ecf\u8fc7\u200b\u9884\u5904\u7406\u200b\u4e4b\u540e\u200b\uff0c\u200b\u9001\u5165\u200b\u5230\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u4e4b\u4e2d\u200b\uff0c\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u52a0\u8f7d\u200b\u5b66\u4e60\u200b\u597d\u200b\u7684\u200b\u6743\u91cd\u200b\u540e\u200b\u5bf9\u200b\u8be5\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\u505a\u51fa\u200b\u9884\u6d4b\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e3a\u200b\u4e00\u4e2a\u200b6\u200b\u7ef4\u200b\u5411\u91cf\u200b\uff0c\u200b\u8be5\u200b\u5411\u91cf\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u53cd\u6620\u200b\u4e86\u200b\u6bcf\u4e2a\u200b\u5c5e\u6027\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6982\u7387\u200b\u503c\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u8be5\u503c\u200b\u8fdb\u4e00\u6b65\u200b\u5361\u200b\u9608\u503c\u200b\u4e4b\u540e\u200b\uff0c\u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u6700\u7ec8\u200b\u7684\u200b\u8f93\u51fa\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u8be5\u200b\u8868\u683c\u200b\u7684\u200b6\u200b\u4e2a\u200b\u5c5e\u6027\u200b\u3002</p> <p>\u200b\u5f53\u200b\u51c6\u5907\u200b\u597d\u200b\u76f8\u5173\u200b\u7684\u200b\u6570\u636e\u200b\u4e4b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e00\u952e\u200b\u542f\u52a8\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>!python tools/train.py -c ./ppcls/configs/PULC/table_attribute/PPLCNet_x1_0.yaml -o Global.device=cpu -o Global.epochs=10\n</code></pre>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#33","title":"3.3 \u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u8bc6\u522b\u200b\u63a8\u7406\u200b\u548c\u200b\u90e8\u7f72","text":""},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#331","title":"3.3.1 \u200b\u6a21\u578b\u200b\u8f6c\u6362","text":"<p>\u200b\u5f53\u200b\u8bad\u7ec3\u200b\u597d\u200b\u6a21\u578b\u200b\u4e4b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u63a8\u7406\u6a21\u578b\u200b\u8fdb\u884c\u200b\u90e8\u7f72\u200b\u3002\u200b\u8f6c\u6362\u200b\u811a\u672c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>!python tools/export_model.py -c ppcls/configs/PULC/table_attribute/PPLCNet_x1_0.yaml -o Global.pretrained_model=output/PPLCNet_x1_0/best_model\n</code></pre> <p>\u200b\u6267\u884c\u200b\u4ee5\u4e0a\u200b\u547d\u4ee4\u200b\u4e4b\u540e\u200b\uff0c\u200b\u4f1a\u200b\u5728\u200b\u5f53\u524d\u76ee\u5f55\u200b\u4e0a\u200b\u751f\u6210\u200b<code>inference</code>\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u8be5\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u5f53\u524d\u200b\u7cbe\u5ea6\u200b\u6700\u9ad8\u200b\u7684\u200b\u63a8\u7406\u6a21\u578b\u200b\u3002</p>"},{"location":"en/applications/%E4%B8%AD%E6%96%87%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB.html#332","title":"3.3.2 \u200b\u6a21\u578b\u200b\u63a8\u7406","text":"<p>\u200b\u5b89\u88c5\u200b\u63a8\u7406\u200b\u9700\u8981\u200b\u7684\u200bpaddleclas\u200b\u5305\u200b, \u200b\u6b64\u65f6\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u4e0b\u8f7d\u5b89\u88c5\u200bpaddleclas\u200b\u7684\u200bdevelop\u200b\u7684\u200bwhl\u200b\u5305\u200b</p> <pre><code>!pip install https://paddleclas.bj.bcebos.com/whl/paddleclas-0.0.0-py3-none-any.whl\n</code></pre> <p>\u200b\u8fdb\u5165\u200b<code>deploy</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b</p> <pre><code>%cd deploy/\n</code></pre> <p>\u200b\u63a8\u7406\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>!python python/predict_cls.py -c configs/PULC/table_attribute/inference_table_attribute.yaml -o Global.inference_model_dir=\"../inference\" -o Global.infer_imgs=\"../dataset/table_attribute/Table_val/val_9.jpg\"\n!python python/predict_cls.py -c configs/PULC/table_attribute/inference_table_attribute.yaml -o Global.inference_model_dir=\"../inference\" -o Global.infer_imgs=\"../dataset/table_attribute/Table_val/val_3253.jpg\"\n</code></pre> <p>\u200b\u63a8\u7406\u200b\u7684\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\uff1a</p> <p></p> <p>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>val_9.jpg:   {'attributes': ['Scanned', 'Little', 'Black-and-White', 'Clear', 'Without-Obstacles', 'Horizontal'], 'output': [1, 1, 1, 1, 1, 1]}\n</code></pre> <p>\u200b\u63a8\u7406\u200b\u7684\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\uff1a</p> <p></p> <p>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>val_3253.jpg:    {'attributes': ['Photo', 'Little', 'Black-and-White', 'Blurry', 'Without-Obstacles', 'Tilted'], 'output': [0, 1, 1, 0, 1, 0]}\n</code></pre> <p>\u200b\u5bf9\u6bd4\u200b\u4e24\u5f20\u200b\u56fe\u7247\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u7b2c\u4e00\u5f20\u200b\u56fe\u7247\u200b\u6bd4\u8f83\u200b\u6e05\u6670\u200b\uff0c\u200b\u8868\u683c\u200b\u5c5e\u6027\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e5f\u200b\u504f\u5411\u200b\u4e8e\u200b\u6bd4\u8f83\u200b\u5bb9\u6613\u200b\u8bc6\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u66f4\u200b\u76f8\u4fe1\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u7b2c\u4e8c\u5f20\u200b\u56fe\u7247\u200b\u6bd4\u8f83\u200b\u6a21\u7cca\u200b\uff0c\u200b\u4e14\u200b\u5b58\u5728\u200b\u503e\u659c\u200b\u73b0\u8c61\u200b\uff0c\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u9519\u8bef\u200b\uff0c\u200b\u9700\u8981\u200b\u6211\u4eec\u200b\u4eba\u5de5\u200b\u8fdb\u4e00\u6b65\u200b\u6821\u9a8c\u200b\u3002\u200b\u901a\u8fc7\u200b\u8868\u683c\u200b\u7684\u200b\u5c5e\u6027\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200b\u201c\u200b\u4eba\u5de5\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u667a\u80fd\u200b\u201d\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u4e3a\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u7684\u200b\u843d\u5730\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u63d0\u4f9b\u200b\u4fdd\u969c\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html","title":"\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u6570\u7801\u7ba1\u200b\u5b57\u7b26\u8bc6\u522b","text":""},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u80cc\u666f\u200b\u4ecb\u7ecd","text":"<p>\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\uff08optical power meter \uff09\u200b\u662f\u200b\u6307\u200b\u7528\u4e8e\u200b\u6d4b\u91cf\u200b\u7edd\u5bf9\u200b\u5149\u200b\u529f\u7387\u200b\u6216\u200b\u901a\u8fc7\u200b\u4e00\u6bb5\u200b\u5149\u7ea4\u200b\u7684\u200b\u5149\u200b\u529f\u7387\u200b\u76f8\u5bf9\u200b\u635f\u8017\u200b\u7684\u200b\u4eea\u5668\u200b\u3002\u200b\u5728\u200b\u5149\u7ea4\u200b\u7cfb\u7edf\u200b\u4e2d\u200b\uff0c\u200b\u6d4b\u91cf\u200b\u5149\u200b\u529f\u7387\u200b\u662f\u200b\u6700\u200b\u57fa\u672c\u200b\u7684\u200b\uff0c\u200b\u975e\u5e38\u200b\u50cf\u200b\u7535\u5b50\u5b66\u200b\u4e2d\u200b\u7684\u200b\u4e07\u7528\u8868\u200b\uff1b\u200b\u5728\u200b\u5149\u7ea4\u200b\u6d4b\u91cf\u200b\u4e2d\u200b\uff0c\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u662f\u200b\u91cd\u200b\u8d1f\u8377\u200b\u5e38\u7528\u200b\u8868\u200b\u3002</p> <p></p> <p>\u200b\u76ee\u524d\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u7f3a\u5c11\u200b\u5c06\u200b\u6570\u636e\u200b\u76f4\u63a5\u200b\u8f93\u51fa\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u9700\u8981\u200b\u4eba\u5de5\u200b\u8bfb\u6570\u200b\u3002\u200b\u8fd9\u200b\u4e00\u9879\u200b\u5de5\u4f5c\u200b\u5355\u8c03\u200b\u91cd\u590d\u200b\uff0c\u200b\u5982\u679c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u673a\u5668\u200b\u66ff\u4ee3\u200b\u4eba\u5de5\u200b\uff0c\u200b\u5c06\u200b\u8282\u7ea6\u200b\u5927\u91cf\u200b\u6210\u672c\u200b\u3002\u200b\u9488\u5bf9\u200b\u4e0a\u8ff0\u200b\u95ee\u9898\u200b\uff0c\u200b\u5e0c\u671b\u200b\u901a\u8fc7\u200b\u6444\u50cf\u5934\u200b\u62cd\u7167\u200b-&gt;\u200b\u667a\u80fd\u200b\u8bfb\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\u9ad8\u6548\u200b\u5730\u200b\u5b8c\u6210\u200b\u6b64\u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u4e3a\u200b\u5b9e\u73b0\u200b\u667a\u80fd\u200b\u8bfb\u6570\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f1a\u200b\u91c7\u53d6\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b+\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u65b9\u6848\u200b\uff1a</p> <p>\u200b\u7b2c\u4e00\u6b65\u200b\uff0c\u200b\u4f7f\u7528\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5b9a\u4f4d\u200b\u51fa\u5149\u200b\u529f\u7387\u200b\u8ba1\u4e2d\u200b\u7684\u200b\u6570\u5b57\u200b\u90e8\u5206\u200b\uff1b</p> <p>\u200b\u7b2c\u4e8c\u6b65\u200b\uff0c\u200b\u4f7f\u7528\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u83b7\u5f97\u200b\u51c6\u786e\u200b\u7684\u200b\u6570\u5b57\u200b\u548c\u200b\u5355\u4f4d\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u4e3b\u8981\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5b8c\u6210\u200b\u7b2c\u4e8c\u6b65\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u90e8\u5206\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1a\u200b\u771f\u5b9e\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u7684\u200b\u5efa\u7acb\u200b\u3001\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u5408\u6210\u200b\u3001\u200b\u57fa\u4e8e\u200b PP-OCRv3 \u200b\u548c\u200b SVTR_Tiny \u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8bc4\u4f30\u200b\u548c\u200b\u63a8\u7406\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u96be\u70b9\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u6570\u7801\u7ba1\u200b\u5b57\u7b26\u200b\u6570\u636e\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u96be\u4ee5\u200b\u83b7\u53d6\u200b\u3002</li> <li>\u200b\u6570\u7801\u7ba1\u200b\u4e2d\u200b\u5c0f\u6570\u70b9\u200b\u5360\u200b\u50cf\u7d20\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u5bb9\u6613\u200b\u6f0f\u200b\u8bc6\u522b\u200b\u3002</li> </ul> <p>\u200b\u9488\u5bf9\u200b\u4ee5\u4e0a\u200b\u95ee\u9898\u200b\uff0c \u200b\u672c\u4f8b\u200b\u9009\u7528\u200b PP-OCRv3 \u200b\u548c\u200b SVTR_Tiny \u200b\u4e24\u4e2a\u200b\u9ad8\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u540c\u65f6\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u771f\u5b9e\u200b\u6570\u636e\u6316\u6398\u200b\u6848\u4f8b\u200b\u548c\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u6848\u4f8b\u200b\u3002\u200b\u57fa\u4e8e\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u6784\u5efa\u200b\u7684\u200b\u771f\u5b9e\u200b\u8bc4\u4f30\u200b\u96c6\u4e0a\u200b\u7cbe\u5ea6\u200b\u4ece\u200b 52% \u200b\u63d0\u5347\u200b\u81f3\u200b 72%\uff0cSVTR_Tiny \u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u53ef\u200b\u8fbe\u5230\u200b 78.9%\u3002</p> <p>aistudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b: \u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u6570\u7801\u7ba1\u200b\u5b57\u7b26\u8bc6\u522b\u200b</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#2-paddleocr","title":"2. PaddleOCR \u200b\u5feb\u901f\u200b\u4f7f\u7528","text":"<p>PaddleOCR \u200b\u65e8\u5728\u200b\u6253\u9020\u200b\u4e00\u5957\u200b\u4e30\u5bcc\u200b\u3001\u200b\u9886\u5148\u200b\u3001\u200b\u4e14\u200b\u5b9e\u7528\u200b\u7684\u200bOCR\u200b\u5de5\u5177\u200b\u5e93\u200b\uff0c\u200b\u52a9\u529b\u200b\u5f00\u53d1\u8005\u200b\u8bad\u7ec3\u200b\u51fa\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5e94\u7528\u200b\u843d\u5730\u200b\u3002</p> <p></p> <p>\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u9002\u7528\u200b\u4e8e\u200b\u901a\u7528\u200b\u573a\u666f\u200b\u7684\u200b\u9ad8\u7cbe\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\uff0c\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u56fe\u7247\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u4e0b\u200b\u5f53\u524d\u200b\u6a21\u578b\u200b\u5728\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u573a\u666f\u200b\u4e0a\u200b\u7684\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_2","title":"\u51c6\u5907\u200b\u73af\u5883","text":"<pre><code>python3 -m pip install -U pip\npython3 -m pip install paddleocr\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_3","title":"\u6d4b\u8bd5\u200b\u6548\u679c","text":"<p>\u200b\u6d4b\u8bd5\u200b\u56fe\u200b\uff1a</p> <p></p> <pre><code>paddleocr --lang=ch --det=Fase --image_dir=data\n</code></pre> <p>\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u6d4b\u8bd5\u200b\u7ed3\u679c\u200b\uff1a</p> <pre><code>('.7000', 0.6885431408882141)\n</code></pre> <p>\u200b\u53d1\u73b0\u200b\u6570\u5b57\u200b\u8bc6\u522b\u200b\u8f83\u51c6\u200b\uff0c\u200b\u7136\u800c\u200b\u5bf9\u200b\u8d1f\u53f7\u200b\u548c\u200b\u5c0f\u6570\u70b9\u200b\u8bc6\u522b\u200b\u4e0d\u200b\u51c6\u786e\u200b\u3002 \u200b\u7531\u4e8e\u200bPP-OCRv3\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5927\u591a\u200b\u4e3a\u200b\u901a\u7528\u200b\u573a\u666f\u200b\u6570\u636e\u200b\uff0c\u200b\u5728\u200b\u7279\u5b9a\u200b\u7684\u200b\u573a\u666f\u200b\u4e0a\u200b\u6548\u679c\u200b\u53ef\u80fd\u200b\u4e0d\u591f\u200b\u597d\u200b\u3002\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u57fa\u4e8e\u200b\u573a\u666f\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5fae\u8c03\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u5c31\u200b\u4e3b\u8981\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5728\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\uff08\u200b\u6570\u7801\u7ba1\u200b\uff09\u200b\u573a\u666f\u200b\u4e0a\u200b\u5fae\u8c03\u200b\u8bad\u7ec3\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u5f00\u59cb\u200b\u8bad\u7ec3","text":""},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#31","title":"3.1 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u7279\u5b9a\u200b\u7684\u200b\u5de5\u4e1a\u200b\u573a\u666f\u200b\u5f80\u5f80\u200b\u5f88\u96be\u200b\u83b7\u53d6\u200b\u5f00\u6e90\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u4e5f\u200b\u662f\u200b\u5982\u6b64\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u5de5\u4e1a\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6444\u50cf\u5934\u200b\u91c7\u96c6\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6536\u96c6\u200b\u5927\u91cf\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\uff0c\u200b\u672c\u4f8b\u200b\u4e2d\u200b\u91cd\u70b9\u200b\u4ecb\u7ecd\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u65b9\u6cd5\u200b\u548c\u200b\u771f\u5b9e\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u4f55\u200b\u5229\u7528\u200b\u6709\u9650\u200b\u7684\u200b\u6570\u636e\u200b\u4f18\u5316\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u5206\u4e3a\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff1a\u200b\u5408\u6210\u200b\u6570\u636e\u200b\uff0c\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\uff0c \u200b\u5176\u4e2d\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u7531\u200b text_renderer \u200b\u5de5\u5177\u200b\u6279\u91cf\u200b\u751f\u6210\u200b\u5f97\u5230\u200b\uff0c \u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u901a\u8fc7\u200b\u722c\u866b\u200b\u7b49\u200b\u65b9\u5f0f\u200b\u5728\u200b\u767e\u5ea6\u200b\u56fe\u7247\u200b\u4e2d\u200b\u641c\u7d22\u200b\u5e76\u200b\u4f7f\u7528\u200b PPOCRLabel \u200b\u6807\u6ce8\u200b\u5f97\u5230\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_4","title":"\u5408\u6210\u200b\u6570\u636e","text":"<p>\u200b\u672c\u4f8b\u200b\u4e2d\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u5de5\u5177\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b text_renderer\uff0c \u200b\u8be5\u200b\u5de5\u5177\u200b\u53ef\u4ee5\u200b\u5408\u6210\u200b\u7528\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\u6570\u636e\u200b:</p> <p></p> <p></p> <pre><code>export https_proxy=http://172.19.57.45:3128\ngit clone https://github.com/oh-my-ocr/text_renderer\n</code></pre> <pre><code>python3 setup.py develop\npython3 -m pip install -r docker/requirements.txt\npython3 main.py \\\n    --config example_data/example.py \\\n    --dataset img \\\n    --num_processes 2 \\\n    --log_period 10\n</code></pre> <p>\u200b\u7ed9\u5b9a\u200b\u5b57\u4f53\u200b\u548c\u200b\u8bed\u6599\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5408\u6210\u200b\u8f83\u4e3a\u200b\u4e30\u5bcc\u200b\u6837\u5f0f\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\u6570\u636e\u200b\u3002 \u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u8bc6\u522b\u200b\u573a\u666f\u200b\uff0c\u200b\u76ee\u6807\u200b\u662f\u200b\u6b63\u786e\u200b\u8bc6\u522b\u200b\u6570\u7801\u7ba1\u200b\u6587\u672c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u6536\u96c6\u200b\u90e8\u5206\u200b\u6570\u7801\u7ba1\u200b\u5b57\u4f53\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u8bed\u6599\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5408\u6210\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5c06\u200b\u6536\u96c6\u200b\u597d\u200b\u7684\u200b\u8bed\u6599\u200b\u5b58\u653e\u200b\u5728\u200b example_data \u200b\u8def\u5f84\u200b\u4e0b\u200b\uff1a</p> <pre><code>ln -s ./fonts/DS* text_renderer/example_data/font/\nln -s ./corpus/digital.txt text_renderer/example_data/text/\n</code></pre> <p>\u200b\u4fee\u6539\u200b<code>text_renderer/example_data/font_list/font_list.txt</code>,\u200b\u9009\u62e9\u200b\u9700\u8981\u200b\u7684\u200b\u5b57\u4f53\u200b\u5f00\u59cb\u200b\u5408\u6210\u200b\uff1a</p> <pre><code>python3 main.py \\\n    --config example_data/digital_example.py \\\n    --dataset img \\\n    --num_processes 2 \\\n    --log_period 10\n</code></pre> <p>\u200b\u5408\u6210\u200b\u56fe\u7247\u200b\u4f1a\u200b\u88ab\u200b\u5b58\u5728\u200b\u76ee\u5f55\u200b text_renderer/example_data/digital/chn_data \u200b\u4e0b\u200b</p> <p>\u200b\u67e5\u770b\u200b\u5408\u6210\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u4f8b\u200b\uff1a</p> <p></p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_5","title":"\u771f\u5b9e\u200b\u6570\u636e\u6316\u6398","text":"<p>\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u4f5c\u4e3a\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\uff0c\u200b\u5426\u5219\u200b\u5f88\u200b\u5bb9\u6613\u200b\u8fc7\u200b\u62df\u5408\u200b\u5230\u200b\u7b80\u5355\u200b\u7684\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u4e2d\u200b\u3002\u200b\u6ca1\u6709\u200b\u5f00\u6e90\u200b\u6570\u636e\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u90e8\u5206\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b+\u200b\u6807\u6ce8\u200b\u5de5\u5177\u200b\u83b7\u5f97\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#1_1","title":"1. \u200b\u6570\u636e\u200b\u641c\u96c6","text":"<p>\u200b\u4f7f\u7528\u200b\u722c\u866b\u200b\u5de5\u5177\u200b\u83b7\u5f97\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#2-ppocrlabel","title":"2. PPOCRLabel \u200b\u5b8c\u6210\u200b\u534a\u81ea\u52a8\u200b\u6807\u6ce8","text":"<p>PPOCRLabel\u200b\u662f\u200b\u4e00\u6b3e\u200b\u9002\u7528\u200b\u4e8e\u200bOCR\u200b\u9886\u57df\u200b\u7684\u200b\u534a\u81ea\u52a8\u5316\u200b\u56fe\u5f62\u200b\u6807\u6ce8\u200b\u5de5\u5177\u200b\uff0c\u200b\u5185\u7f6e\u200bPP-OCR\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6570\u636e\u200b\u81ea\u52a8\u200b\u6807\u6ce8\u200b\u548c\u200b\u91cd\u65b0\u200b\u8bc6\u522b\u200b\u3002\u200b\u4f7f\u7528\u200bPython3\u200b\u548c\u200bPyQT5\u200b\u7f16\u5199\u200b\uff0c\u200b\u652f\u6301\u200b\u77e9\u5f62\u6846\u200b\u6807\u6ce8\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u6ce8\u200b\u3001\u200b\u4e0d\u89c4\u5219\u200b\u6587\u672c\u200b\u6807\u6ce8\u200b\u3001\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u6807\u6ce8\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5bfc\u51fa\u200b\u683c\u5f0f\u200b\u53ef\u200b\u76f4\u63a5\u200b\u7528\u4e8e\u200bPaddleOCR\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002</p> <p></p> <p>\u200b\u6536\u96c6\u200b\u5b8c\u200b\u6570\u636e\u200b\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u5206\u914d\u200b\u4e86\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u96c6\u4e2d\u200b\u4e00\u822c\u200b\u90fd\u200b\u662f\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u5305\u542b\u200b\u5408\u6210\u200b\u6570\u636e\u200b+\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u3002\u200b\u672c\u4f8b\u200b\u4e2d\u200b\u6807\u6ce8\u200b\u4e86\u200b155\u200b\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u6570\u76ee\u200b\u4e3a\u200b100\u200b\u548c\u200b55\u3002</p> <p>\u200b\u6700\u7ec8\u200b <code>data</code> \u200b\u6587\u4ef6\u5939\u200b\u5e94\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u51e0\u200b\u90e8\u5206\u200b\uff1a</p> <pre><code>|-data\n  |- synth_train.txt\n  |- real_train.txt\n  |- real_eval.txt\n  |- synthetic_data\n      |- word_001.png\n      |- word_002.jpg\n      |- word_003.jpg\n      | ...\n  |- real_data\n      |- word_001.png\n      |- word_002.jpg\n      |- word_003.jpg\n      | ...\n  ...\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#32","title":"3.2 \u200b\u6a21\u578b\u200b\u9009\u62e9","text":"<p>\u200b\u672c\u200b\u6848\u4f8b\u200b\u63d0\u4f9b\u200b\u4e86\u200b2\u200b\u79cd\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff1aPP-OCRv3 \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b \u200b\u548c\u200b SVTR_Tiny\uff1a</p> <p>PP-OCRv3 \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff1aPP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u5e76\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u7ed3\u6784\u200b\u6539\u8fdb\u200b\u52a0\u901f\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u3002</p> <p>SVTR_Tiny:SVTR\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u7528\u4e8e\u200b\u573a\u666f\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u5355\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u8be5\u200b\u6a21\u578b\u200b\u5728\u200bpatch-wise image tokenization\u200b\u6846\u67b6\u200b\u5185\u200b\uff0c\u200b\u5b8c\u5168\u200b\u6452\u5f03\u200b\u4e86\u200b\u5e8f\u5217\u200b\u5efa\u6a21\u200b\uff0c\u200b\u5728\u200b\u7cbe\u5ea6\u200b\u5177\u6709\u200b\u7ade\u4e89\u529b\u200b\u7684\u200b\u524d\u63d0\u200b\u4e0b\u200b\uff0c\u200b\u6a21\u578b\u200b\u53c2\u200b\u6570\u91cf\u200b\u66f4\u200b\u5c11\u200b\uff0c\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u4ee5\u4e0a\u200b\u4e24\u4e2a\u200b\u7b56\u7565\u200b\u5728\u200b\u81ea\u5efa\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u548c\u200b\u901f\u5ea6\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\uff08CPU + MKLDNN) 01 PP-OCRv2 8M 74.80% 8.54ms 02 SVTR_Tiny 21M 80.10% 97.00ms 03 SVTR_LCNet(h32) 12M 71.90% 6.60ms 04 SVTR_LCNet(h48) 12M 73.98% 7.60ms 05 + GTC 12M 75.80% 7.60ms 06 + TextConAug 12M 76.30% 7.60ms 07 + TextRotNet 12M 76.90% 7.60ms 08 + UDML 12M 78.40% 7.60ms 09 + UIM 12M 79.40% 7.60ms"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#33","title":"3.3 \u200b\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b PaddleOCR \u200b\u4ee3\u7801\u200b\u5e93\u200b</p> <pre><code>git clone -b release/2.5 https://github.com/PaddlePaddle/PaddleOCR.git\n</code></pre> <p>PaddleOCR\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u811a\u672c\u200b\u3001\u200b\u8bc4\u4f30\u200b\u811a\u672c\u200b\u548c\u200b\u9884\u6d4b\u200b\u811a\u672c\u200b\uff0c\u200b\u672c\u8282\u200b\u5c06\u200b\u4ee5\u200b PP-OCRv3 \u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\uff1a</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step1","title":"Step1\uff1a\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b pretrain model\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4e0b\u8f7d\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200bfinetune</p> <pre><code>cd PaddleOCR/\n# \u200b\u4e0b\u8f7d\u200bPP-OCRv3 \u200b\u4e2d\u6587\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\n# \u200b\u89e3\u538b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\ncd pretrain_models\ntar -xf ch_PP-OCRv3_rec_train.tar &amp;&amp; rm -rf ch_PP-OCRv3_rec_train.tar\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step2","title":"Step2\uff1a\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b\u6587\u4ef6","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\u9700\u8981\u200b\u63d0\u4f9b\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\uff08{word_dict_name}.txt\uff09\uff0c\u200b\u4f7f\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6240\u6709\u200b\u51fa\u73b0\u200b\u7684\u200b\u5b57\u7b26\u200b\u6620\u5c04\u200b\u4e3a\u200b\u5b57\u5178\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\u5b57\u5178\u200b\u9700\u8981\u200b\u5305\u542b\u200b\u6240\u6709\u200b\u5e0c\u671b\u200b\u88ab\u200b\u6b63\u786e\u200b\u8bc6\u522b\u200b\u7684\u200b\u5b57\u7b26\u200b\uff0c{word_dict_name}.txt\u200b\u9700\u8981\u200b\u5199\u6210\u200b\u5982\u4e0b\u200b\u683c\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u4ee5\u200b <code>utf-8</code> \u200b\u7f16\u7801\u200b\u683c\u5f0f\u200b\u4fdd\u5b58\u200b\uff1a</p> <pre><code>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-\n.\n</code></pre> <p>word_dict.txt \u200b\u6bcf\u884c\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5355\u5b57\u200b\uff0c\u200b\u5c06\u200b\u5b57\u7b26\u200b\u4e0e\u200b\u6570\u5b57\u200b\u7d22\u5f15\u200b\u6620\u5c04\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u201c3.14\u201d \u200b\u5c06\u200b\u88ab\u200b\u6620\u5c04\u200b\u6210\u200b [3, 11, 1, 4]</p> <ul> <li>\u200b\u5185\u7f6e\u200b\u5b57\u5178\u200b</li> </ul> <p>PaddleOCR\u200b\u5185\u7f6e\u200b\u4e86\u200b\u4e00\u90e8\u5206\u200b\u5b57\u5178\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6309\u200b\u9700\u200b\u4f7f\u7528\u200b\u3002</p> <p><code>ppocr/utils/ppocr_keys_v1.txt</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b6623\u200b\u4e2a\u5b57\u7b26\u200b\u7684\u200b\u4e2d\u6587\u200b\u5b57\u5178\u200b</p> <p><code>ppocr/utils/ic15_dict.txt</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b36\u200b\u4e2a\u5b57\u7b26\u200b\u7684\u200b\u82f1\u6587\u200b\u5b57\u5178\u200b</p> <ul> <li>\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b</li> </ul> <p>\u200b\u5185\u7f6e\u200b\u5b57\u5178\u200b\u9762\u5411\u200b\u901a\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u5177\u4f53\u200b\u7684\u200b\u5de5\u4e1a\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8bc6\u522b\u200b\u7279\u6b8a\u5b57\u7b26\u200b\uff0c\u200b\u6216\u8005\u200b\u53ea\u200b\u9700\u200b\u8bc6\u522b\u200b\u67d0\u200b\u51e0\u4e2a\u200b\u5b57\u7b26\u200b\uff0c\u200b\u6b64\u65f6\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b\u4f1a\u200b\u66f4\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u4f8b\u5982\u200b\u5728\u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u9700\u8981\u200b\u8bc6\u522b\u200b\u6570\u5b57\u200b\u548c\u200b\u5355\u4f4d\u200b\u3002</p> <p>\u200b\u904d\u5386\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u6807\u7b7e\u200b\u4e2d\u200b\u7684\u200b\u5b57\u7b26\u200b\uff0c\u200b\u5236\u4f5c\u200b\u5b57\u5178\u200b<code>digital_dict.txt</code>\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>-\n.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\nE\nF\nH\nL\nN\nT\nW\nd\nk\nm\nn\no\nz\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step3","title":"Step3\uff1a\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u4e3a\u4e86\u200b\u66f4\u597d\u200b\u7684\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200bch_PP-OCRv3_rec_distillation.yml\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u5e76\u200b\u53c2\u8003\u200b\u4e0b\u5217\u200b\u8bf4\u660e\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff1a</p> <p>\u200b\u4ee5\u200b <code>ch_PP-OCRv3_rec_distillation.yml</code> \u200b\u4e3a\u4f8b\u200b\uff1a</p> <pre><code>Global:\n  ...\n  # \u200b\u6dfb\u52a0\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b\uff0c\u200b\u5982\u200b\u4fee\u6539\u200b\u5b57\u5178\u200b\u8bf7\u200b\u5c06\u200b\u8def\u5f84\u200b\u6307\u5411\u200b\u65b0\u200b\u5b57\u5178\u200b\n  character_dict_path: ppocr/utils/dict/digital_dict.txt\n  ...\n  # \u200b\u8bc6\u522b\u200b\u7a7a\u683c\u200b\n  use_space_char: True\n\n\nOptimizer:\n  ...\n  # \u200b\u6dfb\u52a0\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u7b56\u7565\u200b\n  lr:\n    name: Cosine\n    learning_rate: 0.001\n  ...\n\n...\n\nTrain:\n  dataset:\n    # \u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\uff0c\u200b\u652f\u6301\u200bLMDBDataSet\u200b\u4ee5\u53ca\u200bSimpleDataSet\n    name: SimpleDataSet\n    # \u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\n    data_dir: ./data/\n    # \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\n    label_file_list:\n    - ./train_data/digital_img/digital_train.txt  #11w\n    - ./train_data/digital_img/real_train.txt     #100\n    - ./train_data/digital_img/dbm_img/dbm.txt    #3w\n    ratio_list:\n    - 0.3\n    - 1.0\n    - 1.0\n    transforms:\n      ...\n      - RecResizeImg:\n          # \u200b\u4fee\u6539\u200b image_shape \u200b\u4ee5\u200b\u9002\u5e94\u200b\u957f\u200b\u6587\u672c\u200b\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    ...\n    # \u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\u7684\u200bbatch_size\n    batch_size_per_card: 256\n    ...\n\nEval:\n  dataset:\n    # \u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\uff0c\u200b\u652f\u6301\u200bLMDBDataSet\u200b\u4ee5\u53ca\u200bSimpleDataSet\n    name: SimpleDataSet\n    # \u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\n    data_dir: ./data\n    # \u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\n    label_file_list:\n    - ./train_data/digital_img/real_val.txt\n    transforms:\n      ...\n      - RecResizeImg:\n          # \u200b\u4fee\u6539\u200b image_shape \u200b\u4ee5\u200b\u9002\u5e94\u200b\u957f\u200b\u6587\u672c\u200b\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    # \u200b\u5355\u5361\u200b\u9a8c\u8bc1\u200b\u7684\u200bbatch_size\n    batch_size_per_card: 256\n    ...\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8bad\u7ec3\u200b/\u200b\u9884\u6d4b\u200b/\u200b\u8bc4\u4f30\u200b\u65f6\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u8bf7\u200b\u52a1\u5fc5\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u4e00\u81f4\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step4","title":"Step4\uff1a\u200b\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5b89\u88c5\u200b\u7684\u200b\u662f\u200bcpu\u200b\u7248\u672c\u200b\uff0c\u200b\u8bf7\u200b\u5c06\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b <code>use_gpu</code> \u200b\u5b57\u200b\u6bb5\u200b\u4fee\u6539\u200b\u4e3a\u200bfalse</p> <pre><code># GPU\u200b\u8bad\u7ec3\u200b \u200b\u652f\u6301\u200b\u5355\u5361\u200b\uff0c\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\n# \u200b\u8bad\u7ec3\u200b\u6570\u7801\u7ba1\u200b\u6570\u636e\u200b \u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u4fdd\u5b58\u200b\u4e3a\u200b \"{save_model_dir}\" \u200b\u4e0b\u200b\u7684\u200btrain.log\n\n#\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff08\u200b\u8bad\u7ec3\u200b\u5468\u671f\u957f\u200b\uff0c\u200b\u4e0d\u200b\u5efa\u8bae\u200b\uff09\npython3 tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=./pretrain_models/ch_PP-OCRv3_rec_train/best_accuracy\n\n# \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u8fc7\u200b--gpus\u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=./pretrain_models/en_PP-OCRv3_rec_train/best_accuracy\n</code></pre> <p>PaddleOCR\u200b\u652f\u6301\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u4ea4\u66ff\u200b\u8fdb\u884c\u200b, \u200b\u53ef\u4ee5\u200b\u5728\u200b <code>configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml</code> \u200b\u4e2d\u200b\u4fee\u6539\u200b <code>eval_batch_step</code> \u200b\u8bbe\u7f6e\u200b\u8bc4\u4f30\u200b\u9891\u7387\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u6bcf\u200b500\u200b\u4e2a\u200biter\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\u3002\u200b\u8bc4\u4f30\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9ed8\u8ba4\u200b\u5c06\u200b\u6700\u4f73\u200bacc\u200b\u6a21\u578b\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u4e3a\u200b <code>output/ch_PP-OCRv3_rec_distill/best_accuracy</code> \u3002</p> <p>\u200b\u5982\u679c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u5f88\u5927\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u5c06\u4f1a\u200b\u6bd4\u8f83\u200b\u8017\u65f6\u200b\uff0c\u200b\u5efa\u8bae\u200b\u51cf\u5c11\u200b\u8bc4\u4f30\u200b\u6b21\u6570\u200b\uff0c\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u518d\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#svtr_tiny","title":"SVTR_Tiny \u200b\u8bad\u7ec3","text":"<p>SVTR_Tiny \u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u4e00\u81f4\u200b\uff0cSVTR\u200b\u652f\u6301\u200b\u7684\u200b\u914d\u7f6e\u200b\u548c\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd\u200b\u6587\u6863\u200b</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step1_1","title":"Step1\uff1a\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<pre><code># \u200b\u4e0b\u8f7d\u200b SVTR_Tiny \u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u548c\u200b\u914d\u7f6e\u6587\u4ef6\u200b\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/rec_svtr_tiny_none_ctc_ch_train.tar\n# \u200b\u89e3\u538b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\ntar -xf rec_svtr_tiny_none_ctc_ch_train.tar &amp;&amp; rm -rf rec_svtr_tiny_none_ctc_ch_train.tar\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step2_1","title":"Step2\uff1a\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b\u6587\u4ef6","text":"<p>\u200b\u5b57\u5178\u200b\u4f9d\u7136\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b digital_dict.txt</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step3_1","title":"Step3\uff1a\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u5bf9\u5e94\u200b\u4fee\u6539\u200b\u5b57\u5178\u200b\u8def\u5f84\u200b\u548c\u200b\u6570\u636e\u200b\u8def\u5f84\u200b</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#step4_1","title":"Step4\uff1a\u200b\u542f\u52a8\u200b\u8bad\u7ec3","text":"<pre><code># \u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\npython tools/train.py -c rec_svtr_tiny_none_ctc_ch_train/rec_svtr_tiny_6local_6global_stn_ch.yml \\\n           -o Global.pretrained_model=./rec_svtr_tiny_none_ctc_ch_train/best_accuracy\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#34","title":"3.4 \u200b\u9a8c\u8bc1\u200b\u6548\u679c","text":"<p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</p>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_6","title":"\u6307\u6807\u200b\u8bc4\u4f30","text":"<p>\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>Global.save_model_dir</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u3002\u200b\u5728\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b<code>Global.checkpoints</code>\u200b\u6307\u5411\u200b\u4fdd\u5b58\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\u3002\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml</code>  \u200b\u4fee\u6539\u200bEval\u200b\u4e2d\u200b\u7684\u200b <code>label_file_path</code> \u200b\u8bbe\u7f6e\u200b\u3002</p> <pre><code># GPU \u200b\u8bc4\u4f30\u200b\uff0c Global.checkpoints \u200b\u4e3a\u5f85\u6d4b\u200b\u6743\u91cd\u200b\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.checkpoints={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/applications/%E5%85%89%E5%8A%9F%E7%8E%87%E8%AE%A1%E6%95%B0%E7%A0%81%E7%AE%A1%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.html#_7","title":"\u6d4b\u8bd5\u200b\u8bc6\u522b\u200b\u6548\u679c","text":"<p>\u200b\u4f7f\u7528\u200b PaddleOCR \u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u811a\u672c\u200b\u8fdb\u884c\u200b\u5feb\u901f\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u9ed8\u8ba4\u200b\u9884\u6d4b\u200b\u56fe\u7247\u200b\u5b58\u50a8\u200b\u5728\u200b <code>infer_img</code> \u200b\u91cc\u200b\uff0c\u200b\u901a\u8fc7\u200b <code>-o Global.checkpoints</code> \u200b\u52a0\u8f7d\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\uff1a</p> <p>\u200b\u6839\u636e\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u7684\u200b <code>save_model_dir</code> \u200b\u548c\u200b <code>save_epoch_step</code> \u200b\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u4f1a\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\u53c2\u6570\u200b\u88ab\u200b\u4fdd\u5b58\u200b\u4e0b\u6765\u200b\uff1a</p> <pre><code>output/rec/\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.pdparams\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 iter_epoch_3.pdopt\n\u251c\u2500\u2500 iter_epoch_3.pdparams\n\u251c\u2500\u2500 iter_epoch_3.states\n\u251c\u2500\u2500 latest.pdopt\n\u251c\u2500\u2500 latest.pdparams\n\u251c\u2500\u2500 latest.states\n\u2514\u2500\u2500 train.log\n</code></pre> <p>\u200b\u5176\u4e2d\u200b best_accuracy.\u200b\u662f\u200b\u8bc4\u4f30\u200b\u96c6\u4e0a\u200b\u7684\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\uff1biter_epoch_x. \u200b\u662f\u200b\u4ee5\u200b <code>save_epoch_step</code> \u200b\u4e3a\u200b\u95f4\u9694\u200b\u4fdd\u5b58\u200b\u4e0b\u6765\u200b\u7684\u200b\u6a21\u578b\u200b\uff1blatest.* \u200b\u662f\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200bepoch\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <pre><code># \u200b\u9884\u6d4b\u200b\u82f1\u6587\u200b\u7ed3\u679c\u200b\npython3 tools/infer_rec.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.infer_img=test_digital.png\n</code></pre> <p>\u200b\u9884\u6d4b\u200b\u56fe\u7247\u200b\uff1a</p> <p></p> <p>\u200b\u5f97\u5230\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff1a</p> <pre><code>infer_img: test_digital.png\n        result: ('-70.00', 0.9998967)\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html","title":"\u4e00\u79cd\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u4ea7\u54c1\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":""},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u4ecb\u7ecd","text":"<p>\u200b\u4ea7\u54c1\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u6280\u672f\u200b\u5728\u200b\u5de5\u4e1a\u200b\u573a\u666f\u200b\u4e2d\u200b\u7684\u200b\u4e00\u79cd\u200b\u5e94\u7528\u200b\u3002\u200b\u4ea7\u54c1\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u8bc6\u522b\u200b\u6280\u672f\u200b\u8981\u6c42\u200b\u80fd\u591f\u200b\u5c06\u200b\u4ea7\u54c1\u200b\u751f\u4ea7\u65e5\u671f\u200b\u4ece\u200b\u590d\u6742\u200b\u80cc\u666f\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u5e76\u200b\u8bc6\u522b\u200b\u51fa\u6765\u200b\uff0c\u200b\u5728\u200b\u7269\u6d41\u200b\u7ba1\u7406\u200b\u3001\u200b\u7269\u8d44\u200b\u7ba1\u7406\u200b\u4e2d\u200b\u5f97\u5230\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u3002</p> <p></p> <ul> <li> <p>\u200b\u9879\u76ee\u200b\u96be\u70b9\u200b</p> </li> <li> <p>\u200b\u6ca1\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b</p> </li> <li>\u200b\u56fe\u50cf\u200b\u8d28\u91cf\u200b\u5c42\u6b21\u200b\u4e0d\u9f50\u200b: \u200b\u89d2\u5ea6\u200b\u503e\u659c\u200b\u3001\u200b\u56fe\u7247\u200b\u6a21\u7cca\u200b\u3001\u200b\u5149\u7167\u200b\u4e0d\u8db3\u200b\u3001\u200b\u8fc7\u200b\u66dd\u200b\u7b49\u200b\u95ee\u9898\u200b\u4e25\u91cd\u200b</li> </ul> <p>\u200b\u9488\u5bf9\u200b\u4ee5\u4e0a\u200b\u95ee\u9898\u200b\uff0c \u200b\u672c\u4f8b\u200b\u9009\u7528\u200bPP-OCRv3\u200b\u8fd9\u4e00\u200b\u5f00\u6e90\u200b\u8d85\u200b\u8f7b\u91cf\u200bOCR\u200b\u7cfb\u7edf\u200b\u8fdb\u884c\u200b\u5305\u88c5\u200b\u4ea7\u54c1\u200b\u751f\u4ea7\u65e5\u671f\u200b\u8bc6\u522b\u7cfb\u7edf\u200b\u7684\u200b\u5f00\u53d1\u200b\u3002\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b62.99%\u3002\u200b\u4e3a\u200b\u63d0\u5347\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u5de5\u5177\u200b\u5408\u6210\u200b\u4e86\u200b3k\u200b\u6570\u636e\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u8fd9\u90e8\u5206\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b73.66%\u3002\u200b\u7531\u4e8e\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5206\u5e03\u200b\u5b58\u5728\u200b\u5dee\u5f02\u200b\uff0c\u200b\u4e3a\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7f51\u7edc\u200b\u722c\u866b\u200b\u914d\u5408\u200b\u6570\u636e\u6316\u6398\u200b\u7b56\u7565\u200b\u5f97\u5230\u200b\u4e86\u200b1k\u200b\u5e26\u200b\u6807\u7b7e\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b71.33%\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u7efc\u5408\u200b\u4f7f\u7528\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u548c\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u5c06\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b86.99%\u3002\u200b\u5404\u200b\u7b56\u7565\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u7b56\u7565\u200b \u200b\u7cbe\u5ea6\u200b PP-OCRv3\u200b\u8bc4\u4f30\u200b 62.99 \u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune 73.66 \u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune 71.33 \u200b\u771f\u5b9e\u200b+\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune 86.99 <p>AIStudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b\uff1a \u200b\u4e00\u79cd\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u8bc6\u522b\u65b9\u6cd5\u200b</p>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u73af\u5883\u200b\u642d\u5efa","text":"<p>\u200b\u672c\u200b\u4efb\u52a1\u200b\u57fa\u4e8e\u200bAistudio\u200b\u5b8c\u6210\u200b, \u200b\u5177\u4f53\u200b\u73af\u5883\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b: Linux</li> <li>PaddlePaddle: 2.3</li> <li>PaddleOCR: Release/2.5</li> <li>text_renderer: master</li> </ul> <p>\u200b\u4e0b\u8f7d\u200bPaddlleOCR\u200b\u4ee3\u7801\u200b\u5e76\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5e93\u200b:</p> <pre><code>git clone -b dygraph https://gitee.com/paddlepaddle/PaddleOCR\n\n# \u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5e93\u200b\ncd PaddleOCR\npip install -r PaddleOCR/requirements.txt\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u4f7f\u7528\u200b\u4eba\u5de5\u200b\u9884\u200b\u6807\u6ce8\u200b\u7684\u200b300\u200b\u5f20\u200b\u56fe\u50cf\u200b\u4f5c\u4e3a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> <p>\u200b\u90e8\u5206\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>\u200b\u6570\u636e\u200b\u8def\u5f84\u200b \u200b\u6807\u7b7e\u200b\uff08\u200b\u4e2d\u95f4\u200b\u4ee5\u200b\u5236\u8868\u7b26\u200b\u5206\u9694\u200b\uff09\n</code></pre> \u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u578b\u200b \u200b\u6570\u91cf\u200b \u200b\u6d4b\u8bd5\u200b\u96c6\u200b 300 <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u540e\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e0b\u65b9\u200b\u547d\u4ee4\u200b\u89e3\u538b\u200b:</p> <pre><code>tar -xvf data.tar\nmv data ${PaddleOCR_root}\n</code></pre> <p>\u200b\u6570\u636e\u200b\u89e3\u538b\u200b\u540e\u200b\u7684\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 mining_images            # \u200b\u6316\u6398\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\n\u2502   \u251c\u2500\u2500 mining_train.list        # \u200b\u6316\u6398\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u6587\u4ef6\u200b\u5217\u8868\u200b\n\u2502   \u251c\u2500\u2500 render_images            # \u200b\u5408\u6210\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\n\u2502   \u251c\u2500\u2500 render_train.list        # \u200b\u5408\u6210\u200b\u6570\u636e\u6587\u4ef6\u200b\u5217\u8868\u200b\n\u2502   \u251c\u2500\u2500 val                      # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6570\u636e\u200b\n\u2502   \u2514\u2500\u2500 val.list                 # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6570\u636e\u6587\u4ef6\u200b\u5217\u8868\u200b\n|   \u251c\u2500\u2500 bg                       # \u200b\u5408\u6210\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u80cc\u666f\u200b\u56fe\u50cf\u200b\n\u2502   \u2514\u2500\u2500 corpus                   # \u200b\u5408\u6210\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u8bed\u6599\u200b\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#4-pp-ocrv3","title":"4. \u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u51c6\u5907\u200b\u597d\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u7684\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\u3002</p>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#_1","title":"\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u4e0b\u8f7d\u200bPP-OCR v3\u200b\u4e2d\u82f1\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u53ef\u4ee5\u200b\u5728\u200blink\u200b\u83b7\u53d6\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u547d\u4ee4\u200b:</p> <pre><code>cd ${PaddleOCR_root}\nmkdir ckpt\nwget -nc -P ckpt https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\npushd ckpt/\ntar -xvf ch_PP-OCRv3_rec_train.tar\npopd\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#_2","title":"\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200bPP-OCRv3\u200b\u8bc4\u4f30\u200b:</p> <pre><code>python tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml \\\n                         -o Global.checkpoints=ckpt/ch_PP-OCRv3_rec_train/best_accuracy \\\n                         Eval.dataset.data_dir=./data \\\n                         Eval.dataset.label_file_list=[\"./data/val.list\"]\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\u5404\u200b\u53c2\u6570\u200b\u542b\u4e49\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>-c: \u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0cch_PP-OCRv3_rec_distillation.yml\u200b\u5bf9\u5e94\u200b\u4e8e\u200bOCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u3002\n-o: \u200b\u8986\u76d6\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u53c2\u6570\u200b\nGlobal.checkpoints: \u200b\u6307\u5b9a\u200b\u8bc4\u4f30\u200b\u4f7f\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\nEval.dataset.data_dir: \u200b\u6307\u5b9a\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\nEval.dataset.label_file_list: \u200b\u6307\u5b9a\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#5-finetune","title":"5. \u200b\u57fa\u4e8e\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune","text":""},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#51-text-renderer","title":"5.1 Text Renderer\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u65b9\u6cd5","text":""},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#511-text-renderer","title":"5.1.1 \u200b\u4e0b\u8f7d\u200bText Renderer\u200b\u4ee3\u7801","text":"<p>\u200b\u9996\u5148\u200b\u4ece\u200bgithub\u200b\u6216\u200bgitee\u200b\u4e0b\u8f7d\u200bText Renderer\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5e76\u200b\u5b89\u88c5\u200b\u76f8\u5173\u200b\u4f9d\u8d56\u200b\u3002</p> <pre><code>git clone https://gitee.com/wowowoll/text_renderer.git\n\n# \u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5e93\u200b\ncd text_renderer\npip install -r requirements.txt\n</code></pre> <p>\u200b\u4f7f\u7528\u200btext renderer\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u4e4b\u524d\u200b\u9700\u8981\u200b\u51c6\u5907\u200b\u597d\u200b\u80cc\u666f\u56fe\u7247\u200b\u3001\u200b\u8bed\u6599\u200b\u4ee5\u53ca\u200b\u5b57\u4f53\u5e93\u200b\uff0c\u200b\u4e0b\u9762\u200b\u5c06\u200b\u9010\u4e00\u200b\u4ecb\u7ecd\u200b\u5404\u4e2a\u200b\u6b65\u9aa4\u200b\u3002</p>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#512","title":"5.1.2 \u200b\u51c6\u5907\u200b\u80cc\u666f\u56fe\u7247","text":"<p>\u200b\u89c2\u5bdf\u200b\u65e5\u5e38\u751f\u6d3b\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u56fe\u7247\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u5176\u200b\u80cc\u666f\u200b\u76f8\u5bf9\u200b\u7b80\u5355\u200b\u3002\u200b\u4e3a\u6b64\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u7f51\u4e0a\u200b\u627e\u200b\u4e00\u4e0b\u200b\u56fe\u7247\u200b\uff0c\u200b\u622a\u53d6\u200b\u90e8\u5206\u200b\u56fe\u50cf\u200b\u5757\u200b\u4f5c\u4e3a\u200b\u80cc\u666f\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u5df2\u200b\u51c6\u5907\u200b\u4e86\u200b\u90e8\u5206\u200b\u56fe\u50cf\u200b\u4f5c\u4e3a\u200b\u80cc\u666f\u56fe\u7247\u200b\uff0c\u200b\u5728\u200b\u7b2c\u200b3\u200b\u90e8\u5206\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b,\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u7684\u200b\u80cc\u666f\u200b\u56fe\u50cf\u200b\uff0c\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u80cc\u666f\u200b\u56fe\u50cf\u200b\u5b58\u653e\u200b\u4e8e\u200b\u5982\u4e0b\u200b\u4f4d\u7f6e\u200b\uff1a</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\uff5c   \u251c\u2500\u2500 bg     # \u200b\u5408\u6210\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u80cc\u666f\u200b\u56fe\u50cf\u200b\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#513","title":"5.1.3 \u200b\u51c6\u5907\u200b\u8bed\u6599","text":"<p>\u200b\u89c2\u5bdf\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u751f\u4ea7\u65e5\u671f\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u77e5\u9053\u200b\u5982\u4e0b\u200b\u6570\u636e\u200b\u6709\u200b\u5982\u4e0b\u200b\u7279\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u7531\u200b\u5e74\u6708\u65e5\u200b\u7ec4\u6210\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u53ef\u80fd\u200b\u4ee5\u200b\u201c/\u201d\u3001\u201c-\u201d\u3001\u201c:\u201d\u3001\u201c.\u201d\u200b\u6216\u8005\u200b\u7a7a\u683c\u200b\u95f4\u9694\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u4ee5\u200b\u6c49\u5b57\u200b\u5e74\u6708\u65e5\u200b\u5206\u9694\u200b</li> <li>\u200b\u6709\u4e9b\u200b\u751f\u4ea7\u65e5\u671f\u200b\u5305\u542b\u200b\u5728\u200b\u4ea7\u54c1\u200b\u6279\u53f7\u200b\u4e2d\u200b\uff0c\u200b\u6b64\u65f6\u200b\u53ef\u80fd\u200b\u5305\u542b\u200b\u5177\u4f53\u200b\u65f6\u95f4\u200b\u3001\u200b\u82f1\u6587\u5b57\u6bcd\u200b\u6216\u200b\u6570\u5b57\u200b\u6807\u8bc6\u200b</li> </ol> <p>\u200b\u57fa\u4e8e\u200b\u4ee5\u4e0a\u200b\u4e24\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u8bed\u6599\u200b\u751f\u6210\u200b\u811a\u672c\u200b\uff1a</p> <pre><code>import random\nfrom random import choice\nimport os\n\ncropus_num = 2000 #\u200b\u8bbe\u7f6e\u200b\u8bed\u6599\u200b\u6570\u91cf\u200b\n\ndef get_cropus(f):\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u5e74\u4efd\u200b\n    year = random.randint(0, 22)\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u6708\u4efd\u200b\n    month = random.randint(1, 12)\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u65e5\u671f\u200b\n    day_dict = {31: [1,3,5,7,8,10,12], 30: [4,6,9,11], 28: [2]}\n    for item in day_dict:\n        if month in day_dict[item]:\n            day = random.randint(0, item)\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u5c0f\u65f6\u200b\n    hours = random.randint(0, 24)\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u5206\u949f\u200b\n    minute = random.randint(0, 60)\n     # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u79d2\u6570\u200b\n    second = random.randint(0, 60)\n\n    # \u200b\u968f\u673a\u200b\u751f\u6210\u200b\u4ea7\u54c1\u200b\u6807\u8bc6\u200b\u5b57\u7b26\u200b\n    length = random.randint(0, 6)\n    file_id = []\n    flag = 0\n    my_dict = [i for i in range(48,58)] + [j for j in range(40, 42)] + [k for k in range(65,90)]  # \u200b\u5927\u200b\u5c0f\u5199\u5b57\u6bcd\u200b + \u200b\u62ec\u53f7\u200b\n\n    for i in range(1, length):\n        if flag:\n            if i == flag+2:  #\u200b\u62ec\u53f7\u200b\u5339\u914d\u200b\n                file_id.append(')')\n                flag = 0\n                continue\n        sel = choice(my_dict)\n        if sel == 41:\n            continue\n        if sel == 40:\n            if i == 1 or i &gt; length-3:\n                continue\n            flag = i\n        my_ascii = chr(sel)\n        file_id.append(my_ascii)\n    file_id_str = ''.join(file_id)\n\n    #\u200b\u968f\u673a\u200b\u751f\u6210\u200b\u4ea7\u54c1\u200b\u6807\u8bc6\u200b\u5b57\u7b26\u200b\n    file_id2 = random.randint(0, 9)\n\n    rad = random.random()\n    if rad &lt; 0.3:\n        f.write('20{:02d}{:02d}{:02d} {}'.format(year, month, day, file_id_str))\n    elif 0.3 &lt; rad &lt; 0.5:\n        f.write('20{:02d}\u200b\u5e74\u200b{:02d}\u200b\u6708\u200b{:02d}\u200b\u65e5\u200b'.format(year, month, day))\n    elif 0.5 &lt; rad &lt; 0.7:\n        f.write('20{:02d}/{:02d}/{:02d}'.format(year, month, day))\n    elif 0.7 &lt; rad &lt; 0.8:\n        f.write('20{:02d}-{:02d}-{:02d}'.format(year, month, day))\n    elif 0.8 &lt; rad &lt; 0.9:\n        f.write('20{:02d}.{:02d}.{:02d}'.format(year, month, day))\n    else:\n        f.write('{:02d}:{:02d}:{:02d} {:02d}'.format(hours, minute, second, file_id2))\n\nif __name__ == \"__main__\":\n    file_path = '/home/aistudio/text_renderer/my_data/cropus'\n    if not os.path.exists(file_path):\n        os.makedirs(file_path)\n    file_name = os.path.join(file_path, 'books.txt')\n    f = open(file_name, 'w')\n    for i in range(cropus_num):\n        get_cropus(f)\n        if i &lt; cropus_num-1:\n            f.write('\\n')\n\n    f.close()\n</code></pre> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u5df2\u200b\u51c6\u5907\u200b\u4e86\u200b\u90e8\u5206\u200b\u8bed\u6599\u200b\uff0c\u200b\u5728\u200b\u7b2c\u200b3\u200b\u90e8\u5206\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b,\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u7684\u200b\u8bed\u6599\u5e93\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4f4d\u7f6e\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 corpus              #\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u8bed\u6599\u200b\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#514","title":"5.1.4 \u200b\u4e0b\u8f7d\u200b\u5b57\u4f53","text":"<p>\u200b\u89c2\u5bdf\u200b\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u5176\u200b\u4f7f\u7528\u200b\u7684\u200b\u5b57\u4f53\u200b\u4e3a\u200b\u70b9\u9635\u200b\u4f53\u200b\u3002\u200b\u5b57\u4f53\u200b\u53ef\u4ee5\u200b\u5728\u200b\u5982\u4e0b\u200b\u7f51\u5740\u200b\u4e0b\u8f7d\u200b\uff1a https://www.fonts.net.cn/fonts-en/tag-dianzhen-1.html</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u5df2\u200b\u51c6\u5907\u200b\u4e86\u200b\u90e8\u5206\u200b\u5b57\u4f53\u200b\uff0c\u200b\u5728\u200b\u7b2c\u200b3\u200b\u90e8\u5206\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b,\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u7684\u200b\u5b57\u4f53\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4f4d\u7f6e\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 fonts                #\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u5b57\u4f53\u200b\n</code></pre> <p>\u200b\u4e0b\u8f7d\u200b\u597d\u200b\u5b57\u4f53\u200b\u540e\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u5728\u200blist\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u6307\u5b9a\u200b\u5b57\u4f53\u200b\u6587\u4ef6\u200b\u5b58\u653e\u200b\u8def\u5f84\u200b\uff0c\u200b\u811a\u672c\u200b\u5982\u4e0b\u200b:</p> <pre><code>cd text_renderer/my_data/\ntouch fonts.list\nls /home/aistudio/PaddleOCR/data/fonts/* &gt; fonts.list\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#515","title":"5.1.5 \u200b\u8fd0\u884c\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u547d\u4ee4","text":"<p>\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b\uff0cmy_data\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>my_data/\n\u251c\u2500\u2500 cropus\n\u2502   \u2514\u2500\u2500 books.txt #\u200b\u8bed\u6599\u5e93\u200b\n\u251c\u2500\u2500 eng.txt    #\u200b\u5b57\u7b26\u200b\u5217\u8868\u200b\n\u2514\u2500\u2500 fonts.list #\u200b\u5b57\u4f53\u200b\u5217\u8868\u200b\n</code></pre> <p>\u200b\u5728\u200b\u8fd0\u884c\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u547d\u4ee4\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e24\u5904\u200b\u7ec6\u8282\u200b\u9700\u8981\u200b\u624b\u52a8\u200b\u4fee\u6539\u200b\uff1a</p> <ol> <li> <p>\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>text_renderer/configs/default.yaml</code>\u200b\u4e2d\u200b\u7b2c\u200b9\u200b\u884c\u200benable\u200b\u7684\u200b\u503c\u200b\u8bbe\u4e3a\u200b<code>true</code>\uff0c\u200b\u5373\u200b\u5141\u8bb8\u200b\u5408\u6210\u200b\u5f69\u8272\u56fe\u50cf\u200b\u3002\u200b\u5426\u5219\u200b\u5408\u6210\u200b\u7684\u200b\u90fd\u200b\u662f\u200b\u7070\u5ea6\u200b\u56fe\u200b\u3002</p> <pre><code># color boundary is in R,G,B format\nfont_color:\n+  enable: true #false\n</code></pre> </li> <li> <p>\u200b\u5c06\u200b<code>text_renderer/textrenderer/renderer.py</code>\u200b\u7b2c\u200b184\u200b\u884c\u4f5c\u200b\u5982\u4e0b\u200b\u4fee\u6539\u200b\uff0c\u200b\u53d6\u6d88\u200bpadding\u3002\u200b\u5426\u5219\u200b\u56fe\u7247\u200b\u4e24\u7aef\u200b\u4f1a\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7a7a\u767d\u200b\u3002</p> <pre><code>padding = random.randint(s_bbox_width // 10, s_bbox_width // 8) #\u200b\u4fee\u6539\u200b\u524d\u200b\npadding = 0 #\u200b\u4fee\u6539\u200b\u540e\u200b\n</code></pre> </li> </ol> <p>\u200b\u8fd0\u884c\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u547d\u4ee4\u200b:</p> <pre><code>cd /home/aistudio/text_renderer/\npython main.py --num_img=3000 \\\n                  --fonts_list='./my_data/fonts.list' \\\n                  --corpus_dir \"./my_data/cropus\" \\\n                  --corpus_mode \"list\" \\\n                  --bg_dir \"/home/aistudio/PaddleOCR/data/bg/\" \\\n                  --img_width 0\n</code></pre> <p>\u200b\u5408\u6210\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>text_renderer/output</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u200b\u8fdb\u5165\u200b\u8be5\u200b\u76ee\u5f55\u200b\u67e5\u770b\u200b\u5408\u6210\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b </p> <p>\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u597d\u540e\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u751f\u6210\u200b\u5982\u4e0b\u200b\u683c\u5f0f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\uff0c</p> <pre><code>\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b \u200b\u6807\u7b7e\u200b\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u811a\u672c\u200b\u5373\u53ef\u200b\u751f\u6210\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>import random\n\nabspath = '/home/aistudio/text_renderer/output/default/'\n\n#\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u751f\u6210\u200b\u8def\u5f84\u200b\nfout = open('./render_train.list', 'w', encoding='utf-8')\n\nwith open('./output/default/tmp_labels.txt','r') as f:\n    lines = f.readlines()\n    for item in lines:\n        label = item[9:]\n        filename = item[:8] + '.jpg'\n        fout.write(abspath + filename + '\\t' + label)\n\n    fout.close()\n</code></pre> <p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u6211\u4eec\u200b\u4fbf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u5305\u88c5\u200b\u751f\u4ea7\u65e5\u671f\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u3002 \u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200b<code>text_renderer/output</code>\uff0c\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u4f4d\u4e8e\u200b<code>text_renderer/render_train.list</code>\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u751f\u6210\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u200b\u4f9b\u200b\u5927\u5bb6\u200b\u4f53\u9a8c\u200b,\u200b\u5b8c\u6210\u200b\u6b65\u9aa4\u200b3\u200b\u7684\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b\uff0c\u200b\u53ef\u200b\u5f97\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u4f4d\u4e8e\u200b:</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 render_images     # \u200b\u5408\u6210\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\n\u2502   \u251c\u2500\u2500 render_train.list   #\u200b\u5408\u6210\u200b\u6570\u636e\u6587\u4ef6\u200b\u5217\u8868\u200b\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#52","title":"5.2 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u51c6\u5907\u200b\u597d\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5229\u7528\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune:</p> <pre><code>cd ${PaddleOCR_root}\npython tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml \\\n                       -o Global.pretrained_model=./ckpt/ch_PP-OCRv3_rec_train/best_accuracy \\\n                       Global.epoch_num=20 \\\n                       Global.eval_batch_step='[0, 20]' \\\n                       Train.dataset.data_dir=./data \\\n                       Train.dataset.label_file_list=['./data/render_train.list'] \\\n                       Train.loader.batch_size_per_card=64 \\\n                       Eval.dataset.data_dir=./data \\\n                       Eval.dataset.label_file_list=[\"./data/val.list\"] \\\n                       Eval.loader.batch_size_per_card=64\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\u5404\u200b\u53c2\u6570\u200b\u542b\u4e49\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>-c: \u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0cch_PP-OCRv3_rec_distillation.yml\u200b\u5bf9\u5e94\u200b\u4e8e\u200bOCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u3002\n-o: \u200b\u8986\u76d6\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u53c2\u6570\u200b\nGlobal.pretrained_model: \u200b\u6307\u5b9a\u200bfinetune\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nGlobal.epoch_num: \u200b\u6307\u5b9a\u200b\u8bad\u7ec3\u200b\u7684\u200bepoch\u200b\u6570\u200b\nGlobal.eval_batch_step: \u200b\u95f4\u9694\u200b\u591a\u5c11\u200bstep\u200b\u505a\u200b\u4e00\u6b21\u200b\u8bc4\u4f30\u200b\nTrain.dataset.data_dir: \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\nTrain.dataset.label_file_list: \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\nTrain.loader.batch_size_per_card: \u200b\u8bad\u7ec3\u200b\u5355\u5361\u200bbatch size\nEval.dataset.data_dir: \u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\nEval.dataset.label_file_list: \u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\nEval.loader.batch_size_per_card: \u200b\u8bc4\u4f30\u200b\u5355\u5361\u200bbatch size\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#6-finetune","title":"6. \u200b\u57fa\u4e8e\u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune","text":"<p>\u200b\u4f7f\u7528\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune\u200b\u80fd\u200b\u63d0\u5347\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u548c\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5206\u5e03\u200b\u53ef\u80fd\u200b\u6709\u200b\u4e00\u5b9a\u200b\u5dee\u5f02\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f5c\u7528\u200b\u6709\u9650\u200b\u3002\u200b\u4e3a\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u672c\u200b\u8282\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u6316\u6398\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200bfinetune\u3002</p> <p>\u200b\u6570\u636e\u6316\u6398\u200b\u7684\u200b\u6574\u4f53\u200b\u601d\u8def\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200bpython\u200b\u722c\u866b\u200b\u4ece\u200b\u7f51\u4e0a\u200b\u83b7\u53d6\u200b\u5927\u91cf\u200b\u65e0\u200b\u6807\u7b7e\u200b\u6570\u636e\u200b</li> <li>\u200b\u4f7f\u7528\u200b\u6a21\u578b\u200b\u4ece\u200b\u5927\u91cf\u200b\u65e0\u200b\u6807\u7b7e\u200b\u6570\u636e\u200b\u4e2d\u200b\u6784\u5efa\u200b\u51fa\u200b\u6709\u6548\u200b\u8bad\u7ec3\u200b\u96c6\u200b</li> </ol>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#61-python","title":"6.1 python\u200b\u722c\u866b\u200b\u83b7\u53d6\u6570\u636e","text":"<p>\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u722c\u866b\u200b\u5de5\u5177\u200b\u83b7\u53d6\u200b\u65e0\u200b\u6807\u7b7e\u200b\u56fe\u7247\u200b\u3002\u200b\u56fe\u7247\u200b\u83b7\u53d6\u200b\u540e\u200b\uff0c\u200b\u53ef\u200b\u6309\u200b\u5982\u4e0b\u200b\u76ee\u5f55\u200b\u683c\u5f0f\u200b\u7ec4\u7ec7\u200b\uff1a</p> <pre><code>sprider\n\u251c\u2500\u2500 file.list\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 00000.jpg\n\u2502   \u251c\u2500\u2500 00001.jpg\n...\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#62","title":"6.2 \u200b\u6570\u636e\u6316\u6398","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u5bf9\u200b\u83b7\u53d6\u200b\u5230\u200b\u7684\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u6316\u6398\u200b\uff0c\u200b\u5177\u4f53\u6b65\u9aa4\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b PP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b+svtr-tiny\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5bf9\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6570\u636e\u6316\u6398\u200b\u7b56\u7565\u200b\uff0c\u200b\u5f97\u5230\u200b\u6709\u6548\u200b\u56fe\u7247\u200b\u3002</li> <li>\u200b\u5c06\u200b\u6709\u6548\u200b\u56fe\u7247\u200b\u5bf9\u5e94\u200b\u7684\u200b\u56fe\u50cf\u200b\u533a\u57df\u200b\u548c\u200b\u6807\u7b7e\u200b\u63d0\u53d6\u200b\u51fa\u6765\u200b\uff0c\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002</li> </ol> <p>\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0cPP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff1ahttps://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <p>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</p> <p>\u200b\u5b8c\u6210\u200b\u4e0b\u8f7d\u200b\u540e\u200b\uff0c\u200b\u53ef\u200b\u5c06\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u4e8e\u200b\u5982\u4e0b\u200b\u4f4d\u7f6e\u200b:</p> <pre><code>PaddleOCR\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 rec_vit_sub_64_363_all/  # svtr_tiny\u200b\u9ad8\u7cbe\u5ea6\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\n</code></pre> <pre><code># \u200b\u4e0b\u8f7d\u200b\u89e3\u538b\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\ncd ${PaddleOCR_root}\nwget -nc -P ckpt https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\npushd ckpt\ntar -xvf ch_PP-OCRv3_det_infer.tar\npopd ckpt\n</code></pre> <p>\u200b\u5728\u200b\u4f7f\u7528\u200bPPOCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b+svtr-tiny\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6709\u200b\u5982\u4e0b\u200b\u4e24\u5904\u200b\u7ec6\u8282\u200b\u9700\u8981\u200b\u624b\u52a8\u200b\u4fee\u6539\u200b\uff1a</p> <ol> <li> <p>\u200b\u5c06\u200b<code>tools/infer/predict_rec.py</code>\u200b\u4e2d\u200b\u7b2c\u200b110\u200b\u884c\u200b<code>imgW</code>\u200b\u4fee\u6539\u200b\u4e3a\u200b<code>320</code></p> <pre><code>#imgW = int((imgH * max_wh_ratio))\nimgW = 320\n</code></pre> </li> <li> <p>\u200b\u5c06\u200b<code>tools/infer/predict_system.py</code>\u200b\u7b2c\u200b169\u200b\u884c\u200b\u6dfb\u52a0\u200b\u5982\u4e0b\u200b\u4e00\u884c\u200b\uff0c\u200b\u5c06\u200b\u9884\u6d4b\u200b\u5206\u6570\u200b\u4e5f\u200b\u5199\u5165\u200b\u7ed3\u679c\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002</p> <pre><code>\"scores\": rec_res[idx][1],\n</code></pre> </li> </ol> <p>\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u547d\u4ee4\u200b:</p> <pre><code>python tools/infer/predict_system.py \\\n        --image_dir=\"/home/aistudio/sprider/data\" \\\n        --det_model_dir=\"./ckpt/ch_PP-OCRv3_det_infer/\"  \\\n        --rec_model_dir=\"/home/aistudio/PaddleOCR/data/rec_vit_sub_64_363_all/\" \\\n        --rec_image_shape=\"3,32,320\"\n</code></pre> <p>\u200b\u83b7\u5f97\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6570\u636e\u6316\u6398\u200b\u7b56\u7565\u200b\u5f97\u5230\u200b\u6709\u6548\u200b\u56fe\u7247\u200b\u3002\u200b\u5177\u4f53\u200b\u6316\u6398\u200b\u7b56\u7565\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u9884\u6d4b\u200b\u7f6e\u4fe1\u5ea6\u200b\u9ad8\u4e8e\u200b95%</li> <li>\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u5305\u542b\u200b\u5b57\u7b26\u200b\u201820\u2019\uff0c\u200b\u5373\u200b\u5e74\u4efd\u200b</li> <li>\u200b\u6ca1\u6709\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6216\u8005\u200b\u6709\u200b\u4e2d\u6587\u200b\u5e76\u4e14\u200b\u2018\u200b\u65e5\u200b\u2019\u200b\u548c\u200b'\u200b\u6708\u200b'\u200b\u540c\u65f6\u200b\u5728\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u4e2d\u200b</li> </ol> <pre><code># \u200b\u83b7\u53d6\u200b\u6709\u6548\u200b\u9884\u6d4b\u200b\n\nimport json\nimport re\n\nzh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')  #\u200b\u6b63\u5219\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u7b5b\u9009\u200b\u5b57\u7b26\u200b\u662f\u5426\u200b\u5305\u542b\u200b\u4e2d\u6587\u200b\n\nfile_path = '/home/aistudio/PaddleOCR/inference_results/system_results.txt'\nout_path = '/home/aistudio/PaddleOCR/selected_results.txt'\nf_out = open(out_path, 'w')\n\nwith open(file_path, \"r\", encoding='utf-8') as fin:\n    lines = fin.readlines()\n\n\nfor line in lines:\n    flag = False\n    # \u200b\u8bfb\u53d6\u200b\u6587\u4ef6\u200b\u5185\u5bb9\u200b\n    file_name, json_file = line.strip().split('\\t')\n    preds = json.loads(json_file)\n    res = []\n    for item in preds:\n        transcription = item['transcription'] #\u200b\u83b7\u53d6\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\n        scores = item['scores']               #\u200b\u83b7\u53d6\u200b\u8bc6\u522b\u200b\u5f97\u5206\u200b\n        # \u200b\u6316\u6398\u200b\u7b56\u7565\u200b\n        if scores &gt; 0.95:\n            if '20' in transcription and len(transcription) &gt; 4 and len(transcription) &lt; 12:\n                word = transcription\n                if not(zh_pattern.search(word) and ('\u200b\u65e5\u200b' not in word or '\u200b\u6708\u200b' not in word)):\n                    flag = True\n                    res.append(item)\n    save_pred = file_name + \"\\t\" + json.dumps(\n        res, ensure_ascii=False) + \"\\n\"\n    if flag ==True:\n        f_out.write(save_pred)\n\nf_out.close()\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5c06\u200b\u6709\u6548\u200b\u9884\u6d4b\u200b\u5bf9\u5e94\u200b\u7684\u200b\u56fe\u50cf\u200b\u533a\u57df\u200b\u548c\u200b\u6807\u7b7e\u200b\u63d0\u53d6\u200b\u51fa\u6765\u200b\uff0c\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u811a\u672c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>import cv2\nimport json\nimport numpy as np\n\nPATH = '/home/aistudio/PaddleOCR/inference_results/'  #\u200b\u6570\u636e\u200b\u539f\u59cb\u200b\u8def\u5f84\u200b\nSAVE_PATH = '/home/aistudio/mining_images/'             #\u200b\u88c1\u526a\u200b\u540e\u200b\u6570\u636e\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\nfile_list = '/home/aistudio/PaddleOCR/selected_results.txt' #\u200b\u6570\u636e\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\nlabel_file = '/home/aistudio/mining_images/mining_train.list'  #\u200b\u8f93\u51fa\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u7b7e\u200blist\n\nif not os.path.exists(SAVE_PATH):\n    os.mkdir(SAVE_PATH)\n\nf_label = open(label_file, 'w')\n\n\ndef get_rotate_crop_image(img, points):\n    \"\"\"\n    \u200b\u6839\u636e\u200b\u68c0\u6d4b\u200b\u7ed3\u679c\u200bpoints\uff0c\u200b\u4ece\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200bimg\u200b\u4e2d\u200b\u88c1\u526a\u200b\u51fa\u200b\u76f8\u5e94\u200b\u7684\u200b\u533a\u57df\u200b\n    \"\"\"\n    assert len(points) == 4, \"shape of points must be 4*2\"\n    img_crop_width = int(\n        max(\n            np.linalg.norm(points[0] - points[1]),\n            np.linalg.norm(points[2] - points[3])))\n    img_crop_height = int(\n        max(\n            np.linalg.norm(points[0] - points[3]),\n            np.linalg.norm(points[1] - points[2])))\n    pts_std = np.float32([[0, 0], [img_crop_width, 0],\n                          [img_crop_width, img_crop_height],\n                          [0, img_crop_height]])\n    M = cv2.getPerspectiveTransform(points, pts_std)\n    # \u200b\u5f62\u53d8\u200b\u6216\u200b\u503e\u659c\u200b\uff0c\u200b\u4f1a\u200b\u505a\u200b\u900f\u89c6\u200b\u53d8\u6362\u200b\uff0creshape\u200b\u6210\u200b\u77e9\u5f62\u200b\n    dst_img = cv2.warpPerspective(\n        img,\n        M, (img_crop_width, img_crop_height),\n        borderMode=cv2.BORDER_REPLICATE,\n        flags=cv2.INTER_CUBIC)\n    dst_img_height, dst_img_width = dst_img.shape[0:2]\n    if dst_img_height * 1.0 / dst_img_width &gt;= 1.5:\n        dst_img = np.rot90(dst_img)\n    return dst_img\n\ndef crop_and_get_filelist(file_list):\n    with open(file_list, \"r\", encoding='utf-8') as fin:\n        lines = fin.readlines()\n\n    img_num = 0\n    for line in lines:\n        img_name, json_file = line.strip().split('\\t')\n        preds = json.loads(json_file)\n        for item in preds:\n            transcription = item['transcription']\n            points = item['points']\n            points = np.array(points).astype('float32')\n            #print('processing {}...'.format(img_name))\n\n            img = cv2.imread(PATH+img_name)\n            dst_img = get_rotate_crop_image(img, points)\n            h, w, c = dst_img.shape\n            newWidth = int((32. / h) * w)\n            newImg = cv2.resize(dst_img, (newWidth, 32))\n            new_img_name = '{:05d}.jpg'.format(img_num)\n            cv2.imwrite(SAVE_PATH+new_img_name, dst_img)\n            f_label.write(SAVE_PATH+new_img_name+'\\t'+transcription+'\\n')\n            img_num += 1\n\n\ncrop_and_get_filelist(file_list)\nf_label.close()\n</code></pre>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#63","title":"6.3 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u901a\u8fc7\u200b\u6570\u636e\u6316\u6398\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u4e86\u200b\u771f\u5b9e\u200b\u573a\u666f\u200b\u6570\u636e\u200b\u548c\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\u4f7f\u7528\u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune\uff0c\u200b\u89c2\u5bdf\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u5229\u7528\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune:</p> <pre><code>cd ${PaddleOCR_root}\npython tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml \\\n                       -o Global.pretrained_model=./ckpt/ch_PP-OCRv3_rec_train/best_accuracy \\\n                       Global.epoch_num=20 \\\n                       Global.eval_batch_step='[0, 20]' \\\n                       Train.dataset.data_dir=./data \\\n                       Train.dataset.label_file_list=['./data/mining_train.list'] \\\n                       Train.loader.batch_size_per_card=64 \\\n                       Eval.dataset.data_dir=./data \\\n                       Eval.dataset.label_file_list=[\"./data/val.list\"] \\\n                       Eval.loader.batch_size_per_card=64\n</code></pre> <p>\u200b\u5404\u200b\u53c2\u6570\u200b\u542b\u4e49\u200b\u53c2\u8003\u200b\u7b2c\u200b6\u200b\u90e8\u5206\u5408\u6210\u200b\u6570\u636e\u200bfinetune\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u505a\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200b\uff1a</p> <pre><code>Train.dataset.data_dir: \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\nTrain.dataset.label_file_list: \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\n</code></pre> <p>\u200b\u793a\u4f8b\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u5982\u60f3\u200b\u6362\u6210\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200b<code>Train.dataset.data_dir</code>\u200b\u548c\u200b<code>Train.dataset.label_file_list</code>\u200b\u53c2\u6570\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6570\u636e\u91cf\u200b\u4e0d\u200b\u5927\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4ec5\u200b\u8bad\u7ec3\u200b20\u200b\u4e2a\u200bepoch\u200b\u5373\u53ef\u200b\u3002\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune\u200b\u540e\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e3a\u200bbest acc=71.33%\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6570\u91cf\u200b\u6bd4\u8f83\u200b\u5c11\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4f1a\u200b\u6bd4\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetue\u200b\u7684\u200b\u7565\u4f4e\u200b\u3002</p>"},{"location":"en/applications/%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E6%97%A5%E6%9C%9F%E8%AF%86%E5%88%AB.html#7-finetune","title":"7. \u200b\u57fa\u4e8e\u200b\u5408\u6210\u200b+\u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune","text":"<p>\u200b\u4e3a\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ed3\u5408\u200b\u4f7f\u7528\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u548c\u200b\u6316\u6398\u200b\u5230\u200b\u7684\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\u3002</p> <p>\u200b\u5229\u7528\u200b\u5408\u6210\u200b+\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u5404\u200b\u53c2\u6570\u200b\u542b\u4e49\u200b\u53c2\u8003\u200b\u7b2c\u200b6\u200b\u90e8\u5206\u5408\u6210\u200b\u6570\u636e\u200bfinetune\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u505a\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200b\uff1a</p> <pre><code>Train.dataset.data_dir: \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\nTrain.dataset.label_file_list: \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\n</code></pre> <p>\u200b\u751f\u6210\u200b\u8bad\u7ec3\u200blist\u200b\u6587\u4ef6\u200b:</p> <pre><code># \u200b\u751f\u6210\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6587\u4ef6\u200blist\ncat /home/aistudio/PaddleOCR/data/render_train.list /home/aistudio/PaddleOCR/data/mining_train.list &gt; /home/aistudio/PaddleOCR/data/render_mining_train.list\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b:</p> <pre><code>cd ${PaddleOCR_root}\npython tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml \\\n                       -o Global.pretrained_model=./ckpt/ch_PP-OCRv3_rec_train/best_accuracy \\\n                       Global.epoch_num=40 \\\n                       Global.eval_batch_step='[0, 20]' \\\n                       Train.dataset.data_dir=./data \\\n                       Train.dataset.label_file_list=['./data/render_mining_train.list'] \\\n                       Train.loader.batch_size_per_card=64 \\\n                       Eval.dataset.data_dir=./data \\\n                       Eval.dataset.label_file_list=[\"./data/val.list\"] \\\n                       Eval.loader.batch_size_per_card=64\n</code></pre> <p>\u200b\u793a\u4f8b\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u7684\u200b\u771f\u5b9e\u200b+\u200b\u5408\u6210\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u5982\u60f3\u200b\u6362\u6210\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200bTrain.dataset.data_dir\u200b\u548c\u200bTrain.dataset.label_file_list\u200b\u53c2\u6570\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6570\u636e\u91cf\u200b\u4e0d\u200b\u5927\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4ec5\u200b\u8bad\u7ec3\u200b40\u200b\u4e2a\u200bepoch\u200b\u5373\u53ef\u200b\u3002\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u5408\u6210\u200b\u6570\u636e\u200bfinetune\u200b\u540e\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e3a\u200bbest acc=86.99%\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u539f\u59cb\u200bPP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b62.99%\uff0c\u200b\u4f7f\u7528\u200b\u5408\u6210\u200b\u6570\u636e\u200b+\u200b\u771f\u5b9e\u200b\u6570\u636e\u200bfinetune\u200b\u540e\u200b\uff0c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u80fd\u200b\u63d0\u5347\u200b24%\u3002</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <p>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</p> <p>\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\u90e8\u7f72\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200brepo\u200b\u6587\u6863\u200b\uff1a docs</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html","title":"\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u4ecb\u7ecd","text":"<p>\u200b\u5f2f\u66f2\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u5728\u200bOCR\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u6709\u7740\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u5e94\u7528\u200b\uff0c\u200b\u6bd4\u5982\u200b\uff1a\u200b\u81ea\u7136\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u62db\u724c\u200b\uff0c\u200b\u827a\u672f\u200b\u6587\u5b57\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5e38\u89c1\u200b\u7684\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u3002</p> <p>\u200b\u5728\u200b\u672c\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u4ee5\u200b\u5370\u7ae0\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bPaddleDetection\u200b\u548c\u200bPaddleOCR\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u548c\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u9879\u76ee\u200b\u96be\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u7f3a\u4e4f\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b</li> <li>\u200b\u56fe\u50cf\u200b\u8d28\u91cf\u200b\u53c2\u5dee\u4e0d\u9f50\u200b\uff0c\u200b\u56fe\u50cf\u200b\u6a21\u7cca\u200b\uff0c\u200b\u6587\u5b57\u200b\u4e0d\u200b\u6e05\u6670\u200b</li> </ol> <p>\u200b\u9488\u5bf9\u200b\u4ee5\u4e0a\u200b\u95ee\u9898\u200b\uff0c\u200b\u672c\u200b\u9879\u76ee\u200b\u9009\u7528\u200bPaddleOCR\u200b\u91cc\u200b\u7684\u200bPPOCRLabel\u200b\u5de5\u5177\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u3002\u200b\u57fa\u4e8e\u200bPaddleDetection\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200bPaddleOCR\u200b\u91cc\u200b\u7684\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200bOCR\u200b\u7b97\u6cd5\u200b\u548c\u200b\u4e24\u200b\u9636\u6bb5\u200bOCR\u200b\u7b97\u6cd5\u200b\u5206\u522b\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u3002\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u4efb\u52a1\u200b \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u6570\u91cf\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b 1000 95.00% \u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b-\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200bOCR\u200b\u65b9\u6cd5\u200b 700 47.00% \u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b-\u200b\u4e24\u200b\u9636\u6bb5\u200bOCR\u200b\u65b9\u6cd5\u200b 700 55.00% <p>\u200b\u70b9\u51fb\u200b\u8fdb\u5165\u200b AI Studio \u200b\u9879\u76ee\u200b</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u73af\u5883\u200b\u642d\u5efa","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u9700\u8981\u200b\u51c6\u5907\u200bPaddleDetection\u200b\u548c\u200bPaddleOCR\u200b\u7684\u200b\u9879\u76ee\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u5176\u4e2d\u200bPaddleDetection\u200b\u7528\u4e8e\u200b\u5b9e\u73b0\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\uff0cPaddleOCR\u200b\u7528\u4e8e\u200b\u5b9e\u73b0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#21-paddledetection","title":"2.1 \u200b\u51c6\u5907\u200bPaddleDetection\u200b\u73af\u5883","text":"<p>\u200b\u4e0b\u8f7d\u200bPaddleDetection\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>!git clone https://github.com/PaddlePaddle/PaddleDetection.git\n# \u200b\u5982\u679c\u200b\u514b\u9686\u200bgithub\u200b\u4ee3\u7801\u200b\u8f83\u6162\u200b\uff0c\u200b\u8bf7\u200b\u4ece\u200bgitee\u200b\u4e0a\u200b\u514b\u9686\u200b\u4ee3\u7801\u200b\n#git clone https://gitee.com/PaddlePaddle/PaddleDetection.git\n</code></pre> <p>\u200b\u5b89\u88c5\u200bPaddleDetection\u200b\u4f9d\u8d56\u200b</p> <pre><code>!cd PaddleDetection &amp;&amp; pip install -r requirements.txt\n</code></pre>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#22-paddleocr","title":"2.2 \u200b\u51c6\u5907\u200bPaddleOCR\u200b\u73af\u5883","text":"<p>\u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>!git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# \u200b\u5982\u679c\u200b\u514b\u9686\u200bgithub\u200b\u4ee3\u7801\u200b\u8f83\u6162\u200b\uff0c\u200b\u8bf7\u200b\u4ece\u200bgitee\u200b\u4e0a\u200b\u514b\u9686\u200b\u4ee3\u7801\u200b\n#git clone https://gitee.com/PaddlePaddle/PaddleOCR.git\n</code></pre> <p>\u200b\u5b89\u88c5\u200bPaddleOCR\u200b\u4f9d\u8d56\u200b</p> <pre><code>!cd PaddleOCR &amp;&amp; git checkout dygraph  &amp;&amp; pip install -r requirements.txt\n</code></pre>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u6570\u636e\u200b\u96c6\u200b\u51c6\u5907","text":""},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#31","title":"3.1 \u200b\u6570\u636e\u200b\u6807\u6ce8","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u4e2d\u200b\u4f7f\u7528\u200bPPOCRLabel\u200b\u5de5\u5177\u200b\u6807\u6ce8\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\uff0c\u200b\u6807\u6ce8\u200b\u5185\u5bb9\u200b\u5305\u62ec\u200b\u5370\u7ae0\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u4ee5\u53ca\u200b\u5370\u7ae0\u200b\u4e2d\u200b\u6587\u5b57\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u548c\u200b\u6587\u5b57\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u6ce8\u200b\uff1aPPOCRLabel\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u53c2\u8003\u200b\u6587\u6863\u200b\u3002</p> <p>PPOCRlabel\u200b\u6807\u6ce8\u200b\u5370\u7ae0\u200b\u6570\u636e\u200b\u6b65\u9aa4\u200b\uff1a</p> <ul> <li>\u200b\u6253\u5f00\u200b\u6570\u636e\u200b\u96c6\u200b\u6240\u5728\u200b\u6587\u4ef6\u5939\u200b</li> <li>\u200b\u6309\u4e0b\u200b\u5feb\u6377\u952e\u200bQ\u200b\u8fdb\u884c\u200b4\u200b\u70b9\u200b\uff08\u200b\u591a\u70b9\u200b\uff09\u200b\u6807\u6ce8\u200b\u2014\u2014\u200b\u9488\u5bf9\u200b\u5370\u7ae0\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\uff0c</li> <li>\u200b\u5370\u7ae0\u200b\u5f2f\u66f2\u200b\u6587\u5b57\u200b\u5305\u56f4\u200b\u6846\u200b\u91c7\u7528\u200b\u5076\u6570\u200b\u70b9\u200b\u6807\u6ce8\u200b\uff08\u200b\u6bd4\u5982\u200b4\u200b\u70b9\u200b\uff0c8\u200b\u70b9\u200b\uff0c16\u200b\u70b9\u200b\uff09\uff0c\u200b\u6309\u7167\u200b\u9605\u8bfb\u200b\u987a\u5e8f\u200b\uff0c\u200b\u4ee5\u200b16\u200b\u70b9\u200b\u6807\u6ce8\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4ece\u200b\u6587\u5b57\u200b\u5de6\u4e0a\u65b9\u200b\u5f00\u59cb\u200b\u6807\u6ce8\u200b-&gt;\u200b\u5230\u200b\u6587\u5b57\u200b\u53f3\u4e0a\u65b9\u200b\u6807\u6ce8\u200b8\u200b\u4e2a\u70b9\u200b-&gt;\u200b\u5230\u200b\u6587\u5b57\u200b\u53f3\u4e0b\u65b9\u200b-&gt;\u200b\u6587\u5b57\u200b\u5de6\u4e0b\u65b9\u200b8\u200b\u4e2a\u70b9\u200b\uff0c\u200b\u4e00\u5171\u200b8\u200b\u4e2a\u70b9\u200b\uff0c\u200b\u5f62\u6210\u200b\u5305\u56f4\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u53c2\u8003\u200b\u4e0b\u56fe\u200b\u3002\u200b\u5982\u679c\u200b\u6587\u5b57\u200b\u5f2f\u66f2\u200b\u7a0b\u5ea6\u200b\u4e0d\u9ad8\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u51cf\u5c0f\u200b\u6807\u6ce8\u200b\u5de5\u4f5c\u91cf\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b4\u200b\u70b9\u200b\u30018\u200b\u70b9\u200b\u6807\u6ce8\u200b\uff0c\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u200b\u70b9\u6570\u200b\u76f8\u540c\u200b\u3002\uff08\u200b\u603b\u200b\u70b9\u6570\u200b\u5c3d\u91cf\u200b\u4e0d\u8981\u200b\u8d85\u8fc7\u200b18\u200b\u4e2a\u200b\uff09</li> <li>\u200b\u5bf9\u4e8e\u200b\u9700\u8981\u200b\u8bc6\u522b\u200b\u7684\u200b\u5370\u7ae0\u200b\u4e2d\u975e\u200b\u5f2f\u66f2\u200b\u6587\u5b57\u200b\uff0c\u200b\u91c7\u7528\u200b4\u200b\u70b9\u6846\u200b\u6807\u6ce8\u200b\u5373\u53ef\u200b</li> <li>\u200b\u5bf9\u5e94\u200b\u5305\u56f4\u200b\u6846\u200b\u7684\u200b\u6587\u5b57\u200b\u90e8\u5206\u200b\u9ed8\u8ba4\u200b\u662f\u200b\u201d\u200b\u5f85\u200b\u8bc6\u522b\u200b\u201d\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u4e3a\u200b\u5305\u56f4\u200b\u6846\u5185\u200b\u7684\u200b\u5177\u4f53\u200b\u6587\u5b57\u200b\u5185\u5bb9\u200b</li> <li>\u200b\u5feb\u6377\u952e\u200bW\u200b\u8fdb\u884c\u200b\u77e9\u5f62\u200b\u6807\u6ce8\u200b\u2014\u2014\u200b\u9488\u5bf9\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u533a\u57df\u200b\u4fdd\u8bc1\u200b\u6807\u6ce8\u200b\u6846\u200b\u5305\u56f4\u200b\u6574\u4e2a\u200b\u5370\u7ae0\u200b\uff0c\u200b\u5305\u56f4\u200b\u6846\u200b\u5bf9\u5e94\u200b\u6587\u5b57\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b'\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b'\uff0c\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u5904\u7406\u200b\u3002</li> <li>\u200b\u9488\u5bf9\u200b\u5370\u7ae0\u200b\u4e2d\u200b\u7684\u200b\u6c34\u5e73\u200b\u6587\u5b57\u200b\u53ef\u4ee5\u200b\u89c6\u200b\u60c5\u51b5\u200b\u8003\u8651\u200b\u77e9\u5f62\u200b\u6216\u200b\u56db\u70b9\u200b\u6807\u6ce8\u200b\uff1a\u200b\u4fdd\u8bc1\u200b\u6309\u884c\u200b\u6807\u6ce8\u200b\u5373\u53ef\u200b\u3002\u200b\u5982\u679c\u200b\u80cc\u666f\u200b\u6587\u5b57\u200b\u4e0e\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u6bd4\u8f83\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u6807\u6ce8\u200b\u65f6\u200b\u5c3d\u91cf\u200b\u907f\u5f00\u200b\u80cc\u666f\u200b\u6587\u5b57\u200b\u3002</li> <li>\u200b\u6807\u6ce8\u200b\u5b8c\u6210\u200b\u540e\u200b\u4fee\u6539\u200b\u53f3\u4fa7\u200b\u6587\u672c\u200b\u7ed3\u679c\u200b\uff0c\u200b\u786e\u8ba4\u200b\u65e0\u8bef\u200b\u540e\u200b\u70b9\u51fb\u200b\u4e0b\u65b9\u200bcheck\uff08\u200b\u6216\u200bCTRL+V)\uff0c\u200b\u786e\u8ba4\u200b\u672c\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u6807\u6ce8\u200b\u3002</li> <li>\u200b\u6240\u6709\u200b\u56fe\u7247\u200b\u6807\u6ce8\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u9876\u90e8\u200b\u83dc\u5355\u680f\u200b\u70b9\u51fb\u200bFile -&gt; Export Label\u200b\u5bfc\u51fa\u200blabel.txt\u3002</li> </ul> <p>\u200b\u6807\u6ce8\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a </p> <p>\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6807\u7b7e\u200b\u4e2d\u200b\u5305\u542b\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6807\u6ce8\u200b\u548c\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7684\u200b\u6807\u6ce8\u200b\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>img/1.png    [{\"transcription\": \"\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\", \"points\": [[87, 245], [214, 245], [214, 369], [87, 369]], \"difficult\": false}, {\"transcription\": \"\u200b\u56fd\u5bb6\u7a0e\u52a1\u603b\u5c40\u200b\u6cf8\u6c34\u200b\u5e02\u200b\u7a0e\u52a1\u5c40\u200b\u7b2c\u4e8c\u200b\u7a0e\u52a1\u200b\u5206\u5c40\u200b\", \"points\": [[110, 314], [116, 290], [131, 275], [152, 273], [170, 277], [181, 289], [186, 303], [186, 312], [201, 311], [198, 289], [189, 272], [175, 259], [152, 252], [124, 257], [100, 280], [94, 312]], \"difficult\": false}, {\"transcription\": \"\u200b\u5f81\u7a0e\u200b\u4e13\u7528\u7ae0\u200b\", \"points\": [[117, 334], [183, 334], [183, 352], [117, 352]], \"difficult\": false}]\n</code></pre> <p>\u200b\u6807\u6ce8\u200b\u4e2d\u200b\u5305\u542b\u200b\u8868\u793a\u200b'\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b'\u200b\u7684\u200b\u5750\u6807\u200b\u548c\u200b'\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b'\u200b\u5750\u6807\u200b\u4ee5\u53ca\u200b\u6587\u5b57\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#32","title":"3.2 \u200b\u6570\u636e\u5904\u7406","text":"<p>\u200b\u6807\u6ce8\u200b\u65f6\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u6807\u6ce8\u200b\uff0c\u200b\u6ca1\u6709\u200b\u533a\u5206\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u7684\u200b\u6807\u6ce8\u200b\u6846\u200b\u548c\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u7684\u200b\u6807\u6ce8\u200b\u6846\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bpython\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u6807\u7b7e\u200b\u7684\u200b\u5212\u5206\u200b\u3002</p> <p>\u200b\u5728\u200b\u672c\u200b\u9879\u76ee\u200b\u7684\u200b'/home/aistudio/work/seal_labeled_datas'\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u5b58\u653e\u200b\u4e86\u200b\u6807\u6ce8\u200b\u7684\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b\uff0c\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b'/home/aistudio/work/seal_labeled_datas/Label.txt'\u200b\u4e2d\u200b\u7684\u200b\u6807\u6ce8\u200b\u5185\u5bb9\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>img/test1.png   [{\"transcription\": \"\u200b\u5f85\u200b\u8bc6\u522b\u200b\", \"points\": [[408, 232], [537, 232], [537, 352], [408, 352]], \"difficult\": false}, {\"transcription\": \"\u200b\u7535\u5b50\u200b\u56de\u5355\u200b\", \"points\": [[437, 305], [504, 305], [504, 322], [437, 322]], \"difficult\": false}, {\"transcription\": \"\u200b\u4e91\u5357\u7701\u200b\u519c\u6751\u200b\u4fe1\u7528\u793e\u200b\", \"points\": [[417, 290], [434, 295], [438, 281], [446, 267], [455, 261], [472, 258], [489, 264], [498, 277], [502, 295], [526, 289], [518, 267], [503, 249], [475, 232], [446, 239], [429, 255], [418, 275]], \"difficult\": false}, {\"transcription\": \"\u200b\u4e13\u7528\u7ae0\u200b\", \"points\": [[437, 319], [503, 319], [503, 338], [437, 338]], \"difficult\": false}]\n</code></pre> <p>\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u901a\u8fc7\u200bpython\u200b\u4ee3\u7801\u200b\u5c06\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bad\u7ec3\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7684\u200b\u6807\u6ce8\u200b\u533a\u5206\u200b\u5f00\u200b\u3002</p> <pre><code>import numpy as np\nimport json\nimport cv2\nimport os\nfrom shapely.geometry import Polygon\n\n\ndef poly2box(poly):\n    xmin = np.min(np.array(poly)[:, 0])\n    ymin = np.min(np.array(poly)[:, 1])\n    xmax = np.max(np.array(poly)[:, 0])\n    ymax = np.max(np.array(poly)[:, 1])\n    return np.array([[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]])\n\n\ndef draw_text_det_res(dt_boxes, src_im, color=(255, 255, 0)):\n    for box in dt_boxes:\n        box = np.array(box).astype(np.int32).reshape(-1, 2)\n        cv2.polylines(src_im, [box], True, color=color, thickness=2)\n    return src_im\n\nclass LabelDecode(object):\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data):\n        label = json.loads(data['label'])\n\n        nBox = len(label)\n        seal_boxes = self.get_seal_boxes(label)\n\n        gt_label = []\n\n        for seal_box in seal_boxes:\n            seal_anno = {'seal_box': seal_box}\n            boxes, txts, txt_tags = [], [], []\n\n            for bno in range(0, nBox):\n                box = label[bno]['points']\n                txt = label[bno]['transcription']\n                try:\n                    ints = self.get_intersection(box, seal_box)\n                except Exception as E:\n                    print(E)\n                    continue\n\n                if abs(Polygon(box).area - self.get_intersection(box, seal_box)) &lt; 1e-3 and \\\n                    abs(Polygon(box).area - self.get_union(box, seal_box)) &gt; 1e-3:\n\n                    boxes.append(box)\n                    txts.append(txt)\n                    if txt in ['*', '###', '\u200b\u5f85\u200b\u8bc6\u522b\u200b']:\n                        txt_tags.append(True)\n                    else:\n                        txt_tags.append(False)\n\n            seal_anno['polys'] = boxes\n            seal_anno['texts'] = txts\n            seal_anno['ignore_tags'] = txt_tags\n\n            gt_label.append(seal_anno)\n\n        return gt_label\n\n    def get_seal_boxes(self, label):\n\n        nBox = len(label)\n        seal_box = []\n        for bno in range(0, nBox):\n            box = label[bno]['points']\n            if len(box) == 4:\n                seal_box.append(box)\n\n        if len(seal_box) == 0:\n            return None\n\n        seal_box = self.valid_seal_box(seal_box)\n        return seal_box\n\n\n    def is_seal_box(self, box, boxes):\n        is_seal = True\n        for poly in boxes:\n            if list(box.shape()) != list(box.shape.shape()):\n                if abs(Polygon(box).area - self.get_intersection(box, poly)) &lt; 1e-3:\n                    return False\n            else:\n                if np.sum(np.array(box) - np.array(poly)) &lt; 1e-3:\n                    # continue when the box is same with poly\n                    continue\n                if abs(Polygon(box).area - self.get_intersection(box, poly)) &lt; 1e-3:\n                    return False\n        return is_seal\n\n\n    def valid_seal_box(self, boxes):\n        if len(boxes) == 1:\n            return boxes\n\n        new_boxes = []\n        flag = True\n        for k in range(0, len(boxes)):\n            flag = True\n            tmp_box = boxes[k]\n            for i in range(0, len(boxes)):\n                if k == i: continue\n                if abs(Polygon(tmp_box).area - self.get_intersection(tmp_box, boxes[i])) &lt; 1e-3:\n                    flag = False\n                    continue\n            if flag:\n                new_boxes.append(tmp_box)\n\n        return new_boxes\n\n\n    def get_union(self, pD, pG):\n        return Polygon(pD).union(Polygon(pG)).area\n\n    def get_intersection_over_union(self, pD, pG):\n        return get_intersection(pD, pG) / get_union(pD, pG)\n\n    def get_intersection(self, pD, pG):\n        return Polygon(pD).intersection(Polygon(pG)).area\n\n    def expand_points_num(self, boxes):\n        max_points_num = 0\n        for box in boxes:\n            if len(box) &gt; max_points_num:\n                max_points_num = len(box)\n        ex_boxes = []\n        for box in boxes:\n            ex_box = box + [box[-1]] * (max_points_num - len(box))\n            ex_boxes.append(ex_box)\n        return ex_boxes\n\n\ndef gen_extract_label(data_dir, label_file, seal_gt, seal_ppocr_gt):\n    label_decode_func = LabelDecode()\n    gts = open(label_file, \"r\").readlines()\n\n    seal_gt_list = []\n    seal_ppocr_list = []\n\n    for idx, line in enumerate(gts):\n        img_path, label = line.strip().split(\"\\t\")\n        data = {'label': label, 'img_path':img_path}\n        res = label_decode_func(data)\n        src_img = cv2.imread(os.path.join(data_dir, img_path))\n        if res is None:\n            print(\"ERROR! res is None!\")\n            continue\n\n        anno = []\n        for i, gt in enumerate(res):\n            # print(i, box, type(box), )\n            anno.append({'polys': gt['seal_box'], 'cls':1})\n\n        seal_gt_list.append(f\"{img_path}\\t{json.dumps(anno)}\\n\")\n        seal_ppocr_list.append(f\"{img_path}\\t{json.dumps(res)}\\n\")\n\n    if not os.path.exists(os.path.dirname(seal_gt)):\n        os.makedirs(os.path.dirname(seal_gt))\n    if not os.path.exists(os.path.dirname(seal_ppocr_gt)):\n        os.makedirs(os.path.dirname(seal_ppocr_gt))\n\n    with open(seal_gt, \"w\") as f:\n        f.writelines(seal_gt_list)\n        f.close()\n\n    with open(seal_ppocr_gt, 'w') as f:\n        f.writelines(seal_ppocr_list)\n        f.close()\n\ndef vis_seal_ppocr(data_dir, label_file, save_dir):\n\n    datas = open(label_file, 'r').readlines()\n    for idx, line in enumerate(datas):\n        img_path, label = line.strip().split('\\t')\n        img_path = os.path.join(data_dir, img_path)\n\n        label = json.loads(label)\n        src_im = cv2.imread(img_path)\n        if src_im is None:\n            continue\n\n        for anno in label:\n            seal_box = anno['seal_box']\n            txt_boxes = anno['polys']\n\n             # vis seal box\n            src_im = draw_text_det_res([seal_box], src_im, color=(255, 255, 0))\n            src_im = draw_text_det_res(txt_boxes, src_im, color=(255, 0, 0))\n\n        save_path = os.path.join(save_dir, os.path.basename(img_path))\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        # print(src_im.shape)\n        cv2.imwrite(save_path, src_im)\n\n\ndef draw_html(img_dir, save_name):\n    import glob\n\n    images_dir = glob.glob(img_dir + \"/*\")\n    print(len(images_dir))\n\n    html_path = save_name\n    with open(html_path, 'w') as html:\n        html.write('&lt;html&gt;\\n&lt;body&gt;\\n')\n        html.write('&lt;table border=\"1\"&gt;\\n')\n        html.write(\"&lt;meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=utf-8\\\" /&gt;\")\n\n        html.write(\"&lt;tr&gt;\\n\")\n        html.write(f'&lt;td&gt; \\n GT')\n\n        for i, filename in enumerate(sorted(images_dir)):\n            if filename.endswith(\"txt\"): continue\n            print(filename)\n\n            base = \"{}\".format(filename)\n            if True:\n                html.write(\"&lt;tr&gt;\\n\")\n                html.write(f'&lt;td&gt; {filename}\\n GT')\n                html.write('&lt;td&gt;GT 310\\n&lt;img src=\"%s\" width=640&gt;&lt;/td&gt;' % (base))\n                html.write(\"&lt;/tr&gt;\\n\")\n\n        html.write('&lt;style&gt;\\n')\n        html.write('span {\\n')\n        html.write('    color: red;\\n')\n        html.write('}\\n')\n        html.write('&lt;/style&gt;\\n')\n        html.write('&lt;/table&gt;\\n')\n        html.write('&lt;/html&gt;\\n&lt;/body&gt;\\n')\n    print(\"ok\")\n\n\ndef crop_seal_from_img(label_file, data_dir, save_dir, save_gt_path):\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    datas = open(label_file, 'r').readlines()\n    all_gts = []\n    count = 0\n    for idx, line in enumerate(datas):\n        img_path, label = line.strip().split('\\t')\n        img_path = os.path.join(data_dir, img_path)\n\n        label = json.loads(label)\n        src_im = cv2.imread(img_path)\n        if src_im is None:\n            continue\n\n        for c, anno in enumerate(label):\n            seal_poly = anno['seal_box']\n            txt_boxes = anno['polys']\n            txts = anno['texts']\n            ignore_tags = anno['ignore_tags']\n\n            box = poly2box(seal_poly)\n            img_crop = src_im[box[0][1]:box[2][1], box[0][0]:box[2][0], :]\n\n            save_path = os.path.join(save_dir, f\"{idx}_{c}.jpg\")\n            cv2.imwrite(save_path, np.array(img_crop))\n\n            img_gt = []\n            for i in range(len(txts)):\n                txt_boxes_crop = np.array(txt_boxes[i])\n                txt_boxes_crop[:, 1] -= box[0, 1]\n                txt_boxes_crop[:, 0] -= box[0, 0]\n                img_gt.append({'transcription': txts[i], \"points\": txt_boxes_crop.tolist(), \"ignore_tag\": ignore_tags[i]})\n\n            if len(img_gt) &gt;= 1:\n                count += 1\n            save_gt = f\"{os.path.basename(save_path)}\\t{json.dumps(img_gt)}\\n\"\n\n            all_gts.append(save_gt)\n\n    print(f\"The num of all image: {len(all_gts)}, and the number of useful image: {count}\")\n    if not os.path.exists(os.path.dirname(save_gt_path)):\n        os.makedirs(os.path.dirname(save_gt_path))\n\n    with open(save_gt_path, \"w\") as f:\n        f.writelines(all_gts)\n        f.close()\n    print(\"Done\")\n\n\nif __name__ == \"__main__\":\n    # \u200b\u6570\u636e\u5904\u7406\u200b\n    gen_extract_label(\"./seal_labeled_datas\", \"./seal_labeled_datas/Label.txt\", \"./seal_ppocr_gt/seal_det_img.txt\", \"./seal_ppocr_gt/seal_ppocr_img.txt\")\n    vis_seal_ppocr(\"./seal_labeled_datas\", \"./seal_ppocr_gt/seal_ppocr_img.txt\", \"./seal_ppocr_gt/seal_ppocr_vis/\")\n    draw_html(\"./seal_ppocr_gt/seal_ppocr_vis/\", \"./vis_seal_ppocr.html\")\n    seal_ppocr_img_label = \"./seal_ppocr_gt/seal_ppocr_img.txt\"\n    crop_seal_from_img(seal_ppocr_img_label, \"./seal_labeled_datas/\", \"./seal_img_crop\", \"./seal_img_crop/label.txt\")\n</code></pre> <p>\u200b\u5904\u7406\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u751f\u6210\u200b\u7684\u200b\u6587\u4ef6\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>\u251c\u2500\u2500 seal_img_crop/\n\u2502   \u251c\u2500\u2500 0_0.jpg\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 label.txt\n\u251c\u2500\u2500 seal_ppocr_gt/\n\u2502   \u251c\u2500\u2500 seal_det_img.txt\n\u2502   \u251c\u2500\u2500 seal_ppocr_img.txt\n\u2502   \u2514\u2500\u2500 seal_ppocr_vis/\n\u2502       \u251c\u2500\u2500 test1.png\n\u2502       \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 vis_seal_ppocr.html\n</code></pre> <p>\u200b\u5176\u4e2d\u200b<code>seal_img_crop/label.txt</code>\u200b\u6587\u4ef6\u200b\u4e3a\u200b\u5370\u7ae0\u200b\u8bc6\u522b\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\uff0c\u200b\u5176\u200b\u5185\u5bb9\u200b\u683c\u5f0f\u200b\u4e3a\u200b\uff1a</p> <pre><code>0_0.jpg    [{\"transcription\": \"\\u7535\\u5b50\\u56de\\u5355\", \"points\": [[29, 73], [96, 73], [96, 90], [29, 90]], \"ignore_tag\": false}, {\"transcription\": \"\\u4e91\\u5357\\u7701\\u519c\\u6751\\u4fe1\\u7528\\u793e\", \"points\": [[9, 58], [26, 63], [30, 49], [38, 35], [47, 29], [64, 26], [81, 32], [90, 45], [94, 63], [118, 57], [110, 35], [95, 17], [67, 0], [38, 7], [21, 23], [10, 43]], \"ignore_tag\": false}, {\"transcription\": \"\\u4e13\\u7528\\u7ae0\", \"points\": [[29, 87], [95, 87], [95, 106], [29, 106]], \"ignore_tag\": false}]\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u7528\u4e8e\u200bPaddleOCR\u200b\u7684\u200bPGNet\u200b\u7b97\u6cd5\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002</p> <p><code>seal_ppocr_gt/seal_det_img.txt</code>\u200b\u4e3a\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\uff0c\u200b\u5176\u200b\u5185\u5bb9\u200b\u683c\u5f0f\u200b\u4e3a\u200b\uff1a</p> <pre><code>img/test1.png    [{\"polys\": [[408, 232], [537, 232], [537, 352], [408, 352]], \"cls\": 1}]\n</code></pre> <p>\u200b\u4e3a\u4e86\u200b\u4f7f\u7528\u200bPaddleDetection\u200b\u5de5\u5177\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b<code>seal_det_img.txt</code>\u200b\u8f6c\u6362\u200b\u4e3a\u200bCOCO\u200b\u6216\u8005\u200bVOC\u200b\u7684\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u4e0b\u8ff0\u200b\u4ee3\u7801\u200b\u5c06\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6807\u6ce8\u200b\u8f6c\u6362\u6210\u200bVOC\u200b\u683c\u5f0f\u200b\u3002</p> <pre><code>import numpy as np\nimport json\nimport cv2\nimport os\nfrom shapely.geometry import Polygon\n\nseal_train_gt = \"./seal_ppocr_gt/seal_det_img.txt\"\n# \u200b\u6ce8\u200b\uff1a\u200b\u4ec5\u200b\u7528\u4e8e\u200b\u793a\u4f8b\u200b\uff0c\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u4e2d\u200b\u9700\u8981\u200b\u5206\u522b\u200b\u8f6c\u6362\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u6807\u7b7e\u200b\nseal_valid_gt = \"./seal_ppocr_gt/seal_det_img.txt\"\n\ndef gen_main_train_txt(mode='train'):\n    if mode == \"train\":\n        file_path = seal_train_gt\n    if mode in ['valid', 'test']:\n        file_path = seal_valid_gt\n\n    save_path = f\"./seal_VOC/ImageSets/Main/{mode}.txt\"\n    save_train_path = f\"./seal_VOC/{mode}.txt\"\n    if not os.path.exists(os.path.dirname(save_path)):\n        os.makedirs(os.path.dirname(save_path))\n\n    datas = open(file_path, 'r').readlines()\n    img_names = []\n    train_names = []\n    for line in datas:\n        img_name = line.strip().split('\\t')[0]\n        img_name = os.path.basename(img_name)\n        (i_name, extension) = os.path.splitext(img_name)\n        t_name = 'JPEGImages/'+str(img_name)+' '+'Annotations/'+str(i_name)+'.xml\\n'\n        train_names.append(t_name)\n        img_names.append(i_name + \"\\n\")\n\n    with open(save_train_path, \"w\") as f:\n        f.writelines(train_names)\n        f.close()\n\n    with open(save_path, \"w\") as f:\n        f.writelines(img_names)\n        f.close()\n\n    print(f\"{mode} save done\")\n\n\ndef gen_xml_label(mode='train'):\n    if mode == \"train\":\n        file_path = seal_train_gt\n    if mode in ['valid', 'test']:\n        file_path = seal_valid_gt\n\n    datas = open(file_path, 'r').readlines()\n    img_names = []\n    train_names = []\n    anno_path = \"./seal_VOC/Annotations\"\n    img_path = \"./seal_VOC/JPEGImages\"\n\n    if not os.path.exists(anno_path):\n        os.makedirs(anno_path)\n    if not os.path.exists(img_path):\n        os.makedirs(img_path)\n\n    for idx, line in enumerate(datas):\n        img_name, label = line.strip().split('\\t')\n        img = cv2.imread(os.path.join(\"./seal_labeled_datas\", img_name))\n        cv2.imwrite(os.path.join(img_path, os.path.basename(img_name)), img)\n        height, width, c = img.shape\n        img_name = os.path.basename(img_name)\n        (i_name, extension) = os.path.splitext(img_name)\n        label = json.loads(label)\n\n        xml_file = open((\"./seal_VOC/Annotations\" + '/' + i_name + '.xml'), 'w')\n        xml_file.write('&lt;annotation&gt;\\n')\n        xml_file.write('    &lt;folder&gt;seal_VOC&lt;/folder&gt;\\n')\n        xml_file.write('    &lt;filename&gt;' + str(img_name) + '&lt;/filename&gt;\\n')\n        xml_file.write('    &lt;path&gt;' + 'Annotations/' + str(img_name) + '&lt;/path&gt;\\n')\n        xml_file.write('    &lt;size&gt;\\n')\n        xml_file.write('        &lt;width&gt;' + str(width) + '&lt;/width&gt;\\n')\n        xml_file.write('        &lt;height&gt;' + str(height) + '&lt;/height&gt;\\n')\n        xml_file.write('        &lt;depth&gt;3&lt;/depth&gt;\\n')\n        xml_file.write('    &lt;/size&gt;\\n')\n        xml_file.write('    &lt;segmented&gt;0&lt;/segmented&gt;\\n')\n\n        for anno in label:\n            poly = anno['polys']\n            if anno['cls'] == 1:\n                gt_cls = 'redseal'\n            xmin = np.min(np.array(poly)[:, 0])\n            ymin = np.min(np.array(poly)[:, 1])\n            xmax = np.max(np.array(poly)[:, 0])\n            ymax = np.max(np.array(poly)[:, 1])\n            xmin,ymin,xmax,ymax= int(xmin),int(ymin),int(xmax),int(ymax)\n            xml_file.write('    &lt;object&gt;\\n')\n            xml_file.write('        &lt;name&gt;'+str(gt_cls)+'&lt;/name&gt;\\n')\n            xml_file.write('        &lt;pose&gt;Unspecified&lt;/pose&gt;\\n')\n            xml_file.write('        &lt;truncated&gt;0&lt;/truncated&gt;\\n')\n            xml_file.write('        &lt;difficult&gt;0&lt;/difficult&gt;\\n')\n            xml_file.write('        &lt;bndbox&gt;\\n')\n            xml_file.write('            &lt;xmin&gt;'+str(xmin)+'&lt;/xmin&gt;\\n')\n            xml_file.write('            &lt;ymin&gt;'+str(ymin)+'&lt;/ymin&gt;\\n')\n            xml_file.write('            &lt;xmax&gt;'+str(xmax)+'&lt;/xmax&gt;\\n')\n            xml_file.write('            &lt;ymax&gt;'+str(ymax)+'&lt;/ymax&gt;\\n')\n            xml_file.write('        &lt;/bndbox&gt;\\n')\n            xml_file.write('    &lt;/object&gt;\\n')\n        xml_file.write('&lt;/annotation&gt;')\n        xml_file.close()\n    print(f'{mode} xml save done!')\n\n\ngen_main_train_txt()\ngen_main_train_txt('valid')\ngen_xml_label('train')\ngen_xml_label('valid')\n</code></pre> <p>\u200b\u6570\u636e\u5904\u7406\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u8f6c\u6362\u200b\u4e3a\u200bVOC\u200b\u683c\u5f0f\u200b\u7684\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b\u5728\u200b~/data/seal_VOC\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u76ee\u5f55\u200b\u7ec4\u7ec7\u200b\u7ed3\u6784\u200b\u4e3a\u200b\uff1a</p> <pre><code>\u251c\u2500\u2500 Annotations/\n\u251c\u2500\u2500 ImageSets/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Main/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 train.txt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 valid.txt\n\u251c\u2500\u2500 JPEGImages/\n\u251c\u2500\u2500 train.txt\n\u2514\u2500\u2500 valid.txt\n\u2514\u2500\u2500 label_list.txt\n</code></pre> <p>Annotations\u200b\u4e0b\u200b\u4e3a\u200b\u6570\u636e\u200b\u7684\u200b\u6807\u7b7e\u200b\uff0cJPEGImages\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u4e3a\u200b\u56fe\u50cf\u6587\u4ef6\u200b\uff0clabel_list.txt\u200b\u4e3a\u200b\u6807\u6ce8\u200b\u68c0\u6d4b\u200b\u6846\u200b\u7c7b\u522b\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bPaddleDetection\u200b\u5de5\u5177\u200b\u5e93\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#4","title":"4. \u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u5b9e\u8df5","text":"<p>\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u5370\u7ae0\u200b\u591a\u200b\u662f\u200b\u51fa\u73b0\u200b\u5728\u200b\u5408\u540c\u200b\uff0c\u200b\u53d1\u7968\u200b\uff0c\u200b\u516c\u544a\u200b\u7b49\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7684\u200b\u4efb\u52a1\u200b\u9700\u8981\u200b\u6392\u9664\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u80cc\u666f\u200b\u6587\u5b57\u200b\u7684\u200b\u5f71\u54cd\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u5148\u200b\u68c0\u6d4b\u200b\u51fa\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u3002</p> <p>\u200b\u501f\u52a9\u200bPaddleDetection\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e93\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u5bb9\u6613\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u4f7f\u7528\u200bPaddleDetection\u200b\u8bad\u7ec3\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u9009\u62e9\u200b\u7b97\u6cd5\u200b</li> <li>\u200b\u4fee\u6539\u200b\u6570\u636e\u200b\u96c6\u200b\u914d\u7f6e\u200b\u8def\u5f84\u200b</li> <li>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b</li> </ul> <p>\u200b\u7b97\u6cd5\u200b\u9009\u62e9\u200b</p> <p>PaddleDetection\u200b\u4e2d\u6709\u200b\u8bb8\u591a\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\uff0c\u200b\u8003\u8651\u200b\u5230\u200b\u6bcf\u6761\u200b\u6570\u636e\u200b\u4e2d\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u8f83\u4e3a\u200b\u6e05\u6670\u200b\uff0c\u200b\u4e14\u200b\u8003\u8651\u200b\u5230\u200b\u6027\u80fd\u9700\u6c42\u200b\u3002\u200b\u5728\u200b\u672c\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u91c7\u7528\u200bmobilenetv3\u200b\u4e3a\u200bbackbone\u200b\u7684\u200bppyolo\u200b\u7b97\u6cd5\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5bf9\u5e94\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u662f\u200b\uff1aconfigs/ppyolo/ppyolo_mbv3_large.yml</p> <p>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b</p> <p>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u662f\u200bCOCO\uff0c \u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u4e3a\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b: \u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b'configs/ppyolo/ppyolo_mbv3_large.yml'\u200b\u672b\u5c3e\u200b\u589e\u52a0\u200b\u5982\u4e0b\u200b\u5185\u5bb9\u200b\uff1a</p> <pre><code>metric: VOC\nmap_type: 11point\nnum_classes: 2\n\nTrainDataset:\n  !VOCDataSet\n    dataset_dir: dataset/seal_VOC\n    anno_path: train.txt\n    label_list: label_list.txt\n    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']\n\nEvalDataset:\n  !VOCDataSet\n    dataset_dir: dataset/seal_VOC\n    anno_path: test.txt\n    label_list: label_list.txt\n    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']\n\nTestDataset:\n  !ImageFolder\n    anno_path: dataset/seal_VOC/label_list.txt\n</code></pre> <p>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u5728\u200bPaddleDetection/dataset\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5904\u7406\u200b\u540e\u200b\u7684\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u79fb\u52a8\u200b\u5230\u200bPaddleDetection/dataset\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6216\u8005\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f6f\u200b\u8fde\u63a5\u200b\u3002</p> <pre><code>!ln -s seal_VOC ./PaddleDetection/dataset/\n</code></pre> <p>\u200b\u53e6\u5916\u200b\u56fe\u8c61\u200b\u4e2d\u200b\u5370\u7ae0\u200b\u6570\u91cf\u200b\u6bd4\u8f83\u200b\u5c11\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200bNMS\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\u6570\u91cf\u200b\uff0c\u200b\u5373\u200bkeep_top_k\uff0cnms_top_k \u200b\u4ece\u200b100\uff0c1000\uff0c\u200b\u8c03\u6574\u200b\u4e3a\u200b10\uff0c100\u3002\u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b'configs/ppyolo/ppyolo_mbv3_large.yml'\u200b\u672b\u5c3e\u200b\u589e\u52a0\u200b\u5982\u4e0b\u200b\u5185\u5bb9\u200b\u5b8c\u6210\u200b\u540e\u5904\u7406\u200b\u53c2\u6570\u200b\u7684\u200b\u8c03\u6574\u200b</p> <pre><code>BBoxPostProcess:\n  decode:\n    name: YOLOBox\n    conf_thresh: 0.005\n    downsample_ratio: 32\n    clip_bbox: true\n    scale_x_y: 1.05\n  nms:\n    name: MultiClassNMS\n    keep_top_k: 10  # \u200b\u4fee\u6539\u200b\u524d\u200b100\n    nms_threshold: 0.45\n    nms_top_k: 100  # \u200b\u4fee\u6539\u200b\u524d\u200b1000\n    score_threshold: 0.005\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5728\u200bPaddleDetection\u200b\u4e2d\u200b\u589e\u52a0\u200b\u5370\u7ae0\u200b\u6570\u636e\u200b\u7684\u200b\u5904\u7406\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5373\u200b\u5728\u200bPaddleDetection/ppdet/data/source/\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u521b\u5efa\u200bseal.py\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u586b\u5145\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>import os\nimport numpy as np\nfrom ppdet.core.workspace import register, serializable\nfrom .dataset import DetDataset\nimport cv2\nimport json\n\nfrom ppdet.utils.logger import setup_logger\nlogger = setup_logger(__name__)\n\n\n@register\n@serializable\nclass SealDataSet(DetDataset):\n    \"\"\"\n    Load dataset with COCO format.\n\n    Args:\n        dataset_dir (str): root directory for dataset.\n        image_dir (str): directory for images.\n        anno_path (str): coco annotation file path.\n        data_fields (list): key name of data dictionary, at least have 'image'.\n        sample_num (int): number of samples to load, -1 means all.\n        load_crowd (bool): whether to load crowded ground-truth.\n            False as default\n        allow_empty (bool): whether to load empty entry. False as default\n        empty_ratio (float): the ratio of empty record number to total\n            record's, if empty_ratio is out of [0. ,1.), do not sample the\n            records and use all the empty entries. 1. as default\n    \"\"\"\n\n    def __init__(self,\n                 dataset_dir=None,\n                 image_dir=None,\n                 anno_path=None,\n                 data_fields=['image'],\n                 sample_num=-1,\n                 load_crowd=False,\n                 allow_empty=False,\n                 empty_ratio=1.):\n        super(SealDataSet, self).__init__(dataset_dir, image_dir, anno_path,\n                                          data_fields, sample_num)\n        self.load_image_only = False\n        self.load_semantic = False\n        self.load_crowd = load_crowd\n        self.allow_empty = allow_empty\n        self.empty_ratio = empty_ratio\n\n    def _sample_empty(self, records, num):\n        # if empty_ratio is out of [0. ,1.), do not sample the records\n        if self.empty_ratio &lt; 0. or self.empty_ratio &gt;= 1.:\n            return records\n        import random\n        sample_num = min(\n            int(num * self.empty_ratio / (1 - self.empty_ratio)), len(records))\n        records = random.sample(records, sample_num)\n        return records\n\n    def parse_dataset(self):\n        anno_path = os.path.join(self.dataset_dir, self.anno_path)\n        image_dir = os.path.join(self.dataset_dir, self.image_dir)\n\n        records = []\n        empty_records = []\n        ct = 0\n\n        assert anno_path.endswith('.txt'), \\\n            'invalid seal_gt file: ' + anno_path\n\n        all_datas = open(anno_path, 'r').readlines()\n\n        for idx, line in enumerate(all_datas):\n            im_path, label = line.strip().split('\\t')\n            img_path = os.path.join(image_dir, im_path)\n            label = json.loads(label)\n            im_h, im_w, im_c = cv2.imread(img_path).shape\n\n            coco_rec = {\n                'im_file': img_path,\n                'im_id': np.array([idx]),\n                'h': im_h,\n                'w': im_w,\n            } if 'image' in self.data_fields else {}\n\n            if not self.load_image_only:\n                bboxes = []\n                for anno in label:\n                    poly = anno['polys']\n                    # poly to box\n                    x1 = np.min(np.array(poly)[:, 0])\n                    y1 = np.min(np.array(poly)[:, 1])\n                    x2 = np.max(np.array(poly)[:, 0])\n                    y2 = np.max(np.array(poly)[:, 1])\n                eps = 1e-5\n                if x2 - x1 &gt; eps and y2 - y1 &gt; eps:\n                    clean_box = [\n                        round(float(x), 3) for x in [x1, y1, x2, y2]\n                    ]\n                    anno = {'clean_box': clean_box, 'gt_cls':int(anno['cls'])}\n                    bboxes.append(anno)\n                else:\n                    logger.info(\"invalid box\")\n\n            num_bbox = len(bboxes)\n            if num_bbox &lt;= 0:\n                continue\n\n            gt_bbox = np.zeros((num_bbox, 4), dtype=np.float32)\n            gt_class = np.zeros((num_bbox, 1), dtype=np.int32)\n            is_crowd = np.zeros((num_bbox, 1), dtype=np.int32)\n            # gt_poly = [None] * num_bbox\n\n            for i, box in enumerate(bboxes):\n                gt_class[i][0] = box['gt_cls']\n                gt_bbox[i, :] = box['clean_box']\n                is_crowd[i][0] = 0\n\n            gt_rec = {\n                        'is_crowd': is_crowd,\n                        'gt_class': gt_class,\n                        'gt_bbox': gt_bbox,\n                        # 'gt_poly': gt_poly,\n                    }\n\n            for k, v in gt_rec.items():\n                if k in self.data_fields:\n                    coco_rec[k] = v\n\n            records.append(coco_rec)\n            ct += 1\n            if self.sample_num &gt; 0 and ct &gt;= self.sample_num:\n                break\n        self.roidbs = records\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b</p> <p>\u200b\u542f\u52a8\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\u7684\u200b\u547d\u4ee4\u200b\u4e3a\u200b\uff1a</p> <pre><code>!python3  tools/train.py  -c configs/ppyolo/ppyolo_mbv3_large.yml  --eval\n\n# \u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u4e3a\u200b\uff1a\n!python3 -m paddle.distributed.launch   --gpus 0,1,2,3,4,5,6,7  tools/train.py  -c configs/ppyolo/ppyolo_mbv3_large.yml  --eval\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u4f1a\u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\uff1a</p> <pre><code>[07/05 11:42:09] ppdet.engine INFO: Eval iter: 0\n[07/05 11:42:14] ppdet.metrics.metrics INFO: Accumulating evaluatation results...\n[07/05 11:42:14] ppdet.metrics.metrics INFO: mAP(0.50, 11point) = 99.31%\n[07/05 11:42:14] ppdet.engine INFO: Total sample number: 112, averge FPS: 26.45840794253432\n[07/05 11:42:14] ppdet.engine INFO: Best test bbox ap is 0.996.\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u89c2\u5bdf\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff1a</p> <pre><code>!python3 tools/infer.py -c configs/ppyolo/ppyolo_mbv3_large.yml -o weights=./output/ppyolo_mbv3_large/model_final.pdparams  --img_dir=./test.jpg\n</code></pre> <p>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#5","title":"5. \u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u5b9e\u8df5","text":"<p>\u200b\u5728\u200b\u4f7f\u7528\u200bppyolo\u200b\u68c0\u6d4b\u200b\u5230\u200b\u5370\u7ae0\u200b\u533a\u57df\u200b\u540e\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u501f\u52a9\u200bPaddleOCR\u200b\u91cc\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\uff0c\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u4e2d\u200b\u6587\u5b57\u200b\u7684\u200b\u8bc6\u522b\u200b\u3002</p> <p>PaddleOCR\u200b\u4e2d\u200b\u7684\u200bOCR\u200b\u7b97\u6cd5\u200b\u5305\u542b\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u4ee5\u53ca\u200bOCR\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u7b97\u6cd5\u200b\u3002</p> <p>\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u8d1f\u8d23\u200b\u68c0\u6d4b\u200b\u5230\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u6587\u5b57\u200b\uff0c\u200b\u518d\u200b\u7531\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8bc6\u522b\u200b\u51fa\u200b\u68c0\u6d4b\u200b\u5230\u200b\u7684\u200b\u6587\u5b57\u200b\uff0c\u200b\u8fdb\u800c\u200b\u5b9e\u73b0\u200bOCR\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b+\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u4e32\u8054\u200b\u5b8c\u6210\u200bOCR\u200b\u4efb\u52a1\u200b\u7684\u200b\u67b6\u6784\u200b\u79f0\u4e3a\u200b\u4e24\u200b\u9636\u6bb5\u200b\u7684\u200bOCR\u200b\u7b97\u6cd5\u200b\u3002\u200b\u76f8\u5bf9\u200b\u5e94\u200b\u7684\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u7684\u200bOCR\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e00\u4e2a\u200b\u7b97\u6cd5\u200b\u540c\u65f6\u200b\u5b8c\u6210\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002</p> \u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b \u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b \u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u7b97\u6cd5\u200b DB\\DB++\\EAST\\SAST\\PSENet SVTR\\CRNN\\NRTN\\Abinet\\SAR... PGNet <p>\u200b\u672c\u200b\u8282\u200b\u4e2d\u5c06\u200b\u5206\u522b\u200b\u4ecb\u7ecd\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u4ee5\u53ca\u200b\u4e24\u200b\u9636\u6bb5\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u5728\u200b\u5370\u7ae0\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e0a\u200b\u7684\u200b\u5b9e\u8df5\u200b\u3002</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#51","title":"5.1 \u200b\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u5b9e\u8df5","text":"<p>\u200b\u672c\u200b\u8282\u200b\u4ecb\u7ecd\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u91cc\u200b\u7684\u200bPGNet\u200b\u7b97\u6cd5\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u3002</p> <p>PGNet\u200b\u5c5e\u4e8e\u200b\u7aef\u200b\u5bf9\u200b\u7aef\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u5728\u200bPaddleOCR\u200b\u4e2d\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e3a\u200b\uff1a PaddleOCR/configs/e2e/e2e_r50_vd_pg.yml</p> <p>\u200b\u4f7f\u7528\u200bPGNet\u200b\u5b8c\u6210\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e3a\u200b\uff1a</p> <ul> <li>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b</li> <li>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b</li> </ul> <p>PGNet\u200b\u9ed8\u8ba4\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u4e3a\u200btotaltext\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\uff0c\u200b\u672c\u6b21\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u4e3a\u200b\u4e0a\u200b\u4e00\u8282\u200b\u6570\u636e\u5904\u7406\u200b\u540e\u200b\u5f97\u5230\u200b\u7684\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b\u548c\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\uff1a</p> <p>\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u914d\u7f6e\u200b\u4fee\u6539\u200b\u540e\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: PGDataSet\n    data_dir: ./train_data/seal_ppocr\n    label_file_list: [./train_data/seal_ppocr/seal_ppocr_img.txt]\n    ratio_list: [1.0]\n</code></pre> <p>\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u914d\u7f6e\u200b\u4fee\u6539\u200b\u540e\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Eval:\n  dataset:\n    name: PGDataSet\n    data_dir: ./train_data/seal_ppocr_test\n    label_file_list: [./train_data/seal_ppocr_test/seal_ppocr_img.txt]\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\u7684\u200b\u547d\u4ee4\u200b\u4e3a\u200b:</p> <pre><code>!python3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b47.4%\u3002\u200b\u6570\u636e\u91cf\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6570\u636e\u200b\u8d28\u91cf\u200b\u8f83\u5dee\u200b\u4f1a\u200b\u5f71\u54cd\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u53c2\u4e0e\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u5c06\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u3002</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u70b9\u51fb\u200b\u6587\u672b\u200b\u7684\u200b\u94fe\u63a5\u200b\uff0c\u200b\u52a0\u5165\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u7fa4\u200b\u83b7\u53d6\u200b\u5168\u90e8\u200bOCR\u200b\u5782\u7c7b\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u3001\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u7b49\u200b\u5168\u5957\u200bOCR\u200b\u5b66\u4e60\u200b\u8d44\u6599\u200b\ud83c\udf81</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#52","title":"5.2 \u200b\u4e24\u200b\u9636\u6bb5\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u5b9e\u8df5","text":"<p>\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4f7f\u7528\u200bPGNet\u200b\u5b9e\u73b0\u200b\u5370\u7ae0\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6d41\u7a0b\u200b\u3002\u200b\u672c\u200b\u5c0f\u8282\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u91cc\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u548c\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u5206\u522b\u200b\u5b8c\u6210\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u7684\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u3002</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#521","title":"5.2.1 \u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u68c0\u6d4b","text":"<p>PaddleOCR\u200b\u4e2d\u200b\u5305\u542b\u200b\u4e30\u5bcc\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u5305\u542b\u200bDB\uff0cDB++\uff0cEAST\uff0cSAST\uff0cPSENet\u200b\u7b49\u7b49\u200b\u3002\u200b\u5176\u4e2d\u200bDB\uff0cDB++\uff0cPSENet\u200b\u5747\u200b\u652f\u6301\u200b\u5f2f\u66f2\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u672c\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200bDB++\u200b\u4f5c\u4e3a\u200b\u5370\u7ae0\u200b\u5f2f\u66f2\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u3002</p> <p>PaddleOCR\u200b\u4e2d\u200b\u53d1\u5e03\u200b\u7684\u200bdb++\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u662f\u200b\u82f1\u6587\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u91cd\u65b0\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4fee\u6539\u200b[DB++\u200b\u914d\u7f6e\u6587\u4ef6\u200b](DB++\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4f4d\u4e8e\u200bconfigs/det/det_r50_db++_icdar15.yml \u200b\u4e2d\u200b\u7684\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/seal_ppocr\n    label_file_list: [./train_data/seal_ppocr/seal_ppocr_img.txt]\n    ratio_list: [1.0]\n</code></pre> <p>\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u914d\u7f6e\u200b\u4fee\u6539\u200b\u540e\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Eval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/seal_ppocr_test\n    label_file_list: [./train_data/seal_ppocr_test/seal_ppocr_img.txt]\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>!python3 tools/train.py  -c  configs/det/det_r50_db++_icdar15.yml -o Global.epoch_num=100\n</code></pre> <p>\u200b\u8003\u8651\u200b\u5230\u200b\u6570\u636e\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u901a\u8fc7\u200bGlobal.epoch_num\u200b\u8bbe\u7f6e\u200b\u4ec5\u200b\u8bad\u7ec3\u200b100\u200b\u4e2a\u200bepoch\u3002 \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u9884\u6d4b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u70b9\u51fb\u200b\u6587\u672b\u200b\u7684\u200b\u94fe\u63a5\u200b\uff0c\u200b\u52a0\u5165\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u7fa4\u200b\u83b7\u53d6\u200b\u5168\u90e8\u200bOCR\u200b\u5782\u7c7b\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u3001\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u7b49\u200b\u5168\u5957\u200bOCR\u200b\u5b66\u4e60\u200b\u8d44\u6599\u200b\ud83c\udf81</p>"},{"location":"en/applications/%E5%8D%B0%E7%AB%A0%E5%BC%AF%E6%9B%B2%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#522","title":"5.2.2 \u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b","text":"<p>\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e2d\u200b\u5b8c\u6210\u200b\u4e86\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u672c\u200b\u8282\u200b\u4ecb\u7ecd\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u91c7\u7528\u200bSVTR\u200b\u7b97\u6cd5\u200b\uff0cSVTR\u200b\u7b97\u6cd5\u200b\u662f\u200bIJCAI\u200b\u6536\u5f55\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\uff0cSVTR\u200b\u6a21\u578b\u200b\u5177\u5907\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200b\u7279\u70b9\u200b\u3002</p> <p>\u200b\u5728\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u9700\u8981\u200b\u51c6\u5907\u200b\u5370\u7ae0\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u9700\u8981\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5c06\u200b\u5370\u7ae0\u200b\u4e2d\u200b\u7684\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u526a\u5207\u200b\u51fa\u6765\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002</p> <pre><code>import cv2\nimport numpy as np\n\ndef get_rotate_crop_image(img, points):\n    '''\n    img_height, img_width = img.shape[0:2]\n    left = int(np.min(points[:, 0]))\n    right = int(np.max(points[:, 0]))\n    top = int(np.min(points[:, 1]))\n    bottom = int(np.max(points[:, 1]))\n    img_crop = img[top:bottom, left:right, :].copy()\n    points[:, 0] = points[:, 0] - left\n    points[:, 1] = points[:, 1] - top\n    '''\n    assert len(points) == 4, \"shape of points must be 4*2\"\n    img_crop_width = int(\n        max(\n            np.linalg.norm(points[0] - points[1]),\n            np.linalg.norm(points[2] - points[3])))\n    img_crop_height = int(\n        max(\n            np.linalg.norm(points[0] - points[3]),\n            np.linalg.norm(points[1] - points[2])))\n    pts_std = np.float32([[0, 0], [img_crop_width, 0],\n                          [img_crop_width, img_crop_height],\n                          [0, img_crop_height]])\n    M = cv2.getPerspectiveTransform(points, pts_std)\n    dst_img = cv2.warpPerspective(\n        img,\n        M, (img_crop_width, img_crop_height),\n        borderMode=cv2.BORDER_REPLICATE,\n        flags=cv2.INTER_CUBIC)\n    dst_img_height, dst_img_width = dst_img.shape[0:2]\n    if dst_img_height * 1.0 / dst_img_width &gt;= 1.5:\n        dst_img = np.rot90(dst_img)\n    return dst_img\n\n\ndef run(data_dir, label_file, save_dir):\n    datas = open(label_file, 'r').readlines()\n    for idx, line in enumerate(datas):\n        img_path, label = line.strip().split('\\t')\n        img_path = os.path.join(data_dir, img_path)\n\n        label = json.loads(label)\n        src_im = cv2.imread(img_path)\n        if src_im is None:\n            continue\n\n        for anno in label:\n            seal_box = anno['seal_box']\n            txt_boxes = anno['polys']\n            crop_im = get_rotate_crop_image(src_im, text_boxes)\n\n            save_path = os.path.join(save_dir, f'{idx}.png')\n            if not os.path.exists(save_dir):\n                os.makedirs(save_dir)\n            # print(src_im.shape)\n            cv2.imwrite(save_path, crop_im)\n</code></pre> <p>\u200b\u6570\u636e\u5904\u7406\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u5373\u53ef\u200b\u914d\u7f6e\u200b\u8bad\u7ec3\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u3002SVTR\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u9009\u62e9\u200bconfigs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml \u200b\u4fee\u6539\u200bSVTR\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u90e8\u5206\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/seal_ppocr_crop/\n    label_file_list:\n    - ./train_data/seal_ppocr_crop/train_list.txt\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u9884\u6d4b\u200b\u90e8\u5206\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/seal_ppocr_crop/\n    label_file_list:\n    - ./train_data/seal_ppocr_crop_test/train_list.txt\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>!python3 tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6307\u6807\u200b\u8fbe\u5230\u200b\u4e86\u200b61%\u3002 \u200b\u7531\u4e8e\u200b\u6570\u636e\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u65f6\u4f1a\u200b\u53d1\u73b0\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u4e0a\u200b\u7684\u200bacc\u200b\u6307\u6807\u200b\u8fdc\u5927\u4e8e\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200bacc\u200b\u6307\u6807\u200b\uff0c\u200b\u5373\u200b\u51fa\u73b0\u200b\u8fc7\u200b\u62df\u5408\u200b\u73b0\u8c61\u200b\u3002\u200b\u901a\u8fc7\u200b\u8865\u5145\u200b\u6570\u636e\u200b\u548c\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u53ef\u4ee5\u200b\u7f13\u89e3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html","title":"\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u7684\u200b\u53d1\u7968\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6","text":""},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#1","title":"1. \u200b\u9879\u76ee\u200b\u80cc\u666f\u200b\u53ca\u200b\u610f\u4e49","text":"<p>\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u5728\u200b\u6587\u6863\u200b\u573a\u666f\u200b\u4e2d\u200b\u88ab\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5982\u200b\u8eab\u4efd\u8bc1\u200b\u4e2d\u200b\u7684\u200b\u59d3\u540d\u200b\u3001\u200b\u4f4f\u5740\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\uff0c\u200b\u5feb\u9012\u200b\u5355\u4e2d\u200b\u7684\u200b\u59d3\u540d\u200b\u3001\u200b\u8054\u7cfb\u65b9\u5f0f\u200b\u7b49\u200b\u5173\u952e\u5b57\u200b\u6bb5\u200b\u5185\u5bb9\u200b\u7684\u200b\u62bd\u53d6\u200b\u3002\u200b\u4f20\u7edf\u200b\u57fa\u4e8e\u200b\u6a21\u677f\u200b\u5339\u914d\u200b\u7684\u200b\u65b9\u6848\u200b\u9700\u8981\u200b\u9488\u5bf9\u200b\u4e0d\u540c\u200b\u7684\u200b\u573a\u666f\u200b\u5236\u5b9a\u200b\u6a21\u677f\u200b\u5e76\u200b\u8fdb\u884c\u200b\u9002\u914d\u200b\uff0c\u200b\u8f83\u4e3a\u200b\u7e41\u7410\u200b\uff0c\u200b\u4e0d\u591f\u200b\u9c81\u68d2\u200b\u3002\u200b\u57fa\u4e8e\u200b\u8be5\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u501f\u52a9\u200b\u98de\u6868\u200b\u63d0\u4f9b\u200b\u7684\u200bPaddleOCR\u200b\u5957\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u65b9\u6848\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u5bf9\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u573a\u666f\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#2","title":"2. \u200b\u9879\u76ee\u200b\u5185\u5bb9","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u5f00\u6e90\u200b\u5957\u4ef6\u200b\uff0c\u200b\u4ee5\u200bVI-LayoutXLM\u200b\u591a\u200b\u6a21\u6001\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u6a21\u578b\u200b\u4e3a\u200b\u57fa\u7840\u200b\uff0c\u200b\u9488\u5bf9\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u573a\u666f\u200b\u8fdb\u884c\u200b\u9002\u914d\u200b\uff0c\u200b\u63d0\u53d6\u200b\u8be5\u200b\u573a\u666f\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#3","title":"3. \u200b\u5b89\u88c5\u200b\u73af\u5883","text":"<pre><code># \u200b\u9996\u5148\u200bgit\u200b\u5b98\u65b9\u200b\u7684\u200bPaddleOCR\u200b\u9879\u76ee\u200b\uff0c\u200b\u5b89\u88c5\u200b\u9700\u8981\u200b\u7684\u200b\u4f9d\u8d56\u200b\n# \u200b\u7b2c\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u6253\u5f00\u200b\u8be5\u200b\u6ce8\u91ca\u200b\ngit clone https://gitee.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\n# \u200b\u5b89\u88c5\u200bPaddleOCR\u200b\u7684\u200b\u4f9d\u8d56\u200b\npip install -r requirements.txt\n# \u200b\u5b89\u88c5\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u4efb\u52a1\u200b\u7684\u200b\u4f9d\u8d56\u200b\npip install -r ./ppstructure/kie/requirements.txt\n</code></pre>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#4","title":"4. \u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6","text":"<p>\u200b\u57fa\u4e8e\u200b\u6587\u6863\u200b\u56fe\u50cf\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u5305\u542b\u200b3\u200b\u4e2a\u200b\u90e8\u5206\u200b\uff1a\uff081\uff09\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\uff082\uff09\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\uff083\uff09\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5305\u62ec\u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b\u6216\u8005\u200b\u5173\u7cfb\u200b\u62bd\u53d6\u200b\uff0c\u200b\u4e0b\u9762\u200b\u5206\u522b\u200b\u8fdb\u884c\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#41","title":"4.1 \u200b\u6587\u672c\u200b\u68c0\u6d4b","text":"<p>\u200b\u672c\u6587\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u53d1\u7968\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u9884\u6d4b\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u6807\u6ce8\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u81ea\u5b9a\u4e49\u200b\u8be5\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b8c\u6210\u200b\u7aef\u5230\u200b\u7aef\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u90e8\u5206\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6559\u7a0b\u200b\uff0c\u200b\u6309\u7167\u200b\u8bad\u7ec3\u200b\u6570\u636e\u683c\u5f0f\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5b8c\u6210\u200b\u8be5\u200b\u573a\u666f\u200b\u4e0b\u5782\u200b\u7c7b\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u5fae\u8c03\u200b\u8fc7\u7a0b\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#42","title":"4.2 \u200b\u6587\u672c\u200b\u8bc6\u522b","text":"<p>\u200b\u672c\u6587\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u53d1\u7968\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u9884\u6d4b\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u63d0\u4f9b\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u81ea\u5b9a\u4e49\u200b\u8be5\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b8c\u6210\u200b\u7aef\u5230\u200b\u7aef\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u90e8\u5206\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6559\u7a0b\u200b\uff0c\u200b\u6309\u7167\u200b\u8bad\u7ec3\u200b\u6570\u636e\u683c\u5f0f\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5b8c\u6210\u200b\u8be5\u200b\u573a\u666f\u200b\u4e0b\u5782\u200b\u7c7b\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u5fae\u8c03\u200b\u8fc7\u7a0b\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#43-semantic-entity-recognition","title":"4.3 \u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b \uff08Semantic Entity Recognition\uff09","text":"<p>\u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u7ed9\u5b9a\u200b\u4e00\u6bb5\u200b\u6587\u672c\u200b\u884c\u200b\uff0c\u200b\u786e\u5b9a\u200b\u5176\u200b\u7c7b\u522b\u200b\uff08\u200b\u5982\u200b<code>\u200b\u59d3\u540d\u200b</code>\u3001<code>\u200b\u4f4f\u5740\u200b</code>\u200b\u7b49\u200b\u7c7b\u522b\u200b\uff09\u3002PaddleOCR\u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u7684\u200b\u591a\u200b\u6a21\u6001\u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u65b9\u6cd5\u200b\uff0c\u200b\u878d\u5408\u200b\u6587\u672c\u200b\u3001\u200b\u4f4d\u7f6e\u200b\u4e0e\u200b\u7248\u9762\u200b\u4fe1\u606f\u200b\uff0c\u200b\u76f8\u6bd4\u200bLayoutXLM\u200b\u591a\u200b\u6a21\u6001\u200b\u6a21\u578b\u200b\uff0c\u200b\u53bb\u200b\u9664\u4e86\u200b\u5176\u4e2d\u200b\u7684\u200b\u89c6\u89c9\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u7279\u5f81\u63d0\u53d6\u200b\u90e8\u5206\u200b\uff0c\u200b\u5f15\u5165\u200b\u7b26\u5408\u200b\u9605\u8bfb\u200b\u987a\u5e8f\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\u6392\u5e8f\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f7f\u7528\u200bUDML\u200b\u8054\u5408\u200b\u4e92\u200b\u84b8\u998f\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6700\u7ec8\u200b\u5728\u200b\u7cbe\u5ea6\u200b\u4e0e\u200b\u901f\u5ea6\u200b\u65b9\u9762\u200b\u5747\u200b\u8d85\u8d8a\u200bLayoutXLM\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bVI-LayoutXLM\u200b\u7684\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd\u200b\u4e0e\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aVI-LayoutXLM\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#431","title":"4.3.1 \u200b\u51c6\u5907\u200b\u6570\u636e","text":"<p>\u200b\u53d1\u7968\u200b\u573a\u666f\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u6807\u6ce8\u200b\u51fa\u200b\u5176\u4e2d\u200b\u7684\u200b\u5173\u952e\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u6807\u6ce8\u200b\u4e3a\u200b<code>\u200b\u95ee\u9898\u200b-\u200b\u7b54\u6848\u200b</code>\u200b\u7684\u200bkey-value pair\uff0c\u200b\u5982\u4e0b\u200b\uff0c\u200b\u7f16\u53f7\u200bNo\u200b\u4e3a\u200b12270830\uff0c\u200b\u5219\u200b<code>No</code>\u200b\u5b57\u200b\u6bb5\u200b\u6807\u6ce8\u200b\u4e3a\u200bquestion\uff0c<code>12270830</code>\u200b\u5b57\u200b\u6bb5\u200b\u6807\u6ce8\u200b\u4e3a\u200banswer\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\u3002</p> <p></p> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ul> <li>\u200b\u5982\u679c\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6ca1\u6709\u200b\u6807\u6ce8\u200b \u200b\u975e\u5173\u952e\u200b\u4fe1\u606f\u5185\u5bb9\u200b \u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5728\u200b\u6807\u6ce8\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u4efb\u52a1\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u200b\u9700\u8981\u200b\u6807\u6ce8\u200b\u8be5\u200b\u90e8\u5206\u200b\uff0c\u200b\u5982\u4e0a\u56fe\u200b\u6240\u793a\u200b\uff1b\u200b\u5982\u679c\u200b\u6807\u6ce8\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u5982\u679c\u200b\u540c\u65f6\u200b\u6807\u6ce8\u200b\u4e86\u200b\u975e\u5173\u952e\u200b\u4fe1\u606f\u5185\u5bb9\u200b \u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u8be5\u200b\u90e8\u5206\u200b\u7684\u200blabel\u200b\u8bb0\u200b\u4e3a\u200bother\u3002</li> <li>\u200b\u6807\u6ce8\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u9700\u8981\u200b\u4ee5\u200b\u6587\u672c\u200b\u884c\u4e3a\u200b\u5355\u4f4d\u200b\u8fdb\u884c\u200b\u6807\u6ce8\u200b\uff0c\u200b\u65e0\u9700\u200b\u6807\u6ce8\u200b\u5355\u4e2a\u200b\u5b57\u7b26\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u3002</li> </ul> <p>\u200b\u5df2\u7ecf\u200b\u5904\u7406\u200b\u597d\u200b\u7684\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u6570\u636e\u200b\u96c6\u4ece\u200b\u8fd9\u91cc\u200b\u4e0b\u8f7d\u200b\uff1a\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u3002</p> <p>\u200b\u4e0b\u8f7d\u200b\u597d\u200b\u53d1\u7968\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5e76\u200b\u89e3\u538b\u200b\u5728\u200btrain_data\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <pre><code>train_data\n  |--zzsfp\n       |---class_list.txt\n       |---imgs/\n       |---train.json\n       |---val.json\n</code></pre> <p>\u200b\u5176\u4e2d\u200b<code>class_list.txt</code>\u200b\u662f\u200b\u5305\u542b\u200b<code>other</code>, <code>question</code>, <code>answer</code>\uff0c3\u200b\u4e2a\u200b\u79cd\u7c7b\u200b\u7684\u200b\u7684\u200b\u7c7b\u522b\u200b\u5217\u8868\u200b\uff08\u200b\u4e0d\u200b\u533a\u5206\u200b\u5927\u5c0f\u5199\u200b\uff09\uff0c<code>imgs</code>\u200b\u76ee\u5f55\u200b\u5e95\u4e0b\u200b\uff0c<code>train.json</code>\u200b\u4e0e\u200b<code>val.json</code>\u200b\u5206\u522b\u200b\u8868\u793a\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u8bc4\u4f30\u200b\u96c6\u5408\u200b\u7684\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u3002\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u5305\u542b\u200b30\u200b\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u96c6\u4e2d\u200b\u5305\u542b\u200b8\u200b\u5f20\u200b\u56fe\u7247\u200b\u3002\u200b\u90e8\u5206\u200b\u6807\u6ce8\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <pre><code>b33.jpg [{\"transcription\": \"No\", \"label\": \"question\", \"points\": [[2882, 472], [3026, 472], [3026, 588], [2882, 588]], }, {\"transcription\": \"12269563\", \"label\": \"answer\", \"points\": [[3066, 448], [3598, 448], [3598, 576], [3066, 576]], ]}]\n</code></pre> <p>\u200b\u76f8\u6bd4\u200b\u4e8e\u200bOCR\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6807\u6ce8\u200b\uff0c\u200b\u4ec5\u200b\u591a\u200b\u4e86\u200b<code>label</code>\u200b\u5b57\u200b\u6bb5\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#432","title":"4.3.2 \u200b\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>VI-LayoutXLM\u200b\u7684\u200b\u914d\u7f6e\u200b\u4e3a\u200bser_vi_layoutxlm_xfund_zh_udml.yml\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u6570\u636e\u200b\u3001\u200b\u7c7b\u522b\u200b\u6570\u76ee\u200b\u4ee5\u53ca\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u3002</p> <pre><code>Architecture:\n  model_type: &amp;model_type \"kie\"\n  name: DistillationModel\n  algorithm: Distillation\n  Models:\n    Teacher:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: &amp;algorithm \"LayoutXLM\"\n      Transform:\n      Backbone:\n        name: LayoutXLMForSer\n        pretrained: True\n        # one of base or vi\n        mode: vi\n        checkpoints:\n        # \u200b\u5b9a\u4e49\u200b\u7c7b\u522b\u200b\u6570\u76ee\u200b\n        num_classes: &amp;num_classes 5\n   ...\n\nPostProcess:\n  name: DistillationSerPostProcess\n  model_name: [\"Student\", \"Teacher\"]\n  key: backbone_out\n  # \u200b\u5b9a\u4e49\u200b\u7c7b\u522b\u200b\u6587\u4ef6\u200b\n  class_path: &amp;class_path train_data/zzsfp/class_list.txt\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    # \u200b\u5b9a\u4e49\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u4e0e\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n    data_dir: train_data/zzsfp/imgs\n    label_file_list:\n      - train_data/zzsfp/train.json\n  ...\n\nEval:\n  dataset:\n    # \u200b\u5b9a\u4e49\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u4e0e\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n    name: SimpleDataSet\n    data_dir: train_data/zzsfp/imgs\n    label_file_list:\n      - train_data/zzsfp/val.json\n  ...\n</code></pre> <p>LayoutXLM\u200b\u4e0e\u200bVI-LayoutXLM\u200b\u9488\u5bf9\u200b\u8be5\u200b\u573a\u666f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u8fed\u4ee3\u200b\u8f6e\u6570\u200b Hmean LayoutXLM 50 100.00% VI-LayoutXLM 50 100.00% <p>\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c\u200b\u7531\u4e8e\u200b\u5f53\u524d\u200b\u6570\u636e\u91cf\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u573a\u666f\u200b\u6bd4\u8f83\u7b80\u5355\u200b\uff0c\u200b\u56e0\u6b64\u200b2\u200b\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200bHmean\u200b\u5747\u200b\u8fbe\u5230\u200b\u4e86\u200b100%\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#433","title":"4.3.3 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u77e5\u8bc6\u200b\u84b8\u998f\u200b\u7684\u200b\u7b56\u7565\u200b\uff0c\u200b\u6700\u7ec8\u200b\u4fdd\u7559\u200b\u4e86\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200b\u8bc4\u4f30\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u9488\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b: ser_vi_layoutxlm_xfund_zh.yml\uff0c\u200b\u4fee\u6539\u200b\u5185\u5bb9\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u914d\u7f6e\u200b\u76f8\u540c\u200b\uff0c\u200b\u5305\u62ec\u200b\u7c7b\u522b\u200b\u6570\u200b\u3001\u200b\u7c7b\u522b\u200b\u6620\u5c04\u200b\u6587\u4ef6\u200b\u3001\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u3002</p> <p>\u200b\u4fee\u6539\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6267\u884c\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u5b8c\u6210\u200b\u8bc4\u4f30\u200b\u8fc7\u7a0b\u200b\u3002</p> <pre><code># \u200b\u6ce8\u610f\u200b\uff1a\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5730\u5740\u200b\u4e0e\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u5730\u5740\u200b\uff0c\u200b\u5bf9\u200b\u8bc4\u4f30\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\npython3 tools/eval.py -c ./fapiao/ser_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>[2022/08/18 08:49:58] ppocr INFO: metric eval ***************\n[2022/08/18 08:49:58] ppocr INFO: precision:1.0\n[2022/08/18 08:49:58] ppocr INFO: recall:1.0\n[2022/08/18 08:49:58] ppocr INFO: hmean:1.0\n[2022/08/18 08:49:58] ppocr INFO: fps:1.9740402401574881\n</code></pre>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#434","title":"4.3.4 \u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code>python3 tools/infer_kie_token_ser.py -c fapiao/ser_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy Global.infer_img=./train_data/XFUND/zh_val/val.json Global.infer_mode=False\n</code></pre> <p>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>Global.save_res_path</code>\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u90e8\u5206\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <p></p> <ul> <li>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5728\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u4e3a\u200b\u6807\u6ce8\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4ece\u200bjson\u200b\u6587\u4ef6\u200b\u91cc\u9762\u200b\u8fdb\u884c\u200b\u8bfb\u53d6\u200b\u3002</li> </ul> <p>\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200bOCR\u200b\u5f15\u64ce\u200b\u7ed3\u679c\u200b\u5f97\u5230\u200b\u7684\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff0c\u200b\u5219\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002</p> <pre><code>python3 tools/infer_kie_token_ser.py -c fapiao/ser_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy Global.infer_img=./train_data/zzsfp/imgs/b25.jpg Global.infer_mode=True\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u5b83\u4f1a\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u83b7\u53d6\u200b\u6587\u672c\u200b\u4f4d\u7f6e\u200b\u4e0e\u200b\u5185\u5bb9\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6ca1\u6709\u200b\u6807\u6ce8\u200b\u989d\u5916\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u4e3a\u200bother\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5927\u591a\u6570\u200b\u68c0\u6d4b\u200b\u51fa\u6765\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u88ab\u200b\u9884\u6d4b\u200b\u4e3a\u200bquestion\u200b\u6216\u8005\u200banswer\u3002</p> <p>\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u6784\u5efa\u200b\u57fa\u4e8e\u200b\u4f60\u200b\u5728\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200bOCR\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4f20\u5165\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u7684\u200binference \u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200bOCR\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u4ee5\u53ca\u200bSER\u200b\u7684\u200b\u4e32\u8054\u200b\u8fc7\u7a0b\u200b\u3002</p> <pre><code>python3 tools/infer_kie_token_ser.py -c fapiao/ser_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy Global.infer_img=./train_data/zzsfp/imgs/b25.jpg Global.infer_mode=True Global.kie_rec_model_dir=\"your_rec_model\" Global.kie_det_model_dir=\"your_det_model\"\n</code></pre>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#44-relation-extraction","title":"4.4 \u200b\u5173\u7cfb\u200b\u62bd\u53d6\u200b\uff08Relation Extraction\uff09","text":"<p>\u200b\u4f7f\u7528\u200bSER\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u83b7\u53d6\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u6240\u6709\u200b\u7684\u200bquestion\u200b\u4e0e\u200banswer\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u7ee7\u7eed\u200b\u8fd9\u4e9b\u200b\u5b57\u6bb5\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u4e00\u6b65\u200b\u83b7\u53d6\u200bquestion\u200b\u4e0e\u200banswer\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8fde\u63a5\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u8fdb\u4e00\u6b65\u200b\u8bad\u7ec3\u200b\u5173\u7cfb\u200b\u62bd\u53d6\u200b\u6a21\u578b\u200b\uff0c\u200b\u89e3\u51b3\u200b\u8be5\u200b\u95ee\u9898\u200b\u3002\u200b\u672c\u6587\u200b\u4e5f\u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u591a\u200b\u6a21\u6001\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e0b\u6e38\u200bRE\u200b\u4efb\u52a1\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#441","title":"4.4.1 \u200b\u51c6\u5907\u200b\u6570\u636e","text":"<p>\u200b\u4ee5\u200b\u53d1\u7968\u200b\u573a\u666f\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200bSER\u200b\u4efb\u52a1\u200b\uff0cRE\u200b\u4e2d\u200b\u8fd8\u200b\u9700\u8981\u200b\u6807\u8bb0\u200b\u6bcf\u4e2a\u200b\u6587\u672c\u200b\u884c\u200b\u7684\u200bid\u200b\u4fe1\u606f\u200b\u4ee5\u53ca\u200b\u94fe\u63a5\u200b\u5173\u7cfb\u200blinking\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <p></p> <p></p> <p>\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u7684\u200b\u90e8\u5206\u200b\u5185\u5bb9\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <pre><code>b33.jpg [{\"transcription\": \"No\", \"label\": \"question\", \"points\": [[2882, 472], [3026, 472], [3026, 588], [2882, 588]], \"id\": 0, \"linking\": [[0, 1]]}, {\"transcription\": \"12269563\", \"label\": \"answer\", \"points\": [[3066, 448], [3598, 448], [3598, 576], [3066, 576]], \"id\": 1, \"linking\": [[0, 1]]}]\n</code></pre> <p>\u200b\u76f8\u6bd4\u200b\u4e0e\u200bSER\u200b\u7684\u200b\u6807\u6ce8\u200b\uff0c\u200b\u591a\u200b\u4e86\u200b<code>id</code>\u200b\u4e0e\u200b<code>linking</code>\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5206\u522b\u200b\u8868\u793a\u200b\u552f\u4e00\u200b\u6807\u8bc6\u200b\u4ee5\u53ca\u200b\u8fde\u63a5\u200b\u5173\u7cfb\u200b\u3002</p> <p>\u200b\u5df2\u7ecf\u200b\u5904\u7406\u200b\u597d\u200b\u7684\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u6570\u636e\u200b\u96c6\u4ece\u200b\u8fd9\u91cc\u200b\u4e0b\u8f7d\u200b\uff1a\u200b\u589e\u503c\u7a0e\u200b\u53d1\u7968\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u3002</p>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#442","title":"4.4.2 \u200b\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>\u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u7684\u200bRE\u200b\u4efb\u52a1\u200b\u914d\u7f6e\u200b\u4e3a\u200bre_vi_layoutxlm_xfund_zh_udml.yml\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u6570\u636e\u200b\u8def\u5f84\u200b\u3001\u200b\u7c7b\u522b\u200b\u5217\u8868\u200b\u6587\u4ef6\u200b\u3002</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    # \u200b\u5b9a\u4e49\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u4e0e\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n    data_dir: train_data/zzsfp/imgs\n    label_file_list:\n      - train_data/zzsfp/train.json\n    transforms:\n      - DecodeImage: # load image\n          img_mode: RGB\n          channel_first: False\n      - VQATokenLabelEncode: # Class handling label\n          contains_re: True\n          algorithm: *algorithm\n          class_path: &amp;class_path train_data/zzsfp/class_list.txt\n  ...\n\nEval:\n  dataset:\n    # \u200b\u5b9a\u4e49\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u4e0e\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n    name: SimpleDataSet\n    data_dir: train_data/zzsfp/imgs\n    label_file_list:\n      - train_data/zzsfp/val.json\n  ...\n</code></pre> <p>LayoutXLM\u200b\u4e0e\u200bVI-LayoutXLM\u200b\u9488\u5bf9\u200b\u8be5\u200b\u573a\u666f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u8fed\u4ee3\u200b\u8f6e\u6570\u200b Hmean LayoutXLM 50 98.00% VI-LayoutXLM 50 99.30% <p>\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c\u200b\u5bf9\u4e8e\u200bVI-LayoutXLM\u200b\u76f8\u6bd4\u200bLayoutXLM\u200b\u7684\u200bHmean\u200b\u9ad8\u200b\u4e86\u200b1.3%\u3002</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#443","title":"4.4.3 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u77e5\u8bc6\u200b\u84b8\u998f\u200b\u7684\u200b\u7b56\u7565\u200b\uff0c\u200b\u6700\u7ec8\u200b\u4fdd\u7559\u200b\u4e86\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200b\u8bc4\u4f30\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u9488\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b: re_vi_layoutxlm_xfund_zh.yml\uff0c\u200b\u4fee\u6539\u200b\u5185\u5bb9\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u914d\u7f6e\u200b\u76f8\u540c\u200b\uff0c\u200b\u5305\u62ec\u200b\u7c7b\u522b\u200b\u6620\u5c04\u200b\u6587\u4ef6\u200b\u3001\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\u3002</p> <p>\u200b\u4fee\u6539\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6267\u884c\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u5b8c\u6210\u200b\u8bc4\u4f30\u200b\u8fc7\u7a0b\u200b\u3002</p> <pre><code># \u200b\u6ce8\u610f\u200b\uff1a\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5730\u5740\u200b\u4e0e\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u5730\u5740\u200b\uff0c\u200b\u5bf9\u200b\u8bc4\u4f30\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\npython3 tools/eval.py -c ./fapiao/re_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/re_vi_layoutxlm_fapiao_udml/best_accuracy\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>[2022/08/18 12:17:14] ppocr INFO: metric eval ***************\n[2022/08/18 12:17:14] ppocr INFO: precision:1.0\n[2022/08/18 12:17:14] ppocr INFO: recall:0.9873417721518988\n[2022/08/18 12:17:14] ppocr INFO: hmean:0.9936305732484078\n[2022/08/18 12:17:14] ppocr INFO: fps:2.765963539771157\n</code></pre>"},{"location":"en/applications/%E5%8F%91%E7%A5%A8%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html#444","title":"4.4.4 \u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code># -c \u200b\u540e\u9762\u200b\u7684\u200b\u662f\u200bRE\u200b\u4efb\u52a1\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\n# -o \u200b\u540e\u9762\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u662f\u200bRE\u200b\u4efb\u52a1\u200b\u7684\u200b\u914d\u7f6e\u200b\n# -c_ser \u200b\u540e\u9762\u200b\u7684\u200b\u662f\u200bSER\u200b\u4efb\u52a1\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\n# -c_ser \u200b\u540e\u9762\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u662f\u200bSER\u200b\u4efb\u52a1\u200b\u7684\u200b\u914d\u7f6e\u200b\npython3 tools/infer_kie_token_ser_re.py -c fapiao/re_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/re_vi_layoutxlm_fapiao_trained/best_accuracy Global.infer_img=./train_data/zzsfp/val.json Global.infer_mode=False -c_ser fapiao/ser_vi_layoutxlm.yml -o_ser Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_trained/best_accuracy\n</code></pre> <p>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>Global.save_res_path</code>\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u90e8\u5206\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> <p></p> <ul> <li>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5728\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u4e3a\u200b\u6807\u6ce8\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4ece\u200bjson\u200b\u6587\u4ef6\u200b\u91cc\u9762\u200b\u8fdb\u884c\u200b\u8bfb\u53d6\u200b\u3002</li> </ul> <p>\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200bOCR\u200b\u5f15\u64ce\u200b\u7ed3\u679c\u200b\u5f97\u5230\u200b\u7684\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff0c\u200b\u5219\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002</p> <pre><code>python3 tools/infer_kie_token_ser_re.py -c fapiao/re_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/re_vi_layoutxlm_fapiao_udml/best_accuracy Global.infer_img=./train_data/zzsfp/val.json Global.infer_mode=True -c_ser fapiao/ser_vi_layoutxlm.yml -o_ser Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy\n</code></pre> <p>\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u6784\u5efa\u200b\u57fa\u4e8e\u200b\u4f60\u200b\u5728\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200bOCR\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4f20\u5165\u200b\uff0c\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200bSER + RE\u200b\u7684\u200b\u4e32\u8054\u200b\u8fc7\u7a0b\u200b\u3002</p> <pre><code>python3 tools/infer_kie_token_ser_re.py -c fapiao/re_vi_layoutxlm.yml -o Architecture.Backbone.checkpoints=fapiao/models/re_vi_layoutxlm_fapiao_udml/best_accuracy Global.infer_img=./train_data/zzsfp/val.json Global.infer_mode=True -c_ser fapiao/ser_vi_layoutxlm.yml -o_ser Architecture.Backbone.checkpoints=fapiao/models/ser_vi_layoutxlm_fapiao_udml/best_accuracy Global.kie_rec_model_dir=\"your_rec_model\" Global.kie_det_model_dir=\"your_det_model\"\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html","title":"\u591a\u200b\u6a21\u6001\u200b\u8868\u5355\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1","title":"1 \u200b\u9879\u76ee\u200b\u8bf4\u660e","text":"<p>\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5728\u200b\u91d1\u878d\u200b\u9886\u57df\u200b\u7684\u200b\u5e94\u7528\u200b\u8986\u76d6\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u3001\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u3001\u200b\u89c6\u9891\u200b\u8bc6\u522b\u200b\u7b49\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\uff08OCR\uff09\u200b\u662f\u200b\u91d1\u878d\u200b\u9886\u57df\u200b\u4e2d\u200b\u7684\u200b\u6838\u5fc3\u200bAI\u200b\u80fd\u529b\u200b\uff0c\u200b\u5176\u200b\u5e94\u7528\u200b\u8986\u76d6\u200b\u5ba2\u6237\u670d\u52a1\u200b\u3001\u200b\u98ce\u9669\u200b\u9632\u63a7\u200b\u3001\u200b\u8fd0\u8425\u200b\u7ba1\u7406\u200b\u7b49\u200b\u5404\u9879\u200b\u4e1a\u52a1\u200b\uff0c\u200b\u9488\u5bf9\u200b\u7684\u200b\u5bf9\u8c61\u200b\u5305\u62ec\u200b\u901a\u7528\u200b\u5361\u8bc1\u200b\u7968\u636e\u200b\u8bc6\u522b\u200b\uff08\u200b\u94f6\u884c\u5361\u200b\u3001\u200b\u8eab\u4efd\u8bc1\u200b\u3001\u200b\u8425\u4e1a\u6267\u7167\u200b\u7b49\u200b\uff09\u3001\u200b\u901a\u7528\u200b\u6587\u672c\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\uff08\u200b\u5370\u5237\u4f53\u200b\u3001\u200b\u591a\u200b\u8bed\u8a00\u200b\u3001\u200b\u624b\u5199\u4f53\u200b\u7b49\u200b\uff09\u200b\u4ee5\u53ca\u200b\u4e00\u4e9b\u200b\u91d1\u878d\u200b\u7279\u8272\u200b\u7968\u636e\u200b\u51ed\u8bc1\u200b\u3002\u200b\u901a\u8fc7\u200b\u56e0\u6b64\u200b\u5982\u679c\u200b\u80fd\u591f\u200b\u5728\u200b\u7ed3\u6784\u5316\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u65f6\u200b\u540c\u65f6\u200b\u5229\u7528\u200b\u6587\u5b57\u200b\u3001\u200b\u9875\u9762\u200b\u5e03\u5c40\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4fbf\u200b\u53ef\u200b\u589e\u5f3a\u200b\u4e0d\u540c\u200b\u7248\u5f0f\u200b\u4e0b\u200b\u7684\u200b\u6cdb\u5316\u200b\u6027\u200b\u3002</p> <p>\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b\u65e8\u5728\u200b\u8bc6\u522b\u200b\u5404\u79cd\u200b\u5177\u6709\u200b\u8868\u683c\u200b\u6027\u8d28\u200b\u7684\u200b\u8bc1\u4ef6\u200b\u3001\u200b\u623f\u4ea7\u8bc1\u200b\u3001\u200b\u8425\u4e1a\u6267\u7167\u200b\u3001\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u8868\u200b\u3001\u200b\u53d1\u7968\u200b\u7b49\u200b\u5173\u952e\u200b\u952e\u503c\u200b\u5bf9\u200b(\u200b\u5982\u200b\u59d3\u540d\u200b-\u200b\u5f20\u4e09\u200b)\uff0c\u200b\u5176\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u4e8e\u200b\u94f6\u884c\u200b\u3001\u200b\u8bc1\u5238\u200b\u3001\u200b\u516c\u53f8\u8d22\u52a1\u200b\u7b49\u200b\u9886\u57df\u200b\uff0c\u200b\u5177\u6709\u200b\u5f88\u200b\u9ad8\u200b\u7684\u200b\u5546\u4e1a\u4ef7\u503c\u200b\u3002\u200b\u672c\u6b21\u200b\u8303\u4f8b\u200b\u9879\u76ee\u200b\u5f00\u6e90\u200b\u4e86\u200b\u5168\u200b\u6d41\u7a0b\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b\u65b9\u6848\u200b\uff0c\u200b\u80fd\u591f\u200b\u5728\u200b\u591a\u4e2a\u200b\u573a\u666f\u200b\u5feb\u901f\u200b\u5b9e\u73b0\u200b\u8fc1\u79fb\u200b\u80fd\u529b\u200b\u3002\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b\u901a\u5e38\u200b\u5b58\u5728\u200b\u4ee5\u4e0b\u200b\u96be\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u4eba\u5de5\u200b\u6458\u5f55\u200b\u5de5\u4f5c\u6548\u7387\u200b\u4f4e\u200b\uff1b</li> <li>\u200b\u56fd\u5185\u200b\u5e38\u89c1\u200b\u8868\u5355\u200b\u7248\u5f0f\u200b\u591a\u200b\uff1b</li> <li>\u200b\u4f20\u7edf\u200b\u6280\u672f\u200b\u65b9\u6848\u200b\u6cdb\u5316\u200b\u6548\u679c\u200b\u4e0d\u200b\u6ee1\u8db3\u200b\u3002</li> </ul> <p>\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b\u5305\u542b\u200b\u4e24\u5927\u200b\u9636\u6bb5\u200b\uff1aOCR\u200b\u9636\u6bb5\u200b\u548c\u200b\u6587\u6863\u200b\u89c6\u89c9\u200b\u95ee\u7b54\u200b\u9636\u6bb5\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\uff0cOCR\u200b\u9636\u6bb5\u200b\u9009\u53d6\u200b\u4e86\u200bPaddleOCR\u200b\u7684\u200bPP-OCRv2\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e3b\u8981\u200b\u7531\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u548c\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4e24\u4e2a\u200b\u6a21\u5757\u200b\u7ec4\u6210\u200b\u3002DOC-VQA\u200b\u6587\u6863\u200b\u89c6\u89c9\u200b\u95ee\u7b54\u200b\u9636\u6bb5\u200b\u57fa\u4e8e\u200bPaddleNLP\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u7b97\u6cd5\u200b\u5e93\u200b\u5b9e\u73b0\u200b\u7684\u200bLayoutXLM\u200b\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u57fa\u4e8e\u200b\u591a\u200b\u6a21\u6001\u200b\u65b9\u6cd5\u200b\u7684\u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b(Semantic Entity Recognition, SER)\u200b\u4ee5\u53ca\u200b\u5173\u7cfb\u200b\u62bd\u53d6\u200b(Relation Extraction, RE)\u200b\u4efb\u52a1\u200b\u3002\u200b\u672c\u200b\u6848\u4f8b\u200b\u6d41\u7a0b\u200b\u5982\u200b \u200b\u56fe\u200b1 \u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u6b22\u8fce\u200b\u518d\u200bAIStudio\u200b\u9886\u53d6\u200b\u514d\u8d39\u200b\u7b97\u529b\u200b\u4f53\u9a8c\u200b\u7ebf\u4e0a\u200b\u5b9e\u8bad\u200b\uff0c\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b: \u200b\u591a\u200b\u6a21\u6001\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2","title":"2 \u200b\u5b89\u88c5\u200b\u8bf4\u660e","text":"<p>\u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u6e90\u7801\u200b\uff0c\u200b\u4e0a\u8ff0\u200bAIStudio\u200b\u9879\u76ee\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u5e2e\u200b\u5927\u5bb6\u200b\u6253\u5305\u200b\u597d\u200b\u7684\u200bPaddleOCR(\u200b\u5df2\u7ecf\u200b\u4fee\u6539\u200b\u597d\u200b\u914d\u7f6e\u6587\u4ef6\u200b)\uff0c\u200b\u65e0\u9700\u200b\u4e0b\u8f7d\u200b\u89e3\u538b\u200b\u5373\u53ef\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u73af\u5883\u200b~</p> <pre><code>unzip -q PaddleOCR.zip\n</code></pre> <pre><code># \u200b\u5982\u200b\u4ecd\u200b\u9700\u200b\u5b89\u88c5\u200bor\u200b\u5b89\u88c5\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6267\u884c\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\n# git clone https://github.com/PaddlePaddle/PaddleOCR.git -b dygraph\n# git clone https://gitee.com/PaddlePaddle/PaddleOCR\n</code></pre> <pre><code># \u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5305\u200b\npip install -U pip\npip install -r /home/aistudio/PaddleOCR/requirements.txt\npip install paddleocr\n\npip install yacs gnureadline paddlenlp==2.2.1\npip install xlsxwriter\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#3","title":"3 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200bXFUN\u200b\u6570\u636e\u200b\u96c6\u505a\u200b\u4e3a\u200b\u5b9e\u9a8c\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 XFUN\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u5fae\u8f6f\u200b\u63d0\u51fa\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200bKIE\u200b\u4efb\u52a1\u200b\u7684\u200b\u591a\u200b\u8bed\u8a00\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5171\u200b\u5305\u542b\u200b\u4e03\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b149\u200b\u5f20\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b50\u200b\u5f20\u200b\u9a8c\u8bc1\u200b\u96c6\u200b</p> <p>\u200b\u5206\u522b\u200b\u4e3a\u200b\uff1aZH(\u200b\u4e2d\u6587\u200b)\u3001JA(\u200b\u65e5\u8bed\u200b)\u3001ES(\u200b\u897f\u73ed\u7259\u200b)\u3001FR(\u200b\u6cd5\u8bed\u200b)\u3001IT(\u200b\u610f\u5927\u5229\u200b)\u3001DE(\u200b\u5fb7\u8bed\u200b)\u3001PT(\u200b\u8461\u8404\u7259\u200b)</p> <p>\u200b\u672c\u6b21\u200b\u5b9e\u9a8c\u200b\u9009\u53d6\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6f14\u793a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u6cd5\u8bed\u200b\u6570\u636e\u200b\u96c6\u200b\u4f5c\u4e3a\u200b\u5b9e\u8df5\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u6837\u200b\u4f8b\u56fe\u200b\u5982\u200b \u200b\u56fe\u200b2 \u200b\u6240\u793a\u200b\u3002</p> <p></p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#31","title":"3.1 \u200b\u4e0b\u8f7d\u200b\u5904\u7406\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u5904\u7406\u200b\u597d\u200b\u7684\u200bXFUND\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\uff1ahttps://paddleocr.bj.bcebos.com/dataset/XFUND.tar ,\u200b\u53ef\u4ee5\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u6307\u4ee4\u200b\u5b8c\u6210\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u548c\u200b\u89e3\u538b\u200b\u3002</p> <p></p> <pre><code>wget https://paddleocr.bj.bcebos.com/dataset/XFUND.tar\ntar -xf XFUND.tar\n\n# XFUN\u200b\u5176\u4ed6\u200b\u6570\u636e\u200b\u96c6\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\n# \u200b\u4ee3\u7801\u200b\u94fe\u63a5\u200b\uff1ahttps://github.com/PaddlePaddle/PaddleOCR/blob/release%2F2.4/ppstructure/vqa/helper/trans_xfun_data.py\n# %cd PaddleOCR\n# python3 ppstructure/vqa/tools/trans_xfun_data.py --ori_gt_path=path/to/json_path --output_path=path/to/save_path\n# %cd ../\n</code></pre> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u6307\u4ee4\u200b\u540e\u200b\u5728\u200b /home/aistudio/PaddleOCR/ppstructure/vqa/XFUND \u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b2\u200b\u4e2a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>/home/aistudio/PaddleOCR/ppstructure/vqa/XFUND\n  \u2514\u2500 zh_train/                  \u200b\u8bad\u7ec3\u200b\u96c6\u200b\n      \u251c\u2500\u2500 image/              \u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u6587\u4ef6\u5939\u200b\n      \u251c\u2500\u2500 xfun_normalize_train.json   \u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\n  \u2514\u2500 zh_val/                    \u200b\u9a8c\u8bc1\u200b\u96c6\u200b\n      \u251c\u2500\u2500 image/          \u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u6587\u4ef6\u5939\u200b\n      \u251c\u2500\u2500 xfun_normalize_val.json     \u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\n</code></pre> <p>\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\u4e3a\u200b</p> <pre><code>{\n    \"height\": 3508, # \u200b\u56fe\u50cf\u200b\u9ad8\u5ea6\u200b\n    \"width\": 2480,  # \u200b\u56fe\u50cf\u200b\u5bbd\u5ea6\u200b\n    \"ocr_info\": [\n        {\n            \"text\": \"\u200b\u90ae\u653f\u200b\u5730\u5740\u200b:\",  # \u200b\u5355\u4e2a\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\n            \"label\": \"question\", # \u200b\u6587\u672c\u200b\u6240\u5c5e\u200b\u7c7b\u522b\u200b\n            \"bbox\": [261, 802, 483, 859], # \u200b\u5355\u4e2a\u200b\u6587\u672c\u6846\u200b\n            \"id\": 54,  # \u200b\u6587\u672c\u200b\u7d22\u5f15\u200b\n            \"linking\": [[54, 60]], # \u200b\u5f53\u524d\u200b\u6587\u672c\u200b\u548c\u200b\u5176\u4ed6\u200b\u6587\u672c\u200b\u7684\u200b\u5173\u7cfb\u200b [question, answer]\n            \"words\": []\n        },\n        {\n            \"text\": \"\u200b\u6e56\u5357\u7701\u200b\u6000\u5316\u5e02\u200b\u5e02\u8f96\u533a\u200b\",\n            \"label\": \"answer\",\n            \"bbox\": [487, 810, 862, 859],\n            \"id\": 60,\n            \"linking\": [[54, 60]],\n            \"words\": []\n        }\n    ]\n}\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#32-paddleocr","title":"3.2 \u200b\u8f6c\u6362\u200b\u4e3a\u200bPaddleOCR\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u683c\u5f0f","text":"<p>\u200b\u4f7f\u7528\u200bXFUND\u200b\u8bad\u7ec3\u200bPaddleOCR\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u6539\u4e3a\u200b\u8bad\u7ec3\u200b\u9700\u6c42\u200b\u7684\u200b\u683c\u5f0f\u200b\u3002</p> <p></p> <p>\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b \u200b\u6807\u6ce8\u200b\u6587\u4ef6\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u7528\u200b'\\t'\u200b\u5206\u9694\u200b\uff1a</p> <p>\" \u200b\u56fe\u50cf\u200b\u6587\u4ef6\u540d\u200b                    json.dumps\u200b\u7f16\u7801\u200b\u7684\u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\" ch4_test_images/img_61.jpg    [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]</p> <p>json.dumps\u200b\u7f16\u7801\u200b\u524d\u200b\u7684\u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b\u662f\u200b\u5305\u542b\u200b\u591a\u4e2a\u200b\u5b57\u5178\u200b\u7684\u200blist\uff0c\u200b\u5b57\u5178\u200b\u4e2d\u200b\u7684\u200b <code>points</code> \u200b\u8868\u793a\u200b\u6587\u672c\u6846\u200b\u7684\u200b\u56db\u4e2a\u200b\u70b9\u200b\u7684\u200b\u5750\u6807\u200b(x, y)\uff0c\u200b\u4ece\u200b\u5de6\u4e0a\u89d2\u200b\u7684\u200b\u70b9\u200b\u5f00\u59cb\u200b\u987a\u65f6\u9488\u200b\u6392\u5217\u200b\u3002 <code>transcription</code> \u200b\u8868\u793a\u200b\u5f53\u524d\u200b\u6587\u672c\u6846\u200b\u7684\u200b\u6587\u5b57\u200b\uff0c\u200b\u5f53\u5176\u200b\u5185\u5bb9\u200b\u4e3a\u200b\u201c###\u201d\u200b\u65f6\u200b\uff0c\u200b\u8868\u793a\u200b\u8be5\u200b\u6587\u672c\u6846\u200b\u65e0\u6548\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u4f1a\u200b\u8df3\u8fc7\u200b\u3002</p> <p>\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b \u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u7684\u200b\u683c\u5f0f\u200b\u5982\u4e0b\u200b\uff0c txt\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u9ed8\u8ba4\u200b\u8bf7\u200b\u5c06\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\u548c\u200b\u56fe\u7247\u200b\u6807\u7b7e\u200b\u7528\u200b'\\t'\u200b\u5206\u5272\u200b\uff0c\u200b\u5982\u7528\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u5206\u5272\u200b\u5c06\u200b\u9020\u6210\u200b\u8bad\u7ec3\u200b\u62a5\u9519\u200b\u3002</p> <pre><code>\" \u200b\u56fe\u50cf\u200b\u6587\u4ef6\u540d\u200b                 \u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u4fe1\u606f\u200b \"\n\ntrain_data/rec/train/word_001.jpg   \u200b\u7b80\u5355\u200b\u53ef\u200b\u4f9d\u8d56\u200b\ntrain_data/rec/train/word_002.jpg   \u200b\u7528\u200b\u79d1\u6280\u200b\u8ba9\u200b\u590d\u6742\u200b\u7684\u200b\u4e16\u754c\u200b\u66f4\u200b\u7b80\u5355\u200b\n...\n</code></pre> <pre><code>unzip -q /home/aistudio/data/data140302/XFUND_ori.zip -d /home/aistudio/data/data140302/\n</code></pre> <p>\u200b\u5df2\u7ecf\u200b\u63d0\u4f9b\u200b\u8f6c\u6362\u200b\u811a\u672c\u200b\uff0c\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\uff1a</p> <pre><code>%cd /home/aistudio/\npython trans_xfund_data.py\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#4-ocr","title":"4 OCR","text":"<p>\u200b\u9009\u7528\u200b\u98de\u6868\u200bOCR\u200b\u5f00\u53d1\u200b\u5957\u4ef6\u200bPaddleOCR\u200b\u4e2d\u200b\u7684\u200bPP-OCRv2\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u3002PP-OCRv2\u200b\u5728\u200bPP-OCR\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u5728\u200b5\u200b\u4e2a\u200b\u65b9\u9762\u200b\u91cd\u70b9\u200b\u4f18\u5316\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u91c7\u7528\u200bCML\u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u77e5\u8bc6\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u548c\u200bCopyPaste\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff1b\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u91c7\u7528\u200bLCNet\u200b\u8f7b\u91cf\u7ea7\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u3001UDML \u200b\u6539\u8fdb\u200b\u77e5\u8bc6\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u548c\u200bEnhanced CTC loss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6539\u8fdb\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u5728\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u548c\u200b\u9884\u6d4b\u200b\u6548\u679c\u200b\u4e0a\u200b\u53d6\u5f97\u200b\u660e\u663e\u200b\u63d0\u5347\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv2\u200b\u6280\u672f\u200b\u62a5\u544a\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#41","title":"4.1 \u200b\u6587\u672c\u200b\u68c0\u6d4b","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b2\u200b\u79cd\u200b\u65b9\u6848\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\uff1a</p> <ul> <li>PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</li> <li>XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune</li> </ul>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#411-1","title":"4.1.1 \u200b\u65b9\u6848\u200b1\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":""},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1_1","title":"1\uff09\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>PaddleOCR\u200b\u5df2\u7ecf\u200b\u63d0\u4f9b\u200b\u4e86\u200bPP-OCR\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\uff0c\u200b\u90e8\u5206\u200b\u6a21\u578b\u200b\u5c55\u793a\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u63a8\u8350\u200b\u573a\u666f\u200b \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b \u200b\u65b9\u5411\u200b\u5206\u7c7b\u5668\u200b \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCRv2\u200b\u6a21\u578b\u200b\uff0813.0M\uff09 ch_PP-OCRv2_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200bPP-OCR mobile\u200b\u6a21\u578b\u200b\uff089.4M\uff09 ch_ppocr_mobile_v2.0_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u901a\u7528\u200bPP-OCR server\u200b\u6a21\u578b\u200b\uff08143.4M\uff09 ch_ppocr_server_v2.0_xx \u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u66f4\u200b\u591a\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5305\u62ec\u200b\u591a\u200b\u8bed\u8a00\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPP-OCR \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</p> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR/pretrain/\nwget https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar\ntar -xf ch_PP-OCRv2_det_distill_train.tar &amp;&amp; rm -rf ch_PP-OCRv2_det_distill_train.tar\n% cd ..\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2_1","title":"2\uff09\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u63a5\u7740\u200b\u4f7f\u7528\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5728\u200bXFUND\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u7531\u4e8e\u200b\u84b8\u998f\u200b\u9700\u8981\u200b\u5305\u542b\u200b\u591a\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u751a\u81f3\u200b\u591a\u4e2a\u200bStudent\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200b\u6307\u6807\u200b\u7684\u200b\u65f6\u5019\u200b\u53ea\u200b\u9700\u8981\u200b\u8ba1\u7b97\u200b\u4e00\u4e2a\u200bStudent\u200b\u7f51\u7edc\u200b\u7684\u200b\u6307\u6807\u200b\u5373\u53ef\u200b\uff0ckey\u200b\u5b57\u200b\u6bb5\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bStudent\u200b\u5219\u200b\u8868\u793a\u200b\u53ea\u200b\u8ba1\u7b97\u200bStudent\u200b\u7f51\u7edc\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002</p> <pre><code>Metric:\n  name: DistillationMetric\n  base_metric_name: DetMetric\n  main_indicator: hmean\n  key: \"Student\"\n</code></pre> <p>\u200b\u9996\u5148\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_distill.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5728\u200bXFUND\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5177\u4f53\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR\npython tools/eval.py \\\n    -c configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_distill.yml \\\n    -o Global.checkpoints=\"./pretrain_models/ch_PP-OCRv2_det_distill_train/best_accuracy\"\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 77.26% <p>\u200b\u4f7f\u7528\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200bXFUND\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u8fbe\u5230\u200b77%\u200b\u5de6\u53f3\u200b\uff0c\u200b\u5145\u5206\u8bf4\u660e\u200bppocr\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#412-2xfundfine-tune","title":"4.1.2 \u200b\u65b9\u6848\u200b2\uff1aXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune","text":"<p>PaddleOCR\u200b\u63d0\u4f9b\u200b\u7684\u200b\u84b8\u998f\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5305\u542b\u200b\u4e86\u200b\u591a\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u53d6\u200bStudent\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>import paddle\n# \u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nall_params = paddle.load(\"pretrain/ch_PP-OCRv2_det_distill_train/best_accuracy.pdparams\")\n# \u200b\u67e5\u770b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\n# print(all_params.keys())\n# \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u63d0\u53d6\u200b\ns_params = {key[len(\"student_model.\"):]: all_params[key] for key in all_params if \"student_model.\" in key}\n# \u200b\u67e5\u770b\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(s_params.keys())\n# \u200b\u4fdd\u5b58\u200b\npaddle.save(s_params, \"pretrain/ch_PP-OCRv2_det_distill_train/student.pdparams\")\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1_2","title":"1)\u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv2_det_student.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Global.pretrained_model\uff1a\u200b\u6307\u5411\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\nTrain.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nOptimizer.lr.learning_rate\uff1a\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u672c\u200b\u5b9e\u9a8c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0.005\nTrain.dataset.transforms.EastRandomCropData.size\uff1a\u200b\u8bad\u7ec3\u200b\u5c3a\u5bf8\u200b\u6539\u4e3a\u200b[1600, 1600]\nEval.dataset.transforms.DetResizeForTest\uff1a\u200b\u8bc4\u4f30\u200b\u5c3a\u5bf8\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u5982\u4e0b\u200b\u53c2\u6570\u200b\n       limit_side_len: 1600\n       limit_type: 'min'\n</code></pre> <p>\u200b\u6267\u884c\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python tools/train.py \\\n        -c configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_student.yml\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2_2","title":"2\uff09\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>Global.checkpoints</code>\u3002</p> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b</p> <pre><code>%cd /home/aistudio/PaddleOCR/\npython tools/eval.py \\\n    -c configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_student.yml \\\n    -o Global.checkpoints=\"pretrain/ch_db_mv3-student1600-finetune/best_accuracy\"\n</code></pre> <p>\u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u672a\u200bfinetuen\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u53c2\u6570\u200b(<code>pretrained_model</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u7a7a\u200b\uff0c<code>learning_rate</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0.001)</p> <pre><code>%cd /home/aistudio/PaddleOCR/\npython tools/eval.py \\\n    -c configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_student.yml \\\n    -o Global.checkpoints=\"pretrain/ch_db_mv3-student1600/best_accuracy\"\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans XFUND\u200b\u6570\u636e\u200b\u96c6\u200b 79.27% XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune 85.24% <p>\u200b\u5bf9\u6bd4\u200b\u4ec5\u200b\u4f7f\u7528\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f7f\u7528\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u8fbe\u5230\u200b85%\u200b\u5de6\u53f3\u200b\uff0c\u200b\u8bf4\u660e\u200b\u00a0finetune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#3_1","title":"3\uff09\u200b\u5bfc\u51fa\u200b\u6a21\u578b","text":"<p>\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u662f\u200b\u5305\u542b\u200b\u524d\u5411\u200b\u9884\u6d4b\u200b\u548c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u7684\u200b\u5de5\u4e1a\u200b\u90e8\u7f72\u200b\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u5bfc\u6210\u200b\u90e8\u7f72\u200b\u9700\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\u683c\u5f0f\u200b\u3002 \u200b\u6267\u884c\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5373\u53ef\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u3002</p> <pre><code># \u200b\u52a0\u8f7d\u200b\u914d\u7f6e\u6587\u4ef6\u200b`ch_PP-OCRv2_det_student.yml`\uff0c\u200b\u4ece\u200b`pretrain/ch_db_mv3-student1600-finetune`\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u52a0\u8f7d\u200b`best_accuracy`\u200b\u6a21\u578b\u200b\n# inference\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u5728\u200b`./output/det_db_inference`\u200b\u76ee\u5f55\u200b\u4e0b\u200b\n%cd /home/aistudio/PaddleOCR/\npython tools/export_model.py \\\n    -c configs/det/ch_PP-OCRv2/ch_PP-OCRv2_det_student.yml \\\n    -o Global.pretrained_model=\"pretrain/ch_db_mv3-student1600-finetune/best_accuracy\" \\\n    Global.save_inference_dir=\"./output/det_db_inference/\"\n</code></pre> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>/inference/rec_crnn/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#4","title":"4\uff09\u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u52a0\u8f7d\u200b\u4e0a\u9762\u200b\u5bfc\u51fa\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5bf9\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6216\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code>det_model_dir\uff1a\u200b\u9884\u6d4b\u200b\u6a21\u578b\u200b\nimage_dir\uff1a\u200b\u6d4b\u8bd5\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\nuse_gpu\uff1a\u200b\u662f\u5426\u200b\u4f7f\u7528\u200bGPU\n</code></pre> <p>\u200b\u68c0\u6d4b\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>/home/aistudio/inference_results/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u67e5\u770b\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002</p> <pre><code>%pwd\n!python tools/infer/predict_det.py \\\n    --det_algorithm=\"DB\" \\\n    --det_model_dir=\"./output/det_db_inference/\" \\\n    --image_dir=\"./doc/vqa/input/zh_val_21.jpg\" \\\n    --use_gpu=True\n</code></pre> <p>\u200b\u603b\u7ed3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3001XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune2\u200b\u79cd\u200b\u65b9\u6848\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b49\u200b\uff0c\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans \u200b\u7ed3\u679c\u200b\u5206\u6790\u200b PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 77.26% ppocr\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b XFUND\u200b\u6570\u636e\u200b\u96c6\u200b 79.27% XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune 85.24% finetune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#42","title":"4.2 \u200b\u6587\u672c\u200b\u8bc6\u522b","text":"<p>\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b3\u200b\u79cd\u200b\u65b9\u6848\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\uff1a</p> <ul> <li>PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</li> <li>XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune</li> <li>XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune+\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b</li> </ul>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#421-1","title":"4.2.1 \u200b\u65b9\u6848\u200b1\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":""},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1_3","title":"1\uff09\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR/pretrain/\nwget https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar\ntar -xf ch_PP-OCRv2_rec_train.tar &amp;&amp; rm -rf ch_PP-OCRv2_rec_train.tar\n% cd ..\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2_3","title":"2\uff09\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u9996\u5148\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/det/ch_PP-OCRv2/ch_PP-OCRv2_rec_distillation.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR\nCUDA_VISIBLE_DEVICES=0 python tools/eval.py \\\n    -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec_distillation.yml \\\n    -o Global.checkpoints=./pretrain/ch_PP-OCRv2_rec_train/best_accuracy\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 67.48% <p>\u200b\u4f7f\u7528\u200b\u6587\u672c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200bXFUND\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\uff0cacc\u200b\u8fbe\u5230\u200b67%\u200b\u5de6\u53f3\u200b\uff0c\u200b\u5145\u5206\u8bf4\u660e\u200bppocr\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#422-2xfundfinetune","title":"4.2.2 \u200b\u65b9\u6848\u200b2\uff1aXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune","text":"<p>\u200b\u540c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u53d6\u200bStudent\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200bfinetune\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>import paddle\n# \u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nall_params = paddle.load(\"pretrain/ch_PP-OCRv2_rec_train/best_accuracy.pdparams\")\n# \u200b\u67e5\u770b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(all_params.keys())\n# \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u63d0\u53d6\u200b\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# \u200b\u67e5\u770b\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(s_params.keys())\n# \u200b\u4fdd\u5b58\u200b\npaddle.save(s_params, \"pretrain/ch_PP-OCRv2_rec_train/student.pdparams\")\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1_4","title":"1)\u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Global.pretrained_model\uff1a\u200b\u6307\u5411\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\nGlobal.character_dict_path: \u200b\u5b57\u5178\u200b\u8def\u5f84\u200b\nOptimizer.lr.values\uff1a\u200b\u5b66\u4e60\u200b\u7387\u200b\nTrain.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR/\nCUDA_VISIBLE_DEVICES=0 python tools/train.py \\\n        -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2_4","title":"2\uff09\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>Global.checkpoints</code>\uff0c\u200b\u8fd9\u91cc\u200b\u4e3a\u200b\u5927\u5bb6\u200b\u63d0\u4f9b\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b<code>./pretrain/rec_mobile_pp-OCRv2-student-finetune/best_accuracy</code></p> <pre><code>%cd /home/aistudio/PaddleOCR/\nCUDA_VISIBLE_DEVICES=0 python tools/eval.py \\\n    -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml \\\n    -o Global.checkpoints=./pretrain/rec_mobile_pp-OCRv2-student-finetune/best_accuracy\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune 72.33% <p>\u200b\u4f7f\u7528\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u8fbe\u5230\u200b72%\u200b\u5de6\u53f3\u200b\uff0c\u200b\u8bf4\u660e\u200b finetune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#423-3xfundfinetune","title":"4.2.3 \u200b\u65b9\u6848\u200b3\uff1aXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune+\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e","text":"<p>\u200b\u63a5\u7740\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u8ff0\u200b<code>XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune</code>\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\u3002\u200b\u9996\u5148\u200b\u51c6\u5907\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u4e0a\u200b\u4f20\u5230\u200bAIStudio\uff1a</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#1_5","title":"1)\u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u5728\u200b\u4e0a\u8ff0\u200b<code>XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune</code>\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml</code>\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u7ee7\u7eed\u200b\u4fee\u6539\u200b\u4ee5\u4e0b\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Train.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u771f\u5b9e\u200b\u8bc6\u522b\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.ratio_list\uff1a\u200b\u52a8\u6001\u200b\u91c7\u6837\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>%cd /home/aistudio/PaddleOCR/\nCUDA_VISIBLE_DEVICES=0 python tools/train.py \\\n        -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#2_5","title":"2\uff09\u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>Global.checkpoints</code>\u3002</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python tools/eval.py \\\n    -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml \\\n    -o Global.checkpoints=./pretrain/rec_mobile_pp-OCRv2-student-realdata/best_accuracy\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune+\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b 85.29% <p>\u200b\u4f7f\u7528\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u8fbe\u5230\u200b85%\u200b\u5de6\u53f3\u200b\uff0c\u200b\u8bf4\u660e\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u5bf9\u4e8e\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#3_2","title":"3)\u200b\u5bfc\u51fa\u200b\u6a21\u578b","text":"<p>\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u53ea\u200b\u4fdd\u7559\u200b\u524d\u5411\u200b\u9884\u6d4b\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff1a</p> <pre><code>!python tools/export_model.py \\\n    -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml \\\n    -o Global.pretrained_model=pretrain/rec_mobile_pp-OCRv2-student-realdata/best_accuracy  \\\n    Global.save_inference_dir=./output/rec_crnn_inference/\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#4_1","title":"4)\u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u52a0\u8f7d\u200b\u4e0a\u9762\u200b\u5bfc\u51fa\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5bf9\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6216\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>/home/aistudio/inference_results/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u67e5\u770b\u200b\u68c0\u6d4b\u200b\u3001\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\u3002\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b<code>--rec_char_dict_path</code>\u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200b\u5b57\u5178\u200b\u8def\u5f84\u200b</p> <pre><code>python tools/infer/predict_system.py \\\n    --image_dir=\"./doc/vqa/input/zh_val_21.jpg\" \\\n    --det_model_dir=\"./output/det_db_inference/\" \\\n    --rec_model_dir=\"./output/rec_crnn_inference/\" \\\n    --rec_image_shape=\"3, 32, 320\" \\\n    --rec_char_dict_path=\"/home/aistudio/XFUND/word_dict.txt\"\n</code></pre> <p>\u200b\u603b\u7ed3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3001XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+finetune2\u200b\u79cd\u200b\u65b9\u6848\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b49\u200b\uff0c\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc \u200b\u7ed3\u679c\u200b\u5206\u6790\u200b PP-OCRv2\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 67.48% ppocr\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune 72.33% finetune\u200b\u4f1a\u200b\u63d0\u5347\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u6548\u679c\u200b XFUND\u200b\u6570\u636e\u200b\u96c6\u200b+fine-tune+\u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b 85.29% \u200b\u771f\u5b9e\u200b\u901a\u7528\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u5bf9\u4e8e\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#5-doc-vqa","title":"5 \u200b\u6587\u6863\u200b\u89c6\u89c9\u200b\u95ee\u7b54\u200b(DOC-VQA)","text":"<p>VQA\u200b\u6307\u200b\u89c6\u89c9\u200b\u95ee\u7b54\u200b\uff0c\u200b\u4e3b\u8981\u200b\u9488\u5bf9\u200b\u56fe\u50cf\u200b\u5185\u5bb9\u200b\u8fdb\u884c\u200b\u63d0\u95ee\u200b\u548c\u200b\u56de\u7b54\u200b,DOC-VQA\u200b\u662f\u200bVQA\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u7684\u200b\u4e00\u79cd\u200b\uff0cDOC-VQA\u200b\u4e3b\u8981\u200b\u9488\u5bf9\u200b\u6587\u672c\u200b\u56fe\u50cf\u200b\u7684\u200b\u6587\u5b57\u200b\u5185\u5bb9\u200b\u63d0\u51fa\u200b\u95ee\u9898\u200b\u3002</p> <p>PaddleOCR\u200b\u4e2d\u200bDOC-VQA\u200b\u7cfb\u5217\u200b\u7b97\u6cd5\u200b\u57fa\u4e8e\u200bPaddleNLP\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u7b97\u6cd5\u200b\u5e93\u200b\u5b9e\u73b0\u200bLayoutXLM\u200b\u8bba\u6587\u200b\uff0c\u200b\u652f\u6301\u200b\u57fa\u4e8e\u200b\u591a\u200b\u6a21\u6001\u200b\u65b9\u6cd5\u200b\u7684\u200b \u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b (Semantic Entity Recognition, SER) \u200b\u4ee5\u53ca\u200b \u200b\u5173\u7cfb\u200b\u62bd\u53d6\u200b (Relation Extraction, RE)    \u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u76f4\u63a5\u200b\u4f53\u9a8c\u200b\u9884\u6d4b\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8df3\u8fc7\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b\u5373\u53ef\u200b\u3002</p> <pre><code>%cd pretrain\n#\u200b\u4e0b\u8f7d\u200bSER\u200b\u6a21\u578b\u200b\nwget https://paddleocr.bj.bcebos.com/pplayout/ser_LayoutXLM_xfun_zh.tar &amp;&amp; tar -xvf ser_LayoutXLM_xfun_zh.tar\n#\u200b\u4e0b\u8f7d\u200bRE\u200b\u6a21\u578b\u200b\nwget https://paddleocr.bj.bcebos.com/pplayout/re_LayoutXLM_xfun_zh.tar &amp;&amp; tar -xvf re_LayoutXLM_xfun_zh.tar\n%cd ../\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#51-ser","title":"5.1 SER","text":"<p>SER: \u200b\u8bed\u4e49\u200b\u5b9e\u4f53\u200b\u8bc6\u522b\u200b (Semantic Entity Recognition\uff09, \u200b\u53ef\u4ee5\u200b\u5b8c\u6210\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4e0e\u200b\u5206\u7c7b\u200b\u3002</p> <p></p> <p>\u200b\u56fe\u200b19 \u200b\u4e2d\u200b\u4e0d\u540c\u200b\u989c\u8272\u200b\u7684\u200b\u6846\u200b\u8868\u793a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5bf9\u4e8e\u200bXFUND\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6709\u200bQUESTION, ANSWER, HEADER 3\u200b\u79cd\u7c7b\u200b\u522b\u200b</p> <ul> <li>\u200b\u6df1\u7d2b\u8272\u200b\uff1aHEADER</li> <li>\u200b\u6d45\u7d2b\u8272\u200b\uff1aQUESTION</li> <li>\u200b\u519b\u7eff\u8272\u200b\uff1aANSWER</li> </ul> <p>\u200b\u5728\u200bOCR\u200b\u68c0\u6d4b\u200b\u6846\u200b\u7684\u200b\u5de6\u4e0a\u65b9\u200b\u4e5f\u200b\u6807\u51fa\u200b\u4e86\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7c7b\u522b\u200b\u548c\u200bOCR\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#511","title":"5.1.1 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b <code>configs/vqa/ser/layoutxlm.yml</code>  \u200b\u4ee5\u4e0b\u200b\u56db\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\uff1a</p> <pre><code>Train.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u6307\u200b\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <pre><code>%cd /home/aistudio/PaddleOCR/\nCUDA_VISIBLE_DEVICES=0 python tools/train.py -c configs/vqa/ser/layoutxlm.yml\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u6253\u5370\u200b\u51fa\u200b<code>precision</code>, <code>recall</code>, <code>hmean</code>\u200b\u7b49\u200b\u6307\u6807\u200b\u3002 \u200b\u5728\u200b<code>./output/ser_layoutxlm/</code>\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\uff0c\u200b\u6700\u4f18\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6700\u65b0\u200bepoch\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#512","title":"5.1.2 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5c06\u200b\u5f85\u200b\u8bc4\u4f30\u200b\u7684\u200b\u6a21\u578b\u200b\u6240\u5728\u200b\u6587\u4ef6\u5939\u200b\u8def\u5f84\u200b\u8d4b\u503c\u200b\u7ed9\u200b <code>Architecture.Backbone.checkpoints</code> \u200b\u5b57\u200b\u6bb5\u200b\u5373\u53ef\u200b\u3002</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python tools/eval.py \\\n    -c configs/vqa/ser/layoutxlm.yml \\\n    -o Architecture.Backbone.checkpoints=pretrain/ser_LayoutXLM_xfun_zh/\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u6253\u5370\u200b\u51fa\u200b<code>precision</code>, <code>recall</code>, <code>hmean</code>\u200b\u7b49\u200b\u6307\u6807\u200b\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#513","title":"5.1.3 \u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b<code>OCR\u200b\u5f15\u64ce\u200b + SER</code>\u200b\u7684\u200b\u4e32\u8054\u200b\u9884\u6d4b\u200b, \u200b\u4ee5\u200bSER\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python tools/infer_vqa_token_ser.py \\\n    -c configs/vqa/ser/layoutxlm.yml  \\\n    -o Architecture.Backbone.checkpoints=pretrain/ser_LayoutXLM_xfun_zh/ \\\n    Global.infer_img=doc/vqa/input/zh_val_42.jpg\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u5728\u200b<code>config.Global.save_res_path</code>\u200b\u5b57\u6bb5\u200b\u6240\u200b\u914d\u7f6e\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u4fdd\u5b58\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\u4ee5\u53ca\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\u540d\u4e3a\u200b<code>infer_results.txt</code>\u3002\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u67e5\u770b\u200b\u9884\u6d4b\u200b\u56fe\u7247\u200b\uff1a</p> <pre><code>import cv2\nfrom matplotlib import pyplot as plt\n# \u200b\u5728\u200bnotebook\u200b\u4e2d\u200b\u4f7f\u7528\u200bmatplotlib.pyplot\u200b\u7ed8\u56fe\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u6dfb\u52a0\u200b\u8be5\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u663e\u793a\u200b\n%matplotlib inline\n\nimg = cv2.imread('output/ser/zh_val_42_ser.jpg')\nplt.figure(figsize=(48,24))\nplt.imshow(img)\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#52-re","title":"5.2 RE","text":"<p>\u200b\u57fa\u4e8e\u200b RE \u200b\u4efb\u52a1\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5b8c\u6210\u200b\u5bf9\u200b\u56fe\u8c61\u200b\u4e2d\u200b\u7684\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\u7684\u200b\u5173\u7cfb\u200b\u63d0\u53d6\u200b\uff0c\u200b\u5982\u200b\u5224\u65ad\u200b\u95ee\u9898\u200b\u5bf9\u200b(pair)\u3002</p> <p></p> <p>\u200b\u56fe\u4e2d\u200b\u7ea2\u8272\u200b\u6846\u200b\u8868\u793a\u200b\u95ee\u9898\u200b\uff0c\u200b\u84dd\u8272\u200b\u6846\u200b\u8868\u793a\u200b\u7b54\u6848\u200b\uff0c\u200b\u95ee\u9898\u200b\u548c\u200b\u7b54\u6848\u200b\u4e4b\u95f4\u200b\u4f7f\u7528\u200b\u7eff\u8272\u200b\u7ebf\u200b\u8fde\u63a5\u200b\u3002\u200b\u5728\u200bOCR\u200b\u68c0\u6d4b\u200b\u6846\u200b\u7684\u200b\u5de6\u4e0a\u65b9\u200b\u4e5f\u200b\u6807\u51fa\u200b\u4e86\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7c7b\u522b\u200b\u548c\u200bOCR\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u3002</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#521","title":"5.2.1 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/vqa/re/layoutxlm.yml</code>\u200b\u4e2d\u200b\u7684\u200b\u4ee5\u4e0b\u200b\u56db\u4e2a\u200b\u5b57\u200b\u6bb5\u200b</p> <pre><code>Train.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nTrain.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\nEval.dataset.data_dir\uff1a\u200b\u6307\u6307\u200b\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b\nEval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\n</code></pre> <pre><code>CUDA_VISIBLE_DEVICES=0 python3 tools/train.py -c configs/vqa/re/layoutxlm.yml\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u6253\u5370\u200b\u51fa\u200b<code>precision</code>, <code>recall</code>, <code>hmean</code>\u200b\u7b49\u200b\u6307\u6807\u200b\u3002 \u200b\u5728\u200b<code>./output/re_layoutxlm/</code>\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\uff0c\u200b\u6700\u4f18\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6700\u65b0\u200bepoch\u200b\u7684\u200b\u6a21\u578b\u200b</p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#522","title":"5.2.2 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u5c06\u200b\u5f85\u200b\u8bc4\u4f30\u200b\u7684\u200b\u6a21\u578b\u200b\u6240\u5728\u200b\u6587\u4ef6\u5939\u200b\u8def\u5f84\u200b\u8d4b\u503c\u200b\u7ed9\u200b <code>Architecture.Backbone.checkpoints</code> \u200b\u5b57\u200b\u6bb5\u200b\u5373\u53ef\u200b\u3002</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python3 tools/eval.py \\\n    -c configs/vqa/re/layoutxlm.yml \\\n    -o Architecture.Backbone.checkpoints=pretrain/re_LayoutXLM_xfun_zh/\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u6253\u5370\u200b\u51fa\u200b<code>precision</code>, <code>recall</code>, <code>hmean</code>\u200b\u7b49\u200b\u6307\u6807\u200b\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#523","title":"5.2.3 \u200b\u6a21\u578b\u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200bOCR\u200b\u5f15\u64ce\u200b + SER + RE\u200b\u7684\u200b\u4e32\u8054\u200b\u9884\u6d4b\u200b, \u200b\u4ee5\u9884\u200b\u8bad\u7ec3\u200bSER\u200b\u548c\u200bRE\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\uff0c</p> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u5728\u200bconfig.Global.save_res_path\u200b\u5b57\u6bb5\u200b\u6240\u200b\u914d\u7f6e\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u4fdd\u5b58\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\u4ee5\u53ca\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\u540d\u4e3a\u200binfer_results.txt\u3002</p> <pre><code>cd /home/aistudio/PaddleOCR\nCUDA_VISIBLE_DEVICES=0 python3 tools/infer_vqa_token_ser_re.py \\\n    -c configs/vqa/re/layoutxlm.yml \\\n    -o Architecture.Backbone.checkpoints=pretrain/re_LayoutXLM_xfun_zh/ \\\n    Global.infer_img=test_imgs/ \\\n    -c_ser configs/vqa/ser/layoutxlm.yml \\\n    -o_ser Architecture.Backbone.checkpoints=pretrain/ser_LayoutXLM_xfun_zh/\n</code></pre> <p>\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u5728\u200bconfig.Global.save_res_path\u200b\u5b57\u6bb5\u200b\u6240\u200b\u914d\u7f6e\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u4fdd\u5b58\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\u4ee5\u53ca\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6587\u672c\u6587\u4ef6\u200b\u540d\u4e3a\u200binfer_results.txt, \u200b\u6bcf\u200b\u4e00\u884c\u200b\u8868\u793a\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff0c\u200b\u524d\u9762\u200b\u8868\u793a\u200b\u6d4b\u8bd5\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\uff0c\u200b\u540e\u9762\u200b\u4e3a\u200b\u6d4b\u8bd5\u200b\u7ed3\u679c\u200b\uff1akey\u200b\u5b57\u6bb5\u200b\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200bvalue\u200b\u5b57\u200b\u6bb5\u200b\u3002</p> <pre><code>test_imgs/t131.jpg  {\"\u200b\u653f\u6cbb\u200b\u9762\u7a0e\u200b\": \"\u200b\u7fa4\u4f17\u200b\", \"\u200b\u6027\u522b\u200b\": \"\u200b\u7537\u200b\", \"\u200b\u7c4d\u8d2f\u200b\": \"\u200b\u6cb3\u5317\u7701\u200b\u90af\u90f8\u5e02\u200b\", \"\u200b\u5a5a\u59fb\u72b6\u51b5\u200b\": \"\u200b\u4e8f\u672b\u200b\u5a5a\u53e3\u200b\u5df2\u5a5a\u200b\u53e3\u200b\u5df2\u5a07\u200b\", \"\u200b\u901a\u8baf\u5730\u5740\u200b\": \"\u200b\u90af\u90f8\u5e02\u200b\u9633\u5149\u200b\u82d1\u200b7\u200b\u53f7\u697c\u200b003\", \"\u200b\u6c11\u65cf\u200b\": \"\u200b\u6c49\u65cf\u200b\", \"\u200b\u6bd5\u4e1a\u200b\u9662\u6821\u200b\": \"\u200b\u6cb3\u5357\u200b\u5de5\u4e1a\u200b\u5927\u5b66\u200b\", \"\u200b\u6237\u53e3\u200b\u6027\u8d28\u200b\": \"\u200b\u53e3\u200b\u519c\u6751\u200b\u57ce\u9547\u200b\", \"\u200b\u6237\u53e3\u200b\u5730\u5740\u200b\": \"\u200b\u6cb3\u5317\u7701\u200b\u90af\u90f8\u5e02\u200b\", \"\u200b\u8054\u7cfb\u7535\u8bdd\u200b\": \"13288888888\", \"\u200b\u5065\u5eb7\u72b6\u51b5\u200b\": \"\u200b\u5065\u5eb7\u200b\", \"\u200b\u59d3\u540d\u200b\": \"\u200b\u5c0f\u516d\u200b\", \"\u200b\u597d\u200b\u9ad8\u200bcm\": \"180\", \"\u200b\u51fa\u751f\u5e74\u6708\u200b\": \"1996\u200b\u5e74\u200b8\u200b\u6708\u200b9\u200b\u65e5\u200b\", \"\u200b\u6587\u5316\u200b\u7a0b\u5ea6\u200b\": \"\u200b\u672c\u79d1\u200b\", \"\u200b\u8eab\u4efd\u8bc1\u200b\u53f7\u7801\u200b\": \"458933777777777777\"}\n</code></pre> <p>\u200b\u5c55\u793a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b</p> <pre><code>import cv2\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimg = cv2.imread('./output/re/t131_ser.jpg')\nplt.figure(figsize=(48,24))\nplt.imshow(img)\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#6-excel","title":"6 \u200b\u5bfc\u51fa\u200bExcel","text":"<p>\u200b\u4e3a\u4e86\u200b\u8f93\u51fa\u200b\u4fe1\u606f\u200b\u5339\u914d\u200b\u5bf9\u200b\uff0c\u200b\u6211\u4eec\u200b\u4fee\u6539\u200b<code>tools/infer_vqa_token_ser_re.py</code>\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>line 194-197</code>\u3002</p> <pre><code> fout.write(img_path + \"\\t\" + json.dumps(\n                {\n                    \"ser_result\": result,\n                }, ensure_ascii=False) + \"\\n\")\n</code></pre> <p>\u200b\u66f4\u200b\u6539\u4e3a\u200b</p> <pre><code>result_key = {}\nfor ocr_info_head, ocr_info_tail in result:\n    result_key[ocr_info_head['text']] = ocr_info_tail['text']\n\nfout.write(img_path + \"\\t\" + json.dumps(\n    result_key, ensure_ascii=False) + \"\\n\")\n</code></pre> <p>\u200b\u540c\u65f6\u200b\u5c06\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5bfc\u51fa\u200b\u5230\u200bExcel\u200b\u4e2d\u200b\uff0c\u200b\u6548\u679c\u200b\u5982\u200b \u200b\u56fe\u200b28 \u200b\u6240\u793a\u200b\uff1a </p> <pre><code>import json\nimport xlsxwriter as xw\n\nworkbook = xw.Workbook('output/re/infer_results.xlsx')\nformat1 = workbook.add_format({\n    'align': 'center',\n    'valign': 'vcenter',\n    'text_wrap': True,\n})\nworksheet1 = workbook.add_worksheet('sheet1')\nworksheet1.activate()\ntitle = ['\u200b\u59d3\u540d\u200b', '\u200b\u6027\u522b\u200b', '\u200b\u6c11\u65cf\u200b', '\u200b\u6587\u5316\u200b\u7a0b\u5ea6\u200b', '\u200b\u8eab\u4efd\u8bc1\u200b\u53f7\u7801\u200b', '\u200b\u8054\u7cfb\u7535\u8bdd\u200b', '\u200b\u901a\u8baf\u5730\u5740\u200b']\nworksheet1.write_row('A1', title)\ni = 2\n\nwith open('output/re/infer_results.txt', 'r', encoding='utf-8') as fin:\n    lines = fin.readlines()\n    for line in lines:\n        img_path, result = line.strip().split('\\t')\n        result_key = json.loads(result)\n        # \u200b\u5199\u5165\u200bExcel\n        row_data = [result_key['\u200b\u59d3\u540d\u200b'], result_key['\u200b\u6027\u522b\u200b'], result_key['\u200b\u6c11\u65cf\u200b'], result_key['\u200b\u6587\u5316\u200b\u7a0b\u5ea6\u200b'], result_key['\u200b\u8eab\u4efd\u8bc1\u200b\u53f7\u7801\u200b'],\n                    result_key['\u200b\u8054\u7cfb\u7535\u8bdd\u200b'], result_key['\u200b\u901a\u8baf\u5730\u5740\u200b']]\n        row = 'A' + str(i)\n        worksheet1.write_row(row, row_data, format1)\n        i+=1\nworkbook.close()\n</code></pre>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#_2","title":"\u66f4\u200b\u591a\u200b\u8d44\u6e90","text":"<ul> <li>\u200b\u66f4\u200b\u591a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u77e5\u8bc6\u200b\u3001\u200b\u4ea7\u4e1a\u200b\u6848\u4f8b\u200b\u3001\u200b\u9762\u8bd5\u200b\u5b9d\u5178\u200b\u7b49\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aawesome-DeepLearning</li> <li>\u200b\u66f4\u200b\u591a\u200bPaddleOCR\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aPaddleOCR</li> <li>\u200b\u66f4\u200b\u591a\u200bPaddleNLP\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1aPaddleNLP</li> <li>\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u76f8\u5173\u200b\u8d44\u6599\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\uff1a\u200b\u98de\u6868\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e73\u53f0\u200b</li> </ul>"},{"location":"en/applications/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%8D%95%E8%AF%86%E5%88%AB.html#_3","title":"\u53c2\u8003\u200b\u94fe\u63a5","text":"<ul> <li>LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding, https://arxiv.org/pdf/2104.08836.pdf</li> <li>microsoft/unilm/layoutxlm, https://github.com/microsoft/unilm/tree/master/layoutxlm</li> <li>XFUND dataset, https://github.com/doc-analysis/XFUND</li> </ul>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html","title":"\u5feb\u901f\u200b\u6784\u5efa\u200b\u5361\u8bc1\u7c7b\u200bOCR","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#1","title":"1. \u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b\u5e94\u7528","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#11-ocr","title":"1.1 \u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u4e2d\u200b\u7684\u200bOCR\u200b\u76f8\u5173\u200b\u6280\u672f","text":"<p>\u300a\u201c\u200b\u5341\u56db\u4e94\u200b\u201d\u200b\u6570\u5b57\u200b\u7ecf\u6d4e\u200b\u53d1\u5c55\u200b\u89c4\u5212\u200b\u300b\u200b\u6307\u51fa\u200b\uff0c2020\u200b\u5e74\u200b\u6211\u56fd\u200b\u6570\u5b57\u200b\u7ecf\u6d4e\u200b\u6838\u5fc3\u200b\u4ea7\u4e1a\u200b\u589e\u52a0\u503c\u200b\u5360\u200bGDP\u200b\u6bd4\u91cd\u200b\u8fbe\u200b7.8\uff05\uff0c\u200b\u968f\u7740\u200b\u6570\u5b57\u200b\u7ecf\u6d4e\u200b\u8fc8\u5411\u200b\u5168\u9762\u200b\u6269\u5c55\u200b\uff0c\u200b\u5230\u200b2025\u200b\u5e74\u200b\u8be5\u200b\u6bd4\u4f8b\u200b\u5c06\u200b\u63d0\u5347\u200b\u81f3\u200b10\uff05\u3002</p> <p>\u200b\u5728\u200b\u8fc7\u53bb\u200b\u6570\u5e74\u200b\u7684\u200b\u8de8\u8d8a\u200b\u53d1\u5c55\u200b\u4e0e\u200b\u79ef\u7d2f\u200b\u6c89\u6dc0\u200b\u4e2d\u200b\uff0c\u200b\u6570\u5b57\u200b\u91d1\u878d\u200b\u3001\u200b\u91d1\u878d\u200b\u79d1\u6280\u200b\u5df2\u200b\u5728\u200b\u5bf9\u200b\u91d1\u878d\u4e1a\u200b\u7684\u200b\u91cd\u5851\u200b\u4e0e\u200b\u518d\u9020\u200b\u4e2d\u200b\u5145\u5206\u200b\u5370\u8bc1\u200b\u4e86\u200b\u5176\u200b\u81ea\u8eab\u200b\u4ef7\u503c\u200b\u3002</p> <p>\u200b\u4ee5\u200b\u667a\u80fd\u200b\u4e3a\u200b\u76ee\u6807\u200b\uff0c\u200b\u63d0\u5347\u200b\u91d1\u878d\u200b\u6570\u5b57\u5316\u200b\u6c34\u5e73\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4e1a\u52a1\u6d41\u7a0b\u200b\u81ea\u52a8\u5316\u200b\uff0c\u200b\u964d\u4f4e\u200b\u4eba\u529b\u200b\u6210\u672c\u200b\u3002</p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#12","title":"1.2 \u200b\u91d1\u878d\u200b\u884c\u4e1a\u200b\u4e2d\u200b\u7684\u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b\u573a\u666f\u200b\u4ecb\u7ecd","text":"<p>\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u8eab\u4efd\u8bc1\u200b\u3001\u200b\u94f6\u884c\u5361\u200b\u3001\u200b\u8425\u4e1a\u6267\u7167\u200b\u3001\u200b\u9a7e\u9a76\u8bc1\u200b\u7b49\u200b\u3002</p> <p>\u200b\u5e94\u7528\u200b\u96be\u70b9\u200b\uff1a\u200b\u7531\u4e8e\u200b\u6570\u636e\u200b\u7684\u200b\u91c7\u96c6\u200b\u6765\u6e90\u200b\u591a\u6837\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5b9e\u9645\u200b\u91c7\u96c6\u200b\u6570\u636e\u200b\u5404\u79cd\u200b\u566a\u58f0\u200b\uff1a\u200b\u53cd\u5149\u200b\u3001\u200b\u8936\u76b1\u200b\u3001\u200b\u6a21\u7cca\u200b\u3001\u200b\u503e\u659c\u200b\u7b49\u200b\u5404\u79cd\u200b\u95ee\u9898\u200b\u5e72\u6270\u200b\u3002</p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#13-ocr","title":"1.3 OCR\u200b\u843d\u5730\u200b\u6311\u6218","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#2","title":"2. \u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b\u6280\u672f\u200b\u89e3\u6790","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#21","title":"2.1 \u200b\u5361\u8bc1\u200b\u5206\u7c7b\u200b\u6a21\u578b","text":"<p>\u200b\u5361\u8bc1\u200b\u5206\u7c7b\u200b\uff1a\u200b\u57fa\u4e8e\u200bPPLCNet</p> <p>\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u8f7b\u91cf\u7ea7\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\u5728\u200bCPU\u200b\u73af\u5883\u200b\u4e0b\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b</p> <p></p> <p></p> <p>\u200b\u6a21\u578b\u200b\u6765\u81ea\u200b\u6a21\u578b\u5e93\u200bPaddleClas\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u548c\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u7684\u200b\u5de5\u5177\u96c6\u200b\uff0c\u200b\u52a9\u529b\u200b\u4f7f\u7528\u8005\u200b\u8bad\u7ec3\u200b\u51fa\u200b\u66f4\u597d\u200b\u7684\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u548c\u200b\u5e94\u7528\u200b\u843d\u5730\u200b\u3002</p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#22","title":"2.2 \u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":"<p>\u200b\u68c0\u6d4b\u200b\uff1aDBNet  \u200b\u8bc6\u522b\u200b\uff1aSVRT</p> <p></p> <p>PPOCRv3\u200b\u5728\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u3001\u200b\u8bc6\u522b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u6539\u8fdb\u200b\u4f18\u5316\u200b\uff0c\u200b\u5728\u200b\u4fdd\u8bc1\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u540c\u65f6\u200b\u63d0\u5347\u200b\u9884\u6d4b\u200b\u6548\u7387\u200b</p> <p></p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#3-ocr","title":"3. OCR\u200b\u6280\u672f\u200b\u62c6\u89e3","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#31","title":"3.1\u200b\u6280\u672f\u200b\u6d41\u7a0b","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#32-ocr-","title":"3.2 OCR\u200b\u6280\u672f\u200b\u62c6\u89e3\u200b---\u200b\u5361\u8bc1\u200b\u5206\u7c7b","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#_1","title":"\u5361\u8bc1\u200b\u5206\u7c7b\u200b\uff1a\u200b\u6570\u636e\u200b\u3001\u200b\u6a21\u578b\u200b\u51c6\u5907","text":"<p>A  \u200b\u4f7f\u7528\u200b\u722c\u866b\u200b\u83b7\u53d6\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\uff0c\u200b\u5c06\u200b\u76f8\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u653e\u5728\u200b\u540c\u4e00\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\uff0c\u200b\u6587\u4ef6\u540d\u200b\u4ece\u200b0\u200b\u5f00\u59cb\u200b\u547d\u540d\u200b\u3002\u200b\u5177\u4f53\u200b\u683c\u5f0f\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\u3002</p> <p>\u200b\u200b\u6ce8\u200b\uff1a\u200b\u5361\u8bc1\u7c7b\u200b\u6570\u636e\u200b\uff0c\u200b\u5efa\u8bae\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u6570\u636e\u91cf\u200b\u5728\u200b500\u200b\u5f20\u200b\u4ee5\u4e0a\u200b</p> <p></p> <p>B \u200b\u4e00\u884c\u200b\u547d\u4ee4\u200b\u751f\u6210\u200b\u6807\u7b7e\u200b\u6587\u4ef6\u200b</p> <pre><code>tree -r -i -f | grep -E \"jpg|JPG|jpeg|JPEG|png|PNG|webp\" | awk -F \"/\" '{print $0\" \"$2}' &gt; train_list.txt\n</code></pre> <p>C \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#-","title":"\u5361\u8bc1\u200b\u5206\u7c7b\u200b---\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u4e09\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ul> <li>\u200b\u5168\u5c40\u200b\u53c2\u6570\u200b\uff1a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b/\u200b\u8bad\u7ec3\u200b\u8f6e\u6b21\u200b/\u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b</li> <li>\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff1a\u200b\u5206\u7c7b\u200b\u6570\u200b</li> <li>\u200b\u6570\u636e\u5904\u7406\u200b\uff1a\u200b\u8bad\u7ec3\u200b/\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u8def\u5f84\u200b</li> </ul> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#-_1","title":"\u5361\u8bc1\u200b\u5206\u7c7b\u200b---\u200b\u8bad\u7ec3","text":"<p>\u200b\u6307\u5b9a\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>!python /home/aistudio/work/PaddleClas/tools/train.py -c   /home/aistudio/work/PaddleClas/ppcls/configs/PULC/text_image_orientation/PPLCNet_x1_0.yaml\n</code></pre> <p></p> <p>\u200b\u200b\u6ce8\u200b\uff1a\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u663e\u793a\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u548c\u200b\u8bc4\u4f30\u200b\u7ed3\u679c\u200b\uff08\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u56fa\u5b9a\u200b\u8f6e\u6570\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff09</p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#32-ocr-_1","title":"3.2 OCR\u200b\u6280\u672f\u200b\u62c6\u89e3\u200b---\u200b\u5361\u8bc1\u200b\u8bc6\u522b","text":"<p>\u200b\u5361\u8bc1\u200b\u8bc6\u522b\u200b\uff08\u200b\u4ee5\u200b\u8eab\u4efd\u8bc1\u200b\u68c0\u6d4b\u200b\u4e3a\u4f8b\u200b\uff09 \u200b\u5b58\u5728\u200b\u7684\u200b\u56f0\u96be\u200b\u53ca\u200b\u95ee\u9898\u200b\uff1a</p> <ul> <li> <p>\u200b\u5728\u200b\u81ea\u7136\u200b\u573a\u666f\u200b\u4e0b\u200b\uff0c\u200b\u7531\u4e8e\u200b\u5404\u79cd\u200b\u62cd\u6444\u200b\u8bbe\u5907\u200b\u4ee5\u53ca\u200b\u5149\u7ebf\u200b\u3001\u200b\u89d2\u5ea6\u200b\u4e0d\u540c\u200b\u7b49\u200b\u5f71\u54cd\u200b\u5bfc\u81f4\u200b\u5b9e\u9645\u200b\u5f97\u5230\u200b\u7684\u200b\u8bc1\u4ef6\u200b\u5f71\u50cf\u200b\u5343\u5dee\u4e07\u522b\u200b\u3002</p> </li> <li> <p>\u200b\u5982\u4f55\u200b\u5feb\u901f\u200b\u63d0\u53d6\u200b\u9700\u8981\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b</p> </li> <li> <p>\u200b\u591a\u884c\u200b\u7684\u200b\u6587\u672c\u200b\u4fe1\u606f\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4f55\u200b\u6b63\u786e\u200b\u62fc\u63a5\u200b</p> </li> </ul> <p></p> <ul> <li> <p>OCR\u200b\u6280\u672f\u200b\u62c6\u89e3\u200b---OCR\u200b\u5de5\u5177\u200b\u5e93\u200b</p> <p>PaddleOCR\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e30\u5bcc\u200b\u3001\u200b\u9886\u5148\u200b\u4e14\u200b\u5b9e\u7528\u200b\u7684\u200bOCR\u200b\u5de5\u5177\u200b\u5e93\u200b\uff0c\u200b\u52a9\u529b\u200b\u5f00\u53d1\u8005\u200b\u8bad\u7ec3\u200b\u51fa\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5e76\u200b\u5e94\u7528\u200b\u843d\u5730\u200b</p> </li> </ul> <p>\u200b\u8eab\u4efd\u8bc1\u200b\u8bc6\u522b\u200b\uff1a\u200b\u7528\u200b\u73b0\u6709\u200b\u7684\u200b\u65b9\u6cd5\u200b\u8bc6\u522b\u200b</p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#_2","title":"\u8eab\u4efd\u8bc1\u200b\u8bc6\u522b\u200b\uff1a\u200b\u68c0\u6d4b\u200b+\u200b\u5206\u7c7b","text":"<p>\u200b\u65b9\u6cd5\u200b\uff1a\u200b\u57fa\u4e8e\u200b\u73b0\u6709\u200b\u7684\u200bdbnet\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u52a0\u5165\u200b\u5206\u7c7b\u200b\u65b9\u6cd5\u200b\u3002\u200b\u68c0\u6d4b\u200b\u540c\u65f6\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u4ece\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u4f18\u5316\u200b\u8bc6\u522b\u200b\u6d41\u7a0b\u200b</p> <p></p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#_3","title":"\u6570\u636e\u200b\u6807\u6ce8","text":"<p>\u200b\u4f7f\u7528\u200bPaddleOCRLable\u200b\u8fdb\u884c\u200b\u5feb\u901f\u200b\u6807\u6ce8\u200b</p> <p></p> <ul> <li>\u200b\u4fee\u6539\u200bPPOCRLabel.py\uff0c\u200b\u5c06\u200b\u4e0b\u56fe\u200b\u4e2d\u200b\u7684\u200bkie\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200bTrue</li> </ul> <p></p> <ul> <li>\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u8e29\u200b\u5751\u200b\u5206\u4eab\u200b</li> </ul> <p></p> <p>\u200b    \u200b\u6ce8\u200b\uff1a\u200b\u4e24\u8005\u200b\u53ea\u6709\u200b\u6807\u6ce8\u200b\u6709\u200b\u5dee\u522b\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u6570\u636e\u200b\u96c6\u90fd\u200b\u76f8\u540c\u200b</p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#4","title":"4 . \u200b\u9879\u76ee\u200b\u5b9e\u8df5","text":"<p>AIStudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b\uff1a\u200b\u5feb\u901f\u200b\u6784\u5efa\u200b\u5361\u8bc1\u7c7b\u200bOCR</p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#41","title":"4.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>1\uff09\u200b\u62c9\u53d6\u200bpaddleocr\u200b\u9879\u76ee\u200b\uff0c\u200b\u5982\u679c\u200b\u4ece\u200bgithub\u200b\u4e0a\u62c9\u53d6\u200b\u901f\u5ea6\u6162\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4ece\u200bgitee\u200b\u4e0a\u200b\u83b7\u53d6\u200b\u3002</p> <pre><code>!git clone https://github.com/PaddlePaddle/PaddleOCR.git  -b release/2.6  /home/aistudio/work/\n</code></pre> <p>2\uff09\u200b\u83b7\u53d6\u200b\u5e76\u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u679c\u200b\u8981\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u6a21\u578b\u5e93\u200b\u91cc\u200b\u81ea\u4e3b\u200b\u9009\u62e9\u200b\u5408\u9002\u200b\u6a21\u578b\u200b\u3002</p> <pre><code>!wget -P work/pre_trained/   https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\n!tar -vxf /home/aistudio/work/pre_trained/ch_PP-OCRv3_det_distill_train.tar -C /home/aistudio/work/pre_trained\n</code></pre> <p>3\uff09\u200b\u5b89\u88c5\u200b\u5fc5\u8981\u200b\u4f9d\u8d56\u200b</p> <pre><code>!pip install -r /home/aistudio/work/requirements.txt\n</code></pre>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#42","title":"4.2 \u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4fee\u6539","text":"<p>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b <code>work/configs/det/detmv3db.yml</code></p> <p>\u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u8bf4\u660e\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200bGlobal\u200b\u53d8\u91cf\u200b\u4e2d\u200b\u9700\u8981\u200b\u6dfb\u52a0\u200b\u4ee5\u4e0b\u200b\u4e24\u4e2a\u200b\u53c2\u6570\u200b\uff1a</p> <p>\u200b  - label_list \u200b\u4e3a\u200b\u6807\u7b7e\u200b\u8868\u200b \u200b  - num_classes \u200b\u4e3a\u200b\u5206\u7c7b\u200b\u6570\u200b \u200b\u200b\u4e0a\u8ff0\u200b\u4e24\u4e2a\u200b\u53c2\u6570\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u7684\u200b\u60c5\u51b5\u200b\u914d\u7f6e\u200b\u5373\u53ef\u200b</p> <p></p> <p>\u200b\u5176\u4e2d\u200blable_list\u200b\u5185\u5bb9\u200b\u5982\u4e0b\u200b\u4f8b\u200b\u6240\u793a\u200b\uff0c\u200b\u5efa\u8bae\u200b\u7b2c\u4e00\u4e2a\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b background\uff0c\u200b\u4e0d\u8981\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u5b9e\u9645\u200b\u8981\u200b\u63d0\u53d6\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u79cd\u7c7b\u200b\uff1a</p> <p></p> <p>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u5176\u4ed6\u200b\u8bbe\u7f6e\u200b\u8bf4\u660e\u200b</p> <p></p> <p></p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#43","title":"4.3 \u200b\u4ee3\u7801\u200b\u4fee\u6539","text":""},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#431","title":"4.3.1 \u200b\u6570\u636e\u200b\u8bfb\u53d6","text":"<p>\u200b\u4fee\u6539\u200b PaddleOCR/ppocr/data/imaug/label_ops.py\u200b\u4e2d\u200b\u7684\u200bDetLabelEncode</p> <pre><code>class DetLabelEncode(object):\n\n    # \u200b\u4fee\u6539\u200b\u68c0\u6d4b\u200b\u6807\u7b7e\u200b\u7684\u200b\u7f16\u7801\u200b\u5904\u200b\uff0c\u200b\u65b0\u589e\u200b\u4e86\u200b\u53c2\u6570\u200b\u5206\u7c7b\u200b\u6570\u200b\uff1anum_classes\uff0c\u200b\u91cd\u5199\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5206\u7c7b\u200b\u6807\u7b7e\u200b\u7684\u200b\u8bfb\u53d6\u200b\n\n    def __init__(self, label_list, num_classes=8, **kwargs):\n        self.num_classes = num_classes\n        self.label_list = []\n        if label_list:\n            if isinstance(label_list, str):\n                with open(label_list, 'r+', encoding='utf-8') as f:\n                    for line in f.readlines():\n                        self.label_list.append(line.replace(\"\\n\", \"\"))\n            else:\n                self.label_list = label_list\n        else:\n            assert ' please check label_list whether it is none or config is right'\n\n        if num_classes != len(self.label_list): # \u200b\u6821\u9a8c\u200b\u5206\u7c7b\u200b\u6570\u200b\u548c\u200b\u6807\u7b7e\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\n            assert 'label_list length is not equal to the num_classes'\n\n    def __call__(self, data):\n        label = data['label']\n        label = json.loads(label)\n        nBox = len(label)\n        boxes, txts, txt_tags, classes = [], [], [], []\n        for bno in range(0, nBox):\n            box = label[bno]['points']\n            txt = label[bno]['key_cls']  # \u200b\u6b64\u5904\u200b\u5c06\u200bkie\u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u8bfb\u53d6\u200b\n            boxes.append(box)\n            txts.append(txt)\n\n            if txt in ['*', '###']:\n                txt_tags.append(True)\n                if self.num_classes &gt; 1:\n                    classes.append(-2)\n            else:\n                txt_tags.append(False)\n                if self.num_classes &gt; 1:  # \u200b\u5c06\u200bKIE\u200b\u5185\u5bb9\u200b\u7684\u200bkey\u200b\u6807\u7b7e\u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u6807\u7b7e\u200b\u4f7f\u7528\u200b\n                    classes.append(int(self.label_list.index(txt)))\n\n        if len(boxes) == 0:\n\n            return None\n        boxes = self.expand_points_num(boxes)\n        boxes = np.array(boxes, dtype=np.float32)\n        txt_tags = np.array(txt_tags, dtype=np.bool_)\n        classes = classes\n        data['polys'] = boxes\n        data['texts'] = txts\n        data['ignore_tags'] = txt_tags\n        if self.num_classes &gt; 1:\n            data['classes'] = classes\n        return data\n</code></pre> <p>\u200b\u4fee\u6539\u200bP<code>addleOCR/ppocr/data/imaug/make_shrink_map.py</code>\u200b\u4e2d\u200b\u7684\u200bMakeShrinkMap\u200b\u7c7b\u200b\u3002\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u7684\u200blabel_list\u200b\u4e2d\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u4e3a\u200b\u8981\u200b\u68c0\u6d4b\u200b\u7684\u200b\u4fe1\u606f\u200b\u90a3\u4e48\u200b\u4f1a\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u7684\u200bmask,</p> <p>\u200b\u4e3e\u4f8b\u8bf4\u660e\u200b\uff1a \u200b\u8fd9\u662f\u200b\u68c0\u6d4b\u200b\u7684\u200bmask\u200b\u56fe\u200b\uff0c\u200b\u56fe\u4e2d\u200b\u6709\u200b\u56db\u4e2a\u200bmask\u200b\u90a3\u4e48\u200b\u5b9e\u9645\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5206\u7c7b\u200b\u5e94\u8be5\u200b\u662f\u200b4\u200b\u7c7b\u200b</p> <p></p> <p>label_list\u200b\u4e2d\u200b\u7b2c\u4e00\u4e2a\u200b\u4e3a\u200b\u5173\u952e\u200b\u5206\u7c7b\u200b\uff0c\u200b\u5219\u200b\u5f97\u5230\u200b\u7684\u200b\u5206\u7c7b\u200bMask\u200b\u5b9e\u9645\u200b\u5982\u4e0b\u200b\uff0c\u200b\u4e0e\u200b\u4e0a\u200b\u56fe\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u5c11\u200b\u4e86\u200b\u4e00\u4e2a\u200bbox\uff1a</p> <p></p> <pre><code>class MakeShrinkMap(object):\n    r'''\n    Making binary mask from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    '''\n\n    def __init__(self, min_text_size=8, shrink_ratio=0.4, num_classes=8, **kwargs):\n        self.min_text_size = min_text_size\n        self.shrink_ratio = shrink_ratio\n        self.num_classes = num_classes  #  \u200b\u6dfb\u52a0\u200b\u4e86\u200b\u5206\u7c7b\u200b\n\n    def __call__(self, data):\n        image = data['image']\n        text_polys = data['polys']\n        ignore_tags = data['ignore_tags']\n        if self.num_classes &gt; 1:\n            classes = data['classes']\n\n        h, w = image.shape[:2]\n        text_polys, ignore_tags = self.validate_polygons(text_polys,\n                                                         ignore_tags, h, w)\n        gt = np.zeros((h, w), dtype=np.float32)\n        mask = np.ones((h, w), dtype=np.float32)\n        gt_class = np.zeros((h, w), dtype=np.float32)  # \u200b\u65b0\u589e\u200b\u5206\u7c7b\u200b\n        for i in range(len(text_polys)):\n            polygon = text_polys[i]\n            height = max(polygon[:, 1]) - min(polygon[:, 1])\n            width = max(polygon[:, 0]) - min(polygon[:, 0])\n            if ignore_tags[i] or min(height, width) &lt; self.min_text_size:\n                cv2.fillPoly(mask,\n                             polygon.astype(np.int32)[np.newaxis, :, :], 0)\n                ignore_tags[i] = True\n            else:\n                polygon_shape = Polygon(polygon)\n                subject = [tuple(l) for l in polygon]\n                padding = pyclipper.PyclipperOffset()\n                padding.AddPath(subject, pyclipper.JT_ROUND,\n                                pyclipper.ET_CLOSEDPOLYGON)\n                shrinked = []\n\n                # Increase the shrink ratio every time we get multiple polygon returned back\n                possible_ratios = np.arange(self.shrink_ratio, 1,\n                                            self.shrink_ratio)\n                np.append(possible_ratios, 1)\n                for ratio in possible_ratios:\n                    distance = polygon_shape.area * (\n                        1 - np.power(ratio, 2)) / polygon_shape.length\n                    shrinked = padding.Execute(-distance)\n                    if len(shrinked) == 1:\n                        break\n\n                if shrinked == []:\n                    cv2.fillPoly(mask,\n                                 polygon.astype(np.int32)[np.newaxis, :, :], 0)\n                    ignore_tags[i] = True\n                    continue\n\n                for each_shirnk in shrinked:\n                    shirnk = np.array(each_shirnk).reshape(-1, 2)\n                    cv2.fillPoly(gt, [shirnk.astype(np.int32)], 1)\n                    if self.num_classes &gt; 1:  # \u200b\u7ed8\u5236\u200b\u5206\u7c7b\u200b\u7684\u200bmask\n                        cv2.fillPoly(gt_class, polygon.astype(np.int32)[np.newaxis, :, :], classes[i])\n\n\n        data['shrink_map'] = gt\n\n        if self.num_classes &gt; 1:\n            data['class_mask'] = gt_class\n\n        data['shrink_mask'] = mask\n        return data\n</code></pre> <p>\u200b\u7531\u4e8e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u4f1a\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200bresize\u200b\u8bbe\u7f6e\u200b\uff0cyml\u200b\u4e2d\u200b\u7684\u200b\u64cd\u4f5c\u200b\u4e3a\u200b\uff1a<code>EastRandomCropData</code>\uff0c\u200b\u6240\u4ee5\u200b\u9700\u8981\u200b\u4fee\u6539\u200b<code>PaddleOCR/ppocr/data/imaug/random_crop_data.py</code>\u200b\u4e2d\u200b\u7684\u200b<code>EastRandomCropData</code></p> <pre><code>class EastRandomCropData(object):\n    def __init__(self,\n                 size=(640, 640),\n                 max_tries=10,\n                 min_crop_side_ratio=0.1,\n                 keep_ratio=True,\n                 num_classes=8,\n                 **kwargs):\n        self.size = size\n        self.max_tries = max_tries\n        self.min_crop_side_ratio = min_crop_side_ratio\n        self.keep_ratio = keep_ratio\n        self.num_classes = num_classes\n\n    def __call__(self, data):\n        img = data['image']\n        text_polys = data['polys']\n        ignore_tags = data['ignore_tags']\n        texts = data['texts']\n        if self.num_classes &gt; 1:\n            classes = data['classes']\n        all_care_polys = [\n            text_polys[i] for i, tag in enumerate(ignore_tags) if not tag\n        ]\n        # \u200b\u8ba1\u7b97\u200bcrop\u200b\u533a\u57df\u200b\n        crop_x, crop_y, crop_w, crop_h = crop_area(\n            img, all_care_polys, self.min_crop_side_ratio, self.max_tries)\n        # crop \u200b\u56fe\u7247\u200b \u200b\u4fdd\u6301\u200b\u6bd4\u4f8b\u200b\u586b\u5145\u200b\n        scale_w = self.size[0] / crop_w\n        scale_h = self.size[1] / crop_h\n        scale = min(scale_w, scale_h)\n        h = int(crop_h * scale)\n        w = int(crop_w * scale)\n        if self.keep_ratio:\n            padimg = np.zeros((self.size[1], self.size[0], img.shape[2]),\n                              img.dtype)\n            padimg[:h, :w] = cv2.resize(\n                img[crop_y:crop_y + crop_h, crop_x:crop_x + crop_w], (w, h))\n            img = padimg\n        else:\n            img = cv2.resize(\n                img[crop_y:crop_y + crop_h, crop_x:crop_x + crop_w],\n                tuple(self.size))\n        # crop \u200b\u6587\u672c\u6846\u200b\n        text_polys_crop = []\n        ignore_tags_crop = []\n        texts_crop = []\n        classes_crop = []\n        for poly, text, tag,class_index in zip(text_polys, texts, ignore_tags,classes):\n            poly = ((poly - (crop_x, crop_y)) * scale).tolist()\n            if not is_poly_outside_rect(poly, 0, 0, w, h):\n                text_polys_crop.append(poly)\n                ignore_tags_crop.append(tag)\n                texts_crop.append(text)\n                if self.num_classes &gt; 1:\n                    classes_crop.append(class_index)\n        data['image'] = img\n        data['polys'] = np.array(text_polys_crop)\n        data['ignore_tags'] = ignore_tags_crop\n        data['texts'] = texts_crop\n        if self.num_classes &gt; 1:\n            data['classes'] = classes_crop\n        return data\n</code></pre>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#432-head","title":"4.3.2  head\u200b\u4fee\u6539","text":"<p>\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b<code>ppocr/modeling/heads/det_db_head.py</code>\uff0c\u200b\u5c06\u200bHead\u200b\u7c7b\u4e2d\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u4fee\u6539\u200b\u4e3a\u200b\u5b9e\u9645\u200b\u7684\u200b\u5206\u7c7b\u200b\u6570\u200b\uff0c\u200b\u540c\u65f6\u200b\u5728\u200bDBHead\u200b\u4e2d\u200b\u65b0\u589e\u200b\u5206\u7c7b\u200b\u7684\u200bhead\u3002</p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#433-loss","title":"4.3.3 \u200b\u4fee\u6539\u200bloss","text":"<p>\u200b\u4fee\u6539\u200b<code>PaddleOCR/ppocr/losses/det_db_loss.py</code>\u200b\u4e2d\u200b\u7684\u200bDBLoss\u200b\u7c7b\u200b\uff0c\u200b\u5206\u7c7b\u200b\u91c7\u7528\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002</p> <p></p>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#434","title":"4.3.4 \u200b\u540e\u5904\u7406","text":"<p>\u200b\u7531\u4e8e\u200b\u6d89\u53ca\u200b\u5230\u200beval\u200b\u4ee5\u53ca\u200b\u540e\u7eed\u200b\u63a8\u7406\u200b\u80fd\u5426\u200b\u6b63\u5e38\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u76f8\u5173\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4fee\u6539\u200b\u4f4d\u7f6e\u200b<code>PaddleOCR/ppocr/postprocess/db_postprocess.py</code>\u200b\u4e2d\u200b\u7684\u200bDBPostProcess\u200b\u7c7b\u200b</p> <pre><code>class DBPostProcess(object):\n    \"\"\"\n    The post process for Differentiable Binarization (DB).\n    \"\"\"\n\n    def __init__(self,\n                 thresh=0.3,\n                 box_thresh=0.7,\n                 max_candidates=1000,\n                 unclip_ratio=2.0,\n                 use_dilation=False,\n                 score_mode=\"fast\",\n                 **kwargs):\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.max_candidates = max_candidates\n        self.unclip_ratio = unclip_ratio\n        self.min_size = 3\n        self.score_mode = score_mode\n        assert score_mode in [\n            \"slow\", \"fast\"\n        ], \"Score mode must be in [slow, fast] but got: {}\".format(score_mode)\n\n        self.dilation_kernel = None if not use_dilation else np.array(\n            [[1, 1], [1, 1]])\n\n    def boxes_from_bitmap(self, pred, _bitmap, classes, dest_width, dest_height):\n        \"\"\"\n        _bitmap: single map with shape (1, H, W),\n                whose values are binarized as {0, 1}\n        \"\"\"\n\n        bitmap = _bitmap\n        height, width = bitmap.shape\n\n        outs = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST,\n                                cv2.CHAIN_APPROX_SIMPLE)\n        if len(outs) == 3:\n            img, contours, _ = outs[0], outs[1], outs[2]\n        elif len(outs) == 2:\n            contours, _ = outs[0], outs[1]\n\n        num_contours = min(len(contours), self.max_candidates)\n\n        boxes = []\n        scores = []\n        class_indexes = []\n        class_scores = []\n        for index in range(num_contours):\n            contour = contours[index]\n            points, sside = self.get_mini_boxes(contour)\n            if sside &lt; self.min_size:\n                continue\n            points = np.array(points)\n            if self.score_mode == \"fast\":\n                score, class_index, class_score = self.box_score_fast(pred, points.reshape(-1, 2), classes)\n            else:\n                score, class_index, class_score = self.box_score_slow(pred, contour, classes)\n            if self.box_thresh &gt; score:\n                continue\n\n            box = self.unclip(points).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside &lt; self.min_size + 2:\n                continue\n            box = np.array(box)\n\n            box[:, 0] = np.clip(\n                np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height)\n\n            boxes.append(box.astype(np.int16))\n            scores.append(score)\n\n            class_indexes.append(class_index)\n            class_scores.append(class_score)\n\n        if classes is None:\n            return np.array(boxes, dtype=np.int16), scores\n        else:\n            return np.array(boxes, dtype=np.int16), scores, class_indexes, class_scores\n\n    def unclip(self, box):\n        unclip_ratio = self.unclip_ratio\n        poly = Polygon(box)\n        distance = poly.area * unclip_ratio / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = np.array(offset.Execute(distance))\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] &gt; points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] &gt; points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [\n            points[index_1], points[index_2], points[index_3], points[index_4]\n        ]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box, classes):\n        '''\n        box_score_fast: use bbox mean score as the mean score\n        '''\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int32), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int32), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int32), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int32), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n\n        if classes is None:\n            return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0], None, None\n        else:\n            k = 999\n            class_mask = np.full((ymax - ymin + 1, xmax - xmin + 1), k, dtype=np.int32)\n\n            cv2.fillPoly(class_mask, box.reshape(1, -1, 2).astype(np.int32), 0)\n            classes = classes[ymin:ymax + 1, xmin:xmax + 1]\n\n            new_classes = classes + class_mask\n            a = new_classes.reshape(-1)\n            b = np.where(a &gt;= k)\n            classes = np.delete(a, b[0].tolist())\n\n            class_index = np.argmax(np.bincount(classes))\n            class_score = np.sum(classes == class_index) / len(classes)\n\n            return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0], class_index, class_score\n\n    def box_score_slow(self, bitmap, contour, classes):\n        \"\"\"\n        box_score_slow: use polyon mean score as the mean score\n        \"\"\"\n        h, w = bitmap.shape[:2]\n        contour = contour.copy()\n        contour = np.reshape(contour, (-1, 2))\n\n        xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)\n        xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)\n        ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)\n        ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n\n        contour[:, 0] = contour[:, 0] - xmin\n        contour[:, 1] = contour[:, 1] - ymin\n\n        cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype(np.int32), 1)\n\n        if classes is None:\n            return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0], None, None\n        else:\n            k = 999\n            class_mask = np.full((ymax - ymin + 1, xmax - xmin + 1), k, dtype=np.int32)\n\n            cv2.fillPoly(class_mask, contour.reshape(1, -1, 2).astype(np.int32), 0)\n            classes = classes[ymin:ymax + 1, xmin:xmax + 1]\n\n            new_classes = classes + class_mask\n            a = new_classes.reshape(-1)\n            b = np.where(a &gt;= k)\n            classes = np.delete(a, b[0].tolist())\n\n            class_index = np.argmax(np.bincount(classes))\n            class_score = np.sum(classes == class_index) / len(classes)\n\n            return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0], class_index, class_score\n\n    def __call__(self, outs_dict, shape_list):\n        pred = outs_dict['maps']\n        if isinstance(pred, paddle.Tensor):\n            pred = pred.numpy()\n        pred = pred[:, 0, :, :]\n        segmentation = pred &gt; self.thresh\n\n        if \"classes\" in outs_dict:\n            classes = outs_dict['classes']\n            if isinstance(classes, paddle.Tensor):\n                classes = classes.numpy()\n            classes = classes[:, 0, :, :]\n\n        else:\n            classes = None\n\n        boxes_batch = []\n        for batch_index in range(pred.shape[0]):\n            src_h, src_w, ratio_h, ratio_w = shape_list[batch_index]\n            if self.dilation_kernel is not None:\n                mask = cv2.dilate(\n                    np.array(segmentation[batch_index]).astype(np.uint8),\n                    self.dilation_kernel)\n            else:\n                mask = segmentation[batch_index]\n\n            if classes is None:\n                boxes, scores = self.boxes_from_bitmap(pred[batch_index], mask, None,\n                                                       src_w, src_h)\n                boxes_batch.append({'points': boxes})\n            else:\n                boxes, scores, class_indexes, class_scores = self.boxes_from_bitmap(pred[batch_index], mask,\n                                                                                      classes[batch_index],\n                                                                                      src_w, src_h)\n                boxes_batch.append({'points': boxes, \"classes\": class_indexes, \"class_scores\": class_scores})\n\n        return boxes_batch\n</code></pre>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#44","title":"4.4. \u200b\u6a21\u578b\u200b\u542f\u52a8","text":"<p>\u200b\u5728\u200b\u5b8c\u6210\u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u540e\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u6b63\u5e38\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b</p> <pre><code>!python /home/aistudio/work/PaddleOCR/tools/train.py  -c  /home/aistudio/work/PaddleOCR/configs/det/det_mv3_db.yml\n</code></pre> <p>\u200b\u5176\u4ed6\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>!python /home/aistudio/work/PaddleOCR/tools/eval.py  -c  /home/aistudio/work/PaddleOCR/configs/det/det_mv3_db.yml\n!python /home/aistudio/work/PaddleOCR/tools/infer_det.py  -c  /home/aistudio/work/PaddleOCR/configs/det/det_mv3_db.yml\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</p> <pre><code>!python /home/aistudio/work/PaddleOCR/tools/infer/predict_det.py --image_dir=\"/home/aistudio/work/test_img/\" --det_model_dir=\"/home/aistudio/work/PaddleOCR/output/infer\"\n</code></pre>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#5","title":"5 \u200b\u603b\u7ed3","text":"<ol> <li>\u200b\u5206\u7c7b\u200b+\u200b\u68c0\u6d4b\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u80fd\u591f\u200b\u7f29\u77ed\u200b\u7528\u65f6\u200b\uff0c\u200b\u5177\u4f53\u200b\u7684\u200b\u6a21\u578b\u200b\u9009\u53d6\u200b\u8981\u200b\u6839\u636e\u200b\u4e1a\u52a1\u200b\u573a\u666f\u200b\u6070\u5f53\u200b\u9009\u62e9\u200b\u3002</li> <li>\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u9700\u8981\u200b\u591a\u6b21\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\u8c03\u6574\u200b\u6807\u6ce8\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4e00\u822c\u200b\u8fdb\u884c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\uff0c\u200b\u9700\u8981\u200b\u6807\u6ce8\u200b\u81f3\u5c11\u200b\u4e0a\u200b\u767e\u5f20\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u5408\u7406\u200b\u7684\u200bbatch_size\u200b\u4ee5\u53ca\u200bresize\u200b\u5927\u5c0f\u200b\uff0c\u200b\u540c\u65f6\u200b\u6ce8\u610f\u200blr\u200b\u8bbe\u7f6e\u200b\u3002</li> </ol>"},{"location":"en/applications/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%8D%A1%E8%AF%81%E7%B1%BBOCR.html#references","title":"References","text":"<ol> <li>https://github.com/PaddlePaddle/PaddleOCR</li> <li>https://github.com/PaddlePaddle/PaddleClas</li> <li>https://blog.csdn.net/YY007H/article/details/124491217</li> </ol>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html","title":"\u57fa\u4e8e\u200bPP-OCRv3\u200b\u7684\u200b\u624b\u5199\u200b\u6587\u5b57\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u80cc\u666f\u200b\u53ca\u200b\u610f\u4e49","text":"<p>\u200b\u76ee\u524d\u200b\u5149\u5b66\u200b\u5b57\u7b26\u8bc6\u522b\u200b(OCR)\u200b\u6280\u672f\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u751f\u6d3b\u200b\u5f53\u4e2d\u200b\u88ab\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u662f\u200b\u5927\u591a\u6570\u200b\u6a21\u578b\u200b\u5728\u200b\u901a\u7528\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u8fd8\u6709\u200b\u5f85\u200b\u63d0\u9ad8\u200b\u3002\u200b\u9488\u5bf9\u200b\u4e8e\u200b\u6b64\u200b\u6211\u4eec\u200b\u501f\u52a9\u200b\u98de\u6868\u200b\u63d0\u4f9b\u200b\u7684\u200bPaddleOCR\u200b\u5957\u4ef6\u200b\u8f83\u200b\u5bb9\u6613\u200b\u7684\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u5728\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5e94\u7528\u200b\u3002\u200b\u624b\u5199\u4f53\u200b\u5728\u200b\u65e5\u5e38\u751f\u6d3b\u200b\u4e2d\u200b\u8f83\u4e3a\u200b\u5e38\u89c1\u200b\uff0c\u200b\u7136\u800c\u200b\u624b\u5199\u4f53\u200b\u7684\u200b\u8bc6\u522b\u200b\u5374\u200b\u5b58\u5728\u200b\u7740\u200b\u5f88\u5927\u200b\u7684\u200b\u6311\u6218\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u7684\u200b\u624b\u5199\u200b\u5b57\u4f53\u200b\u98ce\u683c\u200b\u4e0d\u200b\u4e00\u6837\u200b\uff0c\u200b\u8fd9\u200b\u5bf9\u4e8e\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u6765\u8bf4\u200b\u8fd8\u662f\u200b\u76f8\u5f53\u200b\u6709\u200b\u6311\u6218\u200b\u7684\u200b\u3002\u200b\u56e0\u6b64\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u624b\u5199\u4f53\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u73b0\u5b9e\u610f\u4e49\u200b\u3002\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200b\u4e00\u4e9b\u200b\u624b\u5199\u4f53\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u200b\uff1a</p> <p></p>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u9879\u76ee\u200b\u5185\u5bb9","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u5957\u4ef6\u200b\uff0c\u200b\u4ee5\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u4e3a\u200b\u57fa\u7840\u200b\uff0c\u200b\u9488\u5bf9\u200b\u624b\u5199\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u573a\u666f\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u3002</p> <p>Aistudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b\uff1aOCR\u200b\u624b\u5199\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b</p>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#3-pp-ocrv3","title":"3. PP-OCRv3\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd","text":"<p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPP-OCRv3\u200b\u91c7\u7528\u200b\u4e86\u200b6\u200b\u4e2a\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u3002</p> <p></p> <p>\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u6c47\u603b\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b</li> <li>GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b</li> <li>TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b</li> <li>TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</li> <li>UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b</li> <li>UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b</li> </ul> <p>\u200b\u8be6\u7ec6\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u63cf\u8ff0\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv3\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b</p>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#4","title":"4. \u200b\u5b89\u88c5\u200b\u73af\u5883","text":"<pre><code># \u200b\u9996\u5148\u200bgit\u200b\u5b98\u65b9\u200b\u7684\u200bPaddleOCR\u200b\u9879\u76ee\u200b\uff0c\u200b\u5b89\u88c5\u200b\u9700\u8981\u200b\u7684\u200b\u4f9d\u8d56\u200b\ngit clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npip install -r requirements.txt\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#5","title":"5. \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u4f7f\u7528\u200b\u516c\u5f00\u200b\u7684\u200b\u624b\u5199\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200bChinese OCR, \u200b\u4e2d\u79d1\u9662\u200b\u81ea\u52a8\u5316\u200b\u7814\u7a76\u6240\u200b-\u200b\u624b\u5199\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u200bCASIA-HWDB2.x\uff0c\u200b\u4ee5\u53ca\u200b\u7531\u200b\u4e2d\u79d1\u9662\u200b\u624b\u5199\u200b\u6570\u636e\u200b\u548c\u200b\u7f51\u4e0a\u200b\u5f00\u6e90\u200b\u6570\u636e\u200b\u5408\u5e76\u200b\u7ec4\u5408\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u7b49\u200b\uff0c\u200b\u8be5\u200b\u9879\u76ee\u200b\u5df2\u7ecf\u200b\u6302\u8f7d\u200b\u5904\u7406\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u53ef\u200b\u76f4\u63a5\u200b\u4e0b\u8f7d\u200b\u4f7f\u7528\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002</p> <pre><code>\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u89e3\u538b\u200b\u6570\u636e\u200b\ntar -xf hw_data.tar\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#6","title":"6. \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":""},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#61","title":"6.1 \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u9009\u62e9\u200b\u8bf7\u200b\u81ea\u884c\u200b\u9009\u62e9\u200b\u5176\u4ed6\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</p> <pre><code># \u200b\u4f7f\u7528\u200b\u8be5\u200b\u6307\u4ee4\u200b\u4e0b\u8f7d\u200b\u9700\u8981\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget -P ./pretrained_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\n# \u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\ntar -xf ./pretrained_models/ch_PP-OCRv3_rec_train.tar -C pretrained_models\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#62","title":"6.2 \u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b<code>configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml</code>\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u53c2\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\u3002 \u200b\u53e6\u5916\u200b\uff0cbatch_size\u200b\u53ef\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u663e\u5b58\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002 \u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff1a</p> <pre><code>  epoch_num: 100 # \u200b\u8bad\u7ec3\u200bepoch\u200b\u6570\u200b\n  save_model_dir: ./output/ch_PP-OCR_v3_rec\n  save_epoch_step: 10\n  eval_batch_step: [0, 100] # \u200b\u8bc4\u4f30\u200b\u95f4\u9694\u200b\uff0c\u200b\u6bcf\u9694\u200b100step\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\n  pretrained_model: ./pretrained_models/ch_PP-OCRv3_rec_train/best_accuracy  # \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\n\n\n  lr:\n    name: Cosine # \u200b\u4fee\u6539\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u7b56\u7565\u200b\u4e3a\u200bCosine\n    learning_rate: 0.0001 # \u200b\u4fee\u6539\u200bfine-tune\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\n    warmup_epoch: 2 # \u200b\u4fee\u6539\u200bwarmup\u200b\u8f6e\u6570\u200b\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data # \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\n    ext_op_transform_idx: 1\n    label_file_list:\n    - ./train_data/chineseocr-data/rec_hand_line_all_label_train.txt # \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u7b7e\u200b\n    - ./train_data/handwrite/HWDB2.0Train_label.txt\n    - ./train_data/handwrite/HWDB2.1Train_label.txt\n    - ./train_data/handwrite/HWDB2.2Train_label.txt\n    - ./train_data/handwrite/hwdb_ic13/handwriting_hwdb_train_labels.txt\n    - ./train_data/handwrite/HW_Chinese/train_hw.txt\n    ratio_list:\n    - 0.1\n    - 1.0\n    - 1.0\n    - 1.0\n    - 0.02\n    - 1.0\n  loader:\n    shuffle: true\n    batch_size_per_card: 64\n    drop_last: true\n    num_workers: 4\nEval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\n    label_file_list:\n    - ./train_data/chineseocr-data/rec_hand_line_all_label_val.txt # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u7b7e\u200b\n    - ./train_data/handwrite/HWDB2.0Test_label.txt\n    - ./train_data/handwrite/HWDB2.1Test_label.txt\n    - ./train_data/handwrite/HWDB2.2Test_label.txt\n    - ./train_data/handwrite/hwdb_ic13/handwriting_hwdb_val_labels.txt\n    - ./train_data/handwrite/HW_Chinese/test_hw.txt\n  loader:\n    shuffle: false\n    drop_last: false\n    batch_size_per_card: 64\n    num_workers: 4\n</code></pre> <p>\u200b\u7531\u4e8e\u200b\u6570\u636e\u200b\u96c6\u200b\u5927\u591a\u200b\u662f\u200b\u957f\u200b\u6587\u672c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u6ce8\u91ca\u200b\u6389\u200b\u4e0b\u9762\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u8bad\u7ec3\u200b\u51fa\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <pre><code>- RecConAug:\n    prob: 0.5\n    ext_data_num: 2\n    image_shape: [48, 320, 3]\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#63","title":"6.3 \u200b\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u4fee\u6539\u200b\u597d\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b<code>configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml</code>\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u7b49\u200b\u90fd\u200b\u5df2\u7ecf\u200b\u8bbe\u7f6e\u200b\u5b8c\u6bd5\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u3002</p> <pre><code># \u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\npython tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#7","title":"7. \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u6765\u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u6548\u679c\u200b:</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=\"./pretrained_models/ch_PP-OCRv3_rec_train/best_accuracy\"\n</code></pre> <pre><code>[2022/07/14 10:46:22] ppocr INFO: load pretrain successful from ./pretrained_models/ch_PP-OCRv3_rec_train/best_accuracy\neval model:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 687/687 [03:29&lt;00:00,  3.27it/s]\n[2022/07/14 10:49:52] ppocr INFO: metric eval ***************\n[2022/07/14 10:49:52] ppocr INFO: acc:0.03724954461811258\n[2022/07/14 10:49:52] ppocr INFO: norm_edit_dis:0.4859541065843199\n[2022/07/14 10:49:52] ppocr INFO: Teacher_acc:0.0371584699368947\n[2022/07/14 10:49:52] ppocr INFO: Teacher_norm_edit_dis:0.48718814890536477\n[2022/07/14 10:49:52] ppocr INFO: fps:947.8562684823883\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c\u200b\u76f4\u63a5\u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6548\u679c\u200b\u8f83\u5dee\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u4e0d\u662f\u200b\u57fa\u4e8e\u200b\u624b\u5199\u200b\u6587\u5b57\u200b\u8fdb\u884c\u200b\u5355\u72ec\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u57fa\u4e8e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200bfinetune\u3002 \u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u8bc4\u4f30\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200bfinetune\u200b\u6548\u679c\u200b\npython tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_rec/best_accuracy\"\n</code></pre> <p>\u200b\u8bc4\u4f30\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u4e3a\u200b54.3%\u3002</p> <pre><code>[2022/07/14 10:54:06] ppocr INFO: metric eval ***************\n[2022/07/14 10:54:06] ppocr INFO: acc:0.5430100180913\n[2022/07/14 10:54:06] ppocr INFO: norm_edit_dis:0.9203322593158589\n[2022/07/14 10:54:06] ppocr INFO: Teacher_acc:0.5401183969626324\n[2022/07/14 10:54:06] ppocr INFO: Teacher_norm_edit_dis:0.919827504507755\n[2022/07/14 10:54:06] ppocr INFO: fps:928.948733797251\n</code></pre> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</p>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#8","title":"8. \u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u63a8\u7406","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8f6c\u6362\u6210\u200binference\u200b\u6a21\u578b\u200b\u3002inference \u200b\u6a21\u578b\u200b\u4f1a\u200b\u989d\u5916\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u3001\u200b\u52a0\u901f\u200b\u63a8\u7406\u200b\u4e0a\u200b\u6027\u80fd\u4f18\u8d8a\u200b\uff0c\u200b\u7075\u6d3b\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u9002\u5408\u200b\u4e8e\u200b\u5b9e\u9645\u200b\u7cfb\u7edf\u96c6\u6210\u200b\u3002</p>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#81","title":"8.1 \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u5bfc\u51fa\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u8f6c\u5316\u200b\u4e3a\u200b\u63a8\u7406\u6a21\u578b\u200b\npython tools/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_rec/best_accuracy\" Global.save_inference_dir=\"./inference/rec_ppocrv3/\"\n</code></pre>"},{"location":"en/applications/%E6%89%8B%E5%86%99%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB.html#82","title":"8.2 \u200b\u6a21\u578b\u200b\u63a8\u7406","text":"<p>\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b:</p> <pre><code># \u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b\npython tools/infer/predict_rec.py --image_dir=\"train_data/handwrite/HWDB2.0Test_images/104-P16_4.jpg\" --rec_model_dir=\"./inference/rec_ppocrv3/Student\"\n</code></pre> <pre><code>[2022/07/14 10:55:56] ppocr INFO: In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\n[2022/07/14 10:55:58] ppocr INFO: Predicts of train_data/handwrite/HWDB2.0Test_images/104-P16_4.jpg:('\u200b\u54c1\u200b\u7ed3\u6784\u200b,\u200b\u5dee\u5f02\u5316\u200b\u7684\u200b\u591a\u200b\u54c1\u724c\u200b\u6e17\u900f\u200b\u4f7f\u200b\u6b27\u83b1\u96c5\u200b\u786e\u7acb\u200b\u4e86\u200b\u5176\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5316\u5986\u200b', 0.9904912114143372)\n</code></pre> <pre><code># \u200b\u53ef\u89c6\u5316\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u56fe\u7247\u200b\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\nimg_path = 'train_data/handwrite/HWDB2.0Test_images/104-P16_4.jpg'\n\ndef vis(img_path):\n    plt.figure()\n    image = Image.open(img_path)\n    plt.imshow(image)\n    plt.show()\n    # image = image.resize([208, 208])\n\n\nvis(img_path)\n</code></pre> <p></p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html","title":"\u91d1\u878d\u200b\u667a\u80fd\u200b\u6838\u9a8c\u200b\uff1a\u200b\u626b\u63cf\u200b\u5408\u540c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6","text":"<p>\u200b\u672c\u200b\u6848\u4f8b\u200b\u5c06\u200b\u4f7f\u7528\u200bOCR\u200b\u6280\u672f\u200b\u548c\u200b\u901a\u7528\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u6280\u672f\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u5408\u540c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u5ba1\u6838\u200b\u548c\u200b\u6bd4\u200b\u5bf9\u200b\u3002\u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u638c\u63e1\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u63d0\u53d6\u200b\u626b\u63cf\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b</li> <li>\u200b\u4f7f\u7528\u200bPaddleNLP\u200b\u62bd\u53d6\u200b\u81ea\u5b9a\u4e49\u200b\u4fe1\u606f\u200b</li> </ol> <p>\u200b\u70b9\u51fb\u200b\u8fdb\u5165\u200b AI Studio \u200b\u9879\u76ee\u200b</p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#1","title":"1. \u200b\u9879\u76ee\u200b\u80cc\u666f","text":"<p>\u200b\u5408\u540c\u200b\u5ba1\u6838\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u4e8e\u200b\u5927\u4e2d\u578b\u200b\u4f01\u4e1a\u200b\u3001\u200b\u4e0a\u5e02\u516c\u53f8\u200b\u3001\u200b\u8bc1\u5238\u200b\u3001\u200b\u57fa\u91d1\u200b\u516c\u53f8\u200b\u4e2d\u200b\uff0c\u200b\u662f\u200b\u89c4\u907f\u200b\u98ce\u9669\u200b\u7684\u200b\u91cd\u8981\u200b\u4efb\u52a1\u200b\u3002</p> <ul> <li>\u200b\u5408\u540c\u200b\u5185\u5bb9\u200b\u5bf9\u6bd4\u200b\uff1a\u200b\u5408\u540c\u200b\u5ba1\u6838\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u5feb\u901f\u200b\u627e\u51fa\u200b\u4e0d\u540c\u200b\u7248\u672c\u200b\u5408\u540c\u200b\u4fee\u6539\u200b\u533a\u57df\u200b\u3001\u200b\u7248\u672c\u200b\u5dee\u5f02\u200b\uff1b\u200b\u5982\u200b\u5408\u540c\u200b\u76d6\u7ae0\u200b\u5f52\u6863\u200b\u573a\u666f\u200b\u4e2d\u200b\u6709\u6548\u200b\u8bc6\u522b\u200b\u5b9e\u9645\u200b\u7b7e\u7f72\u200b\u7684\u200b\u7eb8\u8d28\u200b\u5408\u540c\u200b\u3001\u200b\u7535\u5b50\u7248\u200b\u5408\u540c\u200b\u5dee\u5f02\u200b\u3002</li> <li>\u200b\u5408\u89c4\u6027\u200b\u68c0\u67e5\u200b\uff1a\u200b\u6cd5\u52a1\u200b\u4eba\u5458\u200b\u8fdb\u884c\u200b\u5408\u540c\u200b\u5ba1\u6838\u200b\uff0c\u200b\u5982\u200b\u5408\u540c\u200b\u5b8c\u5907\u200b\u6027\u200b\u68c0\u67e5\u200b\u3001\u200b\u5927\u5c0f\u5199\u200b\u91d1\u989d\u200b\u68c0\u67e5\u200b\u3001\u200b\u7b7e\u7ea6\u200b\u4e3b\u4f53\u200b\u4e00\u81f4\u6027\u200b\u68c0\u67e5\u200b\u3001\u200b\u53cc\u65b9\u200b\u6743\u5229\u200b\u548c\u200b\u4e49\u52a1\u200b\u5bf9\u200b\u7b49\u200b\u6027\u200b\u5206\u6790\u200b\u7b49\u200b\u3002</li> <li>\u200b\u98ce\u9669\u200b\u70b9\u200b\u8bc6\u522b\u200b\uff1a\u200b\u901a\u8fc7\u200b\u5408\u540c\u200b\u5ba1\u6838\u200b\u53ef\u200b\u8bc6\u522b\u200b\u4e8b\u5b9e\u200b\u503e\u5411\u200b\u578b\u200b\u98ce\u9669\u200b\u70b9\u200b\u548c\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u578b\u200b\u98ce\u9669\u200b\u70b9\u200b\u7b49\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4ea4\u4ed8\u200b\u5730\u70b9\u200b\u7ea6\u5b9a\u200b\u4e0d\u660e\u200b\u3001\u200b\u5408\u540c\u200b\u603b\u200b\u4ef7\u6b3e\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u3001\u200b\u91cd\u8981\u200b\u6761\u6b3e\u200b\u7f3a\u5931\u200b\u7b49\u200b\u98ce\u9669\u200b\u70b9\u200b\u3002</li> </ul> <p></p> <p>\u200b\u4f20\u7edf\u200b\u4e1a\u52a1\u200b\u4e2d\u200b\u5927\u591a\u200b\u4f7f\u7528\u200b\u4eba\u5de5\u200b\u8fdb\u884c\u200b\u7eb8\u8d28\u200b\u7248\u200b\u5408\u540c\u200b\u5ba1\u6838\u200b\uff0c\u200b\u5b58\u5728\u200b\u6210\u672c\u200b\u9ad8\u200b\uff0c\u200b\u5de5\u4f5c\u91cf\u200b\u5927\u200b\uff0c\u200b\u6548\u7387\u200b\u4f4e\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e14\u200b\u4e00\u65e6\u200b\u51fa\u9519\u200b\u5c06\u200b\u9020\u6210\u200b\u5de8\u989d\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u9488\u5bf9\u200b\u4ee5\u4e0a\u200b\u573a\u666f\u200b\uff0c\u200b\u4f7f\u7528\u200bPaddleOCR+PaddleNLP\u200b\u5feb\u901f\u200b\u63d0\u53d6\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u5fae\u8c03\u200b\u5373\u53ef\u200b\u51c6\u786e\u200b\u62bd\u53d6\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u9ad8\u6548\u200b\u5b8c\u6210\u200b\u5408\u540c\u200b\u5185\u5bb9\u200b\u5bf9\u6bd4\u200b\u3001\u200b\u5408\u89c4\u6027\u200b\u68c0\u67e5\u200b\u3001\u200b\u98ce\u9669\u200b\u70b9\u200b\u8bc6\u522b\u200b\u7b49\u200b\u4efb\u52a1\u200b\uff0c\u200b\u63d0\u9ad8\u6548\u7387\u200b\uff0c\u200b\u964d\u4f4e\u200b\u98ce\u9669\u200b\u3002</p> <p></p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#2","title":"2. \u200b\u89e3\u51b3\u65b9\u6848","text":""},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#21","title":"2.1 \u200b\u626b\u63cf\u200b\u5408\u540c\u6587\u672c\u200b\u5185\u5bb9\u200b\u63d0\u53d6","text":"<p>\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u5f00\u6e90\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5b8c\u6210\u200b\u626b\u63cf\u200b\u6587\u6863\u200b\u7684\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\u63d0\u53d6\u200b\uff0c\u200b\u5728\u200b\u6e05\u6670\u200b\u6587\u6863\u200b\u4e0a\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u53ef\u200b\u8fbe\u5230\u200b95%+\u3002\u200b\u4e0b\u9762\u200b\u6765\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\u4e00\u4e0b\u200b\uff1a</p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#211","title":"2.1.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>PaddleOCR\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u9002\u7528\u200b\u4e8e\u200b\u901a\u7528\u200b\u573a\u666f\u200b\u7684\u200b\u9ad8\u7cbe\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\uff0c\u200b\u63d0\u4f9b\u6570\u636e\u200b\u9884\u5904\u7406\u200b-\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b-\u200b\u540e\u5904\u7406\u200b\u5168\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u652f\u6301\u200bpip\u200b\u5b89\u88c5\u200b\uff1a</p> <pre><code>python -m pip install paddleocr\n</code></pre>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#212","title":"2.1.2 \u200b\u6548\u679c\u200b\u6d4b\u8bd5","text":"<p>\u200b\u4f7f\u7528\u200b\u4e00\u5f20\u200b\u5408\u540c\u200b\u56fe\u7247\u200b\u4f5c\u4e3a\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\uff0c\u200b\u611f\u53d7\u200bppocrv3\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\uff1a</p> <p></p> <p>\u200b\u4f7f\u7528\u200b\u4e2d\u6587\u200b\u68c0\u6d4b\u200b+\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u63d0\u53d6\u200b\u6587\u672c\u200b\uff0c\u200b\u5b9e\u4f8b\u200b\u5316\u200bPaddleOCR\u200b\u7c7b\u200b\uff1a</p> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# paddleocr\u200b\u76ee\u524d\u200b\u652f\u6301\u200b\u4e2d\u82f1\u6587\u200b\u3001\u200b\u82f1\u6587\u200b\u3001\u200b\u6cd5\u8bed\u200b\u3001\u200b\u5fb7\u8bed\u200b\u3001\u200b\u97e9\u8bed\u200b\u3001\u200b\u65e5\u8bed\u200b\u7b49\u200b80\u200b\u4e2a\u200b\u8bed\u79cd\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200blang\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u5207\u6362\u200b\nocr = PaddleOCR(use_angle_cls=False, lang=\"ch\")  # need to run only once to download and load model into memory\n</code></pre> <p>\u200b\u4e00\u884c\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b\u9884\u6d4b\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5305\u62ec\u200b<code>\u200b\u68c0\u6d4b\u200b\u6846\u200b</code>\u200b\u548c\u200b<code>\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u5185\u5bb9\u200b</code>:</p> <pre><code>img_path = \"./test_img/hetong2.jpg\"\nresult = ocr.ocr(img_path, cls=False)\nfor line in result:\n    print(line)\n\n# \u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\nfrom PIL import Image\n\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.show()\n</code></pre>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#213","title":"2.1.3 \u200b\u56fe\u7247\u200b\u9884\u5904\u7406","text":"<p>\u200b\u901a\u8fc7\u200b\u4e0a\u200b\u56fe\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5370\u7ae0\u200b\u90e8\u5206\u200b\u9020\u6210\u200b\u7684\u200b\u6587\u672c\u200b\u906e\u76d6\u200b\uff0c\u200b\u5f71\u54cd\u200b\u4e86\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u901a\u9053\u200b\u63d0\u53d6\u200b\uff0c\u200b\u53bb\u9664\u200b\u56fe\u7247\u200b\u4e2d\u200b\u7684\u200b\u7ea2\u8272\u200b\u5370\u7ae0\u200b\uff1a</p> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#\u200b\u8bfb\u5165\u200b\u56fe\u50cf\u200b,\u200b\u4e09\u200b\u901a\u9053\u200b\nimage=cv2.imread(\"./test_img/hetong2.jpg\",cv2.IMREAD_COLOR) #timg.jpeg\n\n#\u200b\u83b7\u5f97\u200b\u4e09\u4e2a\u200b\u901a\u9053\u200b\nBch,Gch,Rch=cv2.split(image)\n\n#\u200b\u4fdd\u5b58\u200b\u4e09\u200b\u901a\u9053\u200b\u56fe\u7247\u200b\ncv2.imwrite('blue_channel.jpg',Bch)\ncv2.imwrite('green_channel.jpg',Gch)\ncv2.imwrite('red_channel.jpg',Rch)\n</code></pre>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#214","title":"2.1.4 \u200b\u5408\u540c\u6587\u672c\u200b\u4fe1\u606f\u63d0\u53d6","text":"<p>\u200b\u7ecf\u8fc7\u200b2.1.3\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u540e\u200b\uff0c\u200b\u5408\u540c\u200b\u7167\u7247\u200b\u7684\u200b\u7ea2\u8272\u200b\u901a\u9053\u200b\u88ab\u200b\u5206\u79bb\u200b\uff0c\u200b\u83b7\u5f97\u200b\u4e86\u200b\u4e00\u5f20\u200b\u76f8\u5bf9\u200b\u66f4\u200b\u5e72\u51c0\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u6b64\u65f6\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200bppocr\u200b\u6a21\u578b\u200b\u63d0\u53d6\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\uff1a</p> <pre><code>import numpy as np\nimport cv2\n\n\nimg_path = './red_channel.jpg'\nresult = ocr.ocr(img_path, cls=False)\n\n# \u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\nfrom PIL import Image\n\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./simfang.ttf')\nim_show = Image.fromarray(im_show)\nvis = np.array(im_show)\nim_show.show()\n</code></pre> <p>\u200b\u5ffd\u7565\u200b\u68c0\u6d4b\u200b\u6846\u200b\u5185\u5bb9\u200b\uff0c\u200b\u63d0\u53d6\u200b\u5b8c\u6574\u200b\u7684\u200b\u5408\u540c\u6587\u672c\u200b\uff1a</p> <pre><code>txts = [line[1][0] for line in result]\nall_context = \"\\n\".join(txts)\nprint(all_context)\n</code></pre> <p>\u200b\u901a\u8fc7\u200b\u4ee5\u4e0a\u200b\u73af\u8282\u200b\u5c31\u200b\u5b8c\u6210\u200b\u4e86\u200b\u626b\u63cf\u200b\u5408\u540c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u7684\u200b\u7b2c\u4e00\u6b65\u200b\uff1a\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\u63d0\u53d6\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u53ef\u4ee5\u200b\u57fa\u4e8e\u200b\u8bc6\u522b\u200b\u51fa\u200b\u7684\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\u62bd\u53d6\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b</p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#22","title":"2.2 \u200b\u5408\u540c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6","text":""},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#221","title":"2.2.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>\u200b\u5b89\u88c5\u200bPaddleNLP</p> <pre><code>pip install --upgrade pip\npip install --upgrade paddlenlp\n</code></pre>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#222","title":"2.2.2 \u200b\u5408\u540c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6","text":"<p>PaddleNLP \u200b\u4f7f\u7528\u200b Taskflow \u200b\u7edf\u4e00\u200b\u7ba1\u7406\u200b\u591a\u200b\u573a\u666f\u200b\u4efb\u52a1\u200b\u7684\u200b\u9884\u6d4b\u200b\u529f\u80fd\u200b\uff0c\u200b\u5176\u4e2d\u200b<code>information_extraction</code> \u200b\u901a\u8fc7\u200b\u5927\u91cf\u200b\u7684\u200b\u6709\u200b\u6807\u7b7e\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u901a\u7528\u200b\u7684\u200b\u573a\u666f\u200b\u4e2d\u200b\u4e00\u822c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u66f4\u6362\u200b\u5173\u952e\u5b57\u200b\u5373\u53ef\u200b\u3002\u200b\u4f8b\u5982\u200b\u5728\u200b\u5408\u540c\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u5b9a\u4e49\u200b\u62bd\u53d6\u200b\u5173\u952e\u5b57\u200b\uff1a</p> <p>\u200b\u7532\u65b9\u200b\u3001\u200b\u4e59\u65b9\u200b\u3001\u200b\u5e01\u79cd\u200b\u3001\u200b\u91d1\u989d\u200b\u3001\u200b\u4ed8\u6b3e\u200b\u65b9\u5f0f\u200b</p> <p>\u200b\u5c06\u200b\u4f7f\u7528\u200bOCR\u200b\u63d0\u53d6\u200b\u597d\u200b\u7684\u200b\u6587\u672c\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e09\u884c\u200b\u547d\u4ee4\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u4e0a\u200b\u6587\u4e2d\u200b\u63d0\u53d6\u200b\u5230\u200b\u7684\u200b\u5408\u540c\u6587\u672c\u200b\u8fdb\u884c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\uff1a</p> <pre><code>from paddlenlp import Taskflow\nschema = [\"\u200b\u7532\u65b9\u200b\",\"\u200b\u4e59\u65b9\u200b\",\"\u200b\u603b\u4ef7\u200b\"]\nie = Taskflow('information_extraction', schema=schema)\nie.set_schema(schema)\nie(all_context)\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200bUIE\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u51c6\u786e\u200b\u7684\u200b\u63d0\u53d6\u200b\u51fa\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u7528\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u4fe1\u606f\u200b\u6bd4\u200b\u5bf9\u200b\u6216\u200b\u5ba1\u6838\u200b\u3002</p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#3","title":"3.\u200b\u6548\u679c\u200b\u4f18\u5316","text":""},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#31","title":"3.1 \u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u540e\u5904\u7406\u200b\u8c03\u4f18","text":"<p>\u200b\u5b9e\u9645\u200b\u56fe\u7247\u200b\u91c7\u96c6\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u90e8\u5206\u200b\u56fe\u7247\u200b\u5f2f\u66f2\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u53c2\u6570\u200b\u8bc6\u522b\u200b\u6587\u672c\u200b\u65f6\u200b\u5b58\u5728\u200b\u6f0f\u68c0\u200b\uff0c\u200b\u5f71\u54cd\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u83b7\u53d6\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\u4e0b\u56fe\u200b:</p> <p></p> <p>\u200b\u76f4\u63a5\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code>img_path = \"./test_img/hetong3.jpg\"\n# \u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\nresult = ocr.ocr(img_path, cls=False)\n# \u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\nfrom PIL import Image\n\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.show()\n</code></pre> <p>\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5f2f\u66f2\u200b\u56fe\u7247\u200b\u5b58\u5728\u200b\u6f0f\u68c0\u200b\uff0c\u200b\u4e00\u822c\u6765\u8bf4\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u540e\u5904\u7406\u200b\u53c2\u6570\u200b\u89e3\u51b3\u200b\uff0c\u200b\u65e0\u9700\u200b\u91cd\u65b0\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002\u200b\u6f0f\u68c0\u200b\u95ee\u9898\u200b\u5f80\u5f80\u200b\u662f\u56e0\u4e3a\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u83b7\u5f97\u200b\u7684\u200b\u5206\u5272\u200b\u56fe\u200b\u592a\u5c0f\u200b\uff0c\u200b\u751f\u6210\u200b\u6846\u200b\u7684\u200b\u5f97\u5206\u200b\u8fc7\u4f4e\u200b\u88ab\u200b\u8fc7\u6ee4\u200b\u6389\u200b\u4e86\u200b\uff0c\u200b\u901a\u5e38\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u8c03\u6574\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li>\u200b\u5f00\u542f\u200b<code>use_dilatiion=True</code> \u200b\u81a8\u80c0\u200b\u5206\u5272\u200b\u533a\u57df\u200b</li> <li>\u200b\u8c03\u5c0f\u200b<code>det_db_box_thresh</code>\u200b\u9608\u503c\u200b</li> </ul> <pre><code># \u200b\u91cd\u65b0\u200b\u5b9e\u4f8b\u200b\u5316\u200b PaddleOCR\nocr = PaddleOCR(use_angle_cls=False, lang=\"ch\", det_db_box_thresh=0.3, use_dilation=True)\n\n# \u200b\u9884\u6d4b\u200b\u5e76\u200b\u53ef\u89c6\u5316\u200b\nimg_path = \"./test_img/hetong3.jpg\"\n# \u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\nresult = ocr.ocr(img_path, cls=False)\n# \u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.show()\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6f0f\u68c0\u200b\u95ee\u9898\u200b\u88ab\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u89e3\u51b3\u200b\uff0c\u200b\u63d0\u53d6\u200b\u5b8c\u6574\u200b\u7684\u200b\u6587\u672c\u200b\u5185\u5bb9\u200b\uff1a</p> <pre><code>txts = [line[1][0] for line in result]\ncontext = \"\\n\".join(txts)\nprint(context)\n</code></pre>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#32","title":"3.2 \u200b\u5173\u952e\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u8c03\u4f18","text":"<p>UIE\u200b\u901a\u8fc7\u200b\u5927\u91cf\u200b\u6709\u200b\u6807\u7b7e\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5f00\u7bb1\u200b\u5373\u7528\u200b\u7684\u200b\u9ad8\u7cbe\u200b\u6a21\u578b\u200b\u3002 \u200b\u7136\u800c\u200b\u9488\u5bf9\u200b\u4e0d\u540c\u200b\u573a\u666f\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u90e8\u5206\u200b\u5b9e\u4f53\u200b\u65e0\u6cd5\u200b\u88ab\u200b\u62bd\u53d6\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002\u200b\u901a\u5e38\u200b\u6765\u8bf4\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u51e0\u4e2a\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u6548\u679c\u200b\u8c03\u4f18\u200b\uff1a</p> <ul> <li>\u200b\u4fee\u6539\u200b schema</li> <li>\u200b\u6dfb\u52a0\u200b\u6b63\u5219\u200b\u65b9\u6cd5\u200b</li> <li>\u200b\u6807\u6ce8\u200b\u5c0f\u200b\u6837\u672c\u200b\u5fae\u8c03\u200b\u6a21\u578b\u200b</li> </ul> <p>\u200b\u4fee\u6539\u200bschema</p> <p>Prompt\u200b\u548c\u200b\u539f\u6587\u200b\u63cf\u8ff0\u200b\u8d8a\u50cf\u200b\uff0c\u200b\u62bd\u53d6\u200b\u6548\u679c\u200b\u8d8a\u200b\u597d\u200b\uff0c\u200b\u4f8b\u5982\u200b</p> <pre><code>\u200b\u4e09\u200b\uff1a\u200b\u5408\u540c\u200b\u4ef7\u683c\u200b\uff1a\u200b\u603b\u4ef7\u200b\u4e3a\u200b\u4eba\u6c11\u5e01\u200b\u5927\u5199\u200b\uff1a\u200b\u53c2\u62fe\u200b\u7396\u200b\u4e07\u200b\u634c\u200b\u4edf\u200b\u4f0d\u4f70\u200b\n\u200b\u5143\u200b\uff0c\u200b\u5c0f\u5199\u200b\uff1a398500.00\u200b\u5143\u200b\u3002\u200b\u603b\u4ef7\u200b\u4e2d\u200b\u5305\u62ec\u200b\u7ad9\u623f\u200b\u5de5\u7a0b\u5efa\u8bbe\u200b\u3001\u200b\u5b89\u88c5\u200b\n\u200b\u53ca\u200b\u76f8\u5173\u200b\u907f\u96f7\u200b\u3001\u200b\u6d88\u9632\u200b\u3001\u200b\u63a5\u5730\u200b\u3001\u200b\u7535\u529b\u200b\u3001\u200b\u6750\u6599\u8d39\u200b\u3001\u200b\u68c0\u9a8c\u8d39\u200b\u3001\u200b\u5b89\u5168\u200b\u3001\n\u200b\u9a8c\u6536\u200b\u7b49\u200b\u6240\u200b\u9700\u200b\u8d39\u7528\u200b\u53ca\u5176\u200b\u4ed6\u200b\u76f8\u5173\u200b\u8d39\u7528\u200b\u548c\u200b\u7a0e\u91d1\u200b\u3002\n</code></pre> <p>schema = [\"\u200b\u603b\u91d1\u989d\u200b\"] \u200b\u65f6\u200b\u65e0\u6cd5\u200b\u51c6\u786e\u200b\u62bd\u53d6\u200b\uff0c\u200b\u4e0e\u200b\u539f\u6587\u200b\u63cf\u8ff0\u200b\u5dee\u5f02\u200b\u8f83\u5927\u200b\u3002 \u200b\u4fee\u6539\u200b schema = [\"\u200b\u603b\u4ef7\u200b\"] \u200b\u518d\u6b21\u200b\u5c1d\u8bd5\u200b\uff1a</p> <pre><code>from paddlenlp import Taskflow\n# schema = [\"\u200b\u603b\u91d1\u989d\u200b\"]\nschema = [\"\u200b\u603b\u4ef7\u200b\"]\nie = Taskflow('information_extraction', schema=schema)\nie.set_schema(schema)\nie(all_context)\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b UIE\u200b\u7684\u200b\u5efa\u6a21\u200b\u65b9\u5f0f\u200b\u4e3b\u8981\u200b\u662f\u200b\u901a\u8fc7\u200b <code>Prompt</code> \u200b\u65b9\u5f0f\u200b\u6765\u200b\u5efa\u6a21\u200b\uff0c <code>Prompt</code> \u200b\u5728\u200b\u5c0f\u200b\u6837\u672c\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u5fae\u8c03\u200b\u6548\u679c\u200b\u975e\u5e38\u200b\u6709\u6548\u200b\u3002\u200b\u8be6\u7ec6\u200b\u7684\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b+\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u6b65\u9aa4\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u9879\u76ee\u200b\uff1a</p> <p>PaddleNLP\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b\u6280\u672f\u200b\u91cd\u78c5\u200b\u5347\u7ea7\u200b\uff01</p> <p>\u200b\u5de5\u5355\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b</p> <p>\u200b\u5feb\u9012\u200b\u5355\u200b\u4fe1\u606f\u200b\u62bd\u53d6\u200b</p>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#_2","title":"\u603b\u7ed3","text":"<p>\u200b\u626b\u63cf\u200b\u5408\u540c\u200b\u7684\u200b\u5173\u952e\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b PaddleOCR + PaddleNLP \u200b\u7ec4\u5408\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4e24\u4e2a\u200b\u5de5\u5177\u200b\u5747\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u4f18\u52bf\u200b\uff1a</p> <ul> <li>\u200b\u4f7f\u7528\u200b\u7b80\u5355\u200b\uff1awhl\u200b\u5305\u200b\u4e00\u952e\u200b\u5b89\u88c5\u200b\uff0c3\u200b\u884c\u200b\u547d\u4ee4\u200b\u8c03\u7528\u200b</li> <li>\u200b\u6548\u679c\u200b\u9886\u5148\u200b\uff1a\u200b\u4f18\u79c0\u200b\u7684\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u53ef\u200b\u8986\u76d6\u200b\u51e0\u4e4e\u200b\u5168\u90e8\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b</li> <li>\u200b\u8c03\u4f18\u200b\u6210\u672c\u4f4e\u200b\uff1aOCR\u200b\u6a21\u578b\u200b\u53ef\u200b\u901a\u8fc7\u200b\u540e\u5904\u7406\u200b\u53c2\u6570\u200b\u7684\u200b\u8c03\u6574\u200b\u9002\u914d\u200b\u7565\u6709\u200b\u504f\u5dee\u200b\u7684\u200b\u626b\u63cf\u200b\u6587\u672c\u200b\uff0c UIE\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6781\u5c11\u200b\u7684\u200b\u6807\u6ce8\u200b\u6837\u672c\u200b\u5fae\u8c03\u200b\uff0c\u200b\u6210\u672c\u200b\u5f88\u200b\u4f4e\u200b\u3002</li> </ul>"},{"location":"en/applications/%E6%89%AB%E6%8F%8F%E5%90%88%E5%90%8C%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96.html#_3","title":"\u4f5c\u4e1a","text":"<p>\u200b\u5c1d\u8bd5\u200b\u81ea\u5df1\u200b\u89e3\u6790\u200b\u51fa\u200b <code>test_img/homework.png</code> \u200b\u626b\u63cf\u200b\u5408\u540c\u200b\u4e2d\u200b\u7684\u200b [\u200b\u7532\u65b9\u200b\u3001\u200b\u4e59\u65b9\u200b] \u200b\u5173\u952e\u8bcd\u200b\uff1a</p> <p></p> <p>\u200b\u66f4\u200b\u591a\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5782\u7c7b\u200b\u6a21\u578b\u200b\u83b7\u53d6\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html","title":"\u57fa\u4e8e\u200bPP-OCRv3\u200b\u7684\u200b\u6db2\u6676\u5c4f\u200b\u8bfb\u6570\u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u80cc\u666f\u200b\u53ca\u200b\u610f\u4e49","text":"<p>\u200b\u76ee\u524d\u200b\u5149\u5b66\u200b\u5b57\u7b26\u8bc6\u522b\u200b(OCR)\u200b\u6280\u672f\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u751f\u6d3b\u200b\u5f53\u4e2d\u200b\u88ab\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u662f\u200b\u5927\u591a\u6570\u200b\u6a21\u578b\u200b\u5728\u200b\u901a\u7528\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u8fd8\u6709\u200b\u5f85\u200b\u63d0\u9ad8\u200b\uff0c\u200b\u9488\u5bf9\u200b\u4e8e\u200b\u6b64\u200b\u6211\u4eec\u200b\u501f\u52a9\u200b\u98de\u6868\u200b\u63d0\u4f9b\u200b\u7684\u200bPaddleOCR\u200b\u5957\u4ef6\u200b\u8f83\u200b\u5bb9\u6613\u200b\u7684\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u5728\u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5e94\u7528\u200b\u3002</p> <p>\u200b\u8be5\u200b\u9879\u76ee\u200b\u4ee5\u200b\u56fd\u5bb6\u200b\u8d28\u91cf\u200b\u57fa\u7840\u200b\uff08NQI\uff09\u200b\u4e3a\u200b\u51c6\u7ef3\u200b\uff0c\u200b\u5145\u5206\u5229\u7528\u200b\u5927\u200b\u6570\u636e\u200b\u3001\u200b\u4e91\u200b\u8ba1\u7b97\u200b\u3001\u200b\u7269\u200b\u8054\u7f51\u200b\u7b49\u200b\u9ad8\u65b0\u6280\u672f\u200b\uff0c\u200b\u6784\u5efa\u200b\u8986\u76d6\u200b\u8ba1\u91cf\u200b\u7aef\u200b\u3001\u200b\u5b9e\u9a8c\u5ba4\u200b\u7aef\u200b\u3001\u200b\u6570\u636e\u200b\u7aef\u200b\u548c\u200b\u786c\u4ef6\u200b\u7aef\u7684\u200b\u5b8c\u6574\u200b\u8ba1\u91cf\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff0c\u200b\u89e3\u51b3\u200b\u4f20\u7edf\u200b\u8ba1\u91cf\u200b\u6821\u51c6\u200b\u4e2d\u200b\u5b58\u5728\u200b\u7684\u200b\u96be\u9898\u200b\uff0c\u200b\u62d3\u5bbd\u200b\u8ba1\u91cf\u200b\u68c0\u6d4b\u200b\u670d\u52a1\u4f53\u7cfb\u200b\u548c\u200b\u670d\u52a1\u200b\u9886\u57df\u200b\uff1b\u200b\u89e3\u51b3\u200b\u65e0\u6570\u200b\u4f20\u200b\u63a5\u53e3\u200b\u6216\u200b\u6570\u4f20\u200b\u63a5\u53e3\u200b\u4e0d\u200b\u7edf\u4e00\u200b\u3001\u200b\u4e0d\u200b\u516c\u5f00\u200b\u7684\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\u6240\u5904\u200b\u7684\u200b\u73af\u5883\u200b\u6bd4\u8f83\u200b\u6076\u52a3\u200b\uff0c\u200b\u4e0d\u200b\u9002\u5408\u200b\u4eba\u5de5\u200b\u8bfb\u53d6\u6570\u636e\u200b\u3002\u200b\u901a\u8fc7\u200bOCR\u200b\u6280\u672f\u200b\u5b9e\u73b0\u200b\u8fdc\u7a0b\u200b\u8ba1\u91cf\u200b\uff0c\u200b\u5f15\u9886\u200b\u8ba1\u91cf\u200b\u884c\u4e1a\u200b\u5411\u200b\u667a\u6167\u200b\u8ba1\u91cf\u200b\u8f6c\u578b\u200b\u548c\u200b\u53d1\u5c55\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u9879\u76ee\u200b\u5185\u5bb9","text":"<p>\u200b\u672c\u200b\u9879\u76ee\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u5f00\u6e90\u200b\u5957\u4ef6\u200b\uff0c\u200b\u4ee5\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u4e3a\u200b\u57fa\u7840\u200b\uff0c\u200b\u9488\u5bf9\u200b\u6db2\u6676\u5c4f\u200b\u8bfb\u6570\u200b\u8bc6\u522b\u200b\u573a\u666f\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u3002</p> <p>Aistudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b\uff1aOCR\u200b\u6db2\u6676\u5c4f\u200b\u8bfb\u6570\u200b\u8bc6\u522b\u200b</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u5b89\u88c5\u200b\u73af\u5883","text":"<pre><code># \u200b\u9996\u5148\u200bgit\u200b\u5b98\u65b9\u200b\u7684\u200bPaddleOCR\u200b\u9879\u76ee\u200b\uff0c\u200b\u5b89\u88c5\u200b\u9700\u8981\u200b\u7684\u200b\u4f9d\u8d56\u200b\n# \u200b\u7b2c\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u6253\u5f00\u200b\u8be5\u200b\u6ce8\u91ca\u200b\n# git clone https://gitee.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npip install -r requirements.txt\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#4","title":"4. \u200b\u6587\u5b57\u200b\u68c0\u6d4b","text":"<p>\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u5b9a\u4f4d\u200b\u51fa\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u3002\u200b\u8fd1\u5e74\u6765\u200b\u5b66\u672f\u754c\u200b\u5173\u4e8e\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u7684\u200b\u7814\u7a76\u200b\u975e\u5e38\u200b\u4e30\u5bcc\u200b\uff0c\u200b\u4e00\u7c7b\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u89c6\u4e3a\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u573a\u666f\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u901a\u7528\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u8fdb\u884c\u200b\u6539\u8fdb\u200b\u9002\u914d\u200b\uff0c\u200b\u5982\u200bTextBoxes[1]\u200b\u57fa\u4e8e\u200b\u4e00\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u5668\u200bSSD[2]\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u8c03\u6574\u200b\u76ee\u6807\u200b\u6846\u4f7f\u200b\u4e4b\u200b\u9002\u5408\u200b\u6781\u7aef\u200b\u957f\u5bbd\u200b\u6bd4\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\uff0cCTPN[3]\u200b\u5219\u200b\u662f\u200b\u57fa\u4e8e\u200bFaster RCNN[4]\u200b\u67b6\u6784\u200b\u6539\u8fdb\u200b\u800c\u200b\u6765\u200b\u3002\u200b\u4f46\u662f\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5728\u200b\u76ee\u6807\u200b\u4fe1\u606f\u200b\u4ee5\u53ca\u200b\u4efb\u52a1\u200b\u672c\u8eab\u200b\u4e0a\u200b\u4ecd\u200b\u5b58\u5728\u200b\u4e00\u4e9b\u200b\u533a\u522b\u200b\uff0c\u200b\u5982\u200b\u6587\u672c\u200b\u4e00\u822c\u200b\u957f\u5bbd\u200b\u6bd4\u8f83\u200b\u5927\u200b\uff0c\u200b\u5f80\u5f80\u200b\u5448\u200b\u201c\u200b\u6761\u72b6\u200b\u201d\uff0c\u200b\u6587\u672c\u200b\u884c\u200b\u4e4b\u95f4\u200b\u53ef\u80fd\u200b\u6bd4\u8f83\u200b\u5bc6\u96c6\u200b\uff0c\u200b\u5f2f\u66f2\u200b\u6587\u672c\u200b\u7b49\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53c8\u200b\u884d\u751f\u200b\u4e86\u200b\u5f88\u591a\u200b\u4e13\u7528\u200b\u4e8e\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u7684\u200b\u7b97\u6cd5\u200b\u3002\u200b\u672c\u200b\u9879\u76ee\u200b\u57fa\u4e8e\u200bPP-OCRv3\u200b\u7b97\u6cd5\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#41-pp-ocrv3","title":"4.1 PP-OCRv3\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd","text":"<p>PP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u662f\u200b\u5bf9\u200bPP-OCRv2\u200b\u4e2d\u200b\u7684\u200bCML\uff08Collaborative Mutual Learning) \u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cCML\u200b\u7684\u200b\u6838\u5fc3\u601d\u60f3\u200b\u7ed3\u5408\u200b\u4e86\u200b\u2460\u200b\u4f20\u7edf\u200b\u7684\u200bTeacher\u200b\u6307\u5bfc\u200bStudent\u200b\u7684\u200b\u6807\u51c6\u200b\u84b8\u998f\u200b\u4e0e\u200b \u2461Students\u200b\u7f51\u7edc\u200b\u4e4b\u95f4\u200b\u7684\u200bDML\u200b\u4e92\u200b\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200bStudents\u200b\u7f51\u7edc\u200b\u4e92\u200b\u5b66\u4e60\u200b\u7684\u200b\u540c\u65f6\u200b\uff0cTeacher\u200b\u7f51\u7edc\u200b\u4e88\u4ee5\u200b\u6307\u5bfc\u200b\u3002PP-OCRv3\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u6548\u679c\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u5728\u200b\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200bLK-PAN\u200b\u548c\u200b\u5f15\u5165\u200b\u4e86\u200bDML\uff08Deep Mutual Learning\uff09\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff1b\u200b\u5728\u200b\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200bRSE-FPN\u3002 </p> <p>\u200b\u8be6\u7ec6\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u63cf\u8ff0\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv3\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#42","title":"4.2 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\u5c4f\u5e55\u200b\u5b57\u7b26\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\u6570\u636e\u200b\u6765\u6e90\u4e8e\u200b\u5b9e\u9645\u200b\u9879\u76ee\u200b\u4e2d\u200b\u5404\u79cd\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\u7684\u200b\u6570\u663e\u200b\u5c4f\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5728\u200b\u7f51\u4e0a\u200b\u641c\u96c6\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u6570\u663e\u200b\u5c4f\u200b\uff0c\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u96c6\u200b755\u200b\u5f20\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b355\u200b\u5f20\u200b\u3002</p> <pre><code># \u200b\u5728\u200bPaddleOCR\u200b\u4e0b\u200b\u521b\u5efa\u200b\u65b0\u200b\u7684\u200b\u6587\u4ef6\u5939\u200btrain_data\nmkdir train_data\n# \u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\u5e76\u200b\u89e3\u538b\u200b\u5230\u200b\u6307\u5b9a\u200b\u8def\u5f84\u200b\u4e0b\u200b\nunzip icdar2015.zip  -d train_data\n</code></pre> <pre><code># \u200b\u968f\u673a\u200b\u67e5\u770b\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\u56fe\u7247\u200b\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\ntrain = './train_data/icdar2015/text_localization/test'\n# \u200b\u4ece\u200b\u6307\u5b9a\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u9009\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\ndef get_one_image(train):\n    plt.figure()\n    files = os.listdir(train)\n    n = len(files)\n    ind = np.random.randint(0,n)\n    img_dir = os.path.join(train,files[ind])\n    image = Image.open(img_dir)\n    plt.imshow(image)\n    plt.show()\n    image = image.resize([208, 208])\n\nget_one_image(train)\n</code></pre> <p></p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#43","title":"4.3 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":""},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#431","title":"4.3.1 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u8bc4\u4f30","text":"<p>\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u9009\u62e9\u200b\u8bf7\u200b\u81ea\u884c\u200b\u9009\u62e9\u200b\u5176\u4ed6\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b</p> <pre><code>#\u200b\u4f7f\u7528\u200b\u8be5\u200b\u6307\u4ee4\u200b\u4e0b\u8f7d\u200b\u9700\u8981\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget -P ./pretrained_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\n# \u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\ntar -xf ./pretrained_models/ch_PP-OCRv3_det_distill_train.tar -C pretrained_models\n</code></pre> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u6765\u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u6548\u679c\u200b:</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model=\"./pretrained_models/ch_PP-OCRv3_det_distill_train/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 47.50%"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#432-finetune","title":"4.3.2 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200bfinetune","text":""},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_1","title":"\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bconfigs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u53c2\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\u3002 \u200b\u53e6\u5916\u200b\uff0cbatch_size\u200b\u53ef\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u663e\u5b58\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002 \u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff1a</p> <pre><code>epoch:100\nsave_epoch_step:10\neval_batch_step:[0, 50]\nsave_model_dir: ./output/ch_PP-OCR_v3_det/\npretrained_model: ./pretrained_models/ch_PP-OCRv3_det_distill_train/best_accuracy\nlearning_rate: 0.00025\nnum_workers: 0 # \u200b\u5982\u679c\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5efa\u8bae\u200b\u5c06\u200bTrain\u200b\u548c\u200bEval\u200b\u7684\u200bloader\u200b\u90e8\u5206\u200b\u7684\u200bnum_workers\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b`/dev/shm insufficient`\u200b\u7684\u200b\u62a5\u9519\u200b\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_2","title":"\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u4fee\u6539\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200bconfigs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml\uff0c\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model=./pretrained_models/ch_PP-OCRv3_det_distill_train/best_accuracy\n</code></pre> <p>\u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_det/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 47.50% 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune 65.20%"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#433-finetune_student","title":"4.3.3 \u200b\u57fa\u4e8e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bFinetune_student\u200b\u6a21\u578b","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bconfigs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u53c2\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\u3002 \u200b\u53e6\u5916\u200b\uff0cbatch_size\u200b\u53ef\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u663e\u5b58\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002 \u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff1a</p> <pre><code>epoch:100\nsave_epoch_step:10\neval_batch_step:[0, 50]\nsave_model_dir: ./output/ch_PP-OCR_v3_det_student/\npretrained_model: ./pretrained_models/ch_PP-OCRv3_det_distill_train/student\nlearning_rate: 0.00025\nnum_workers: 0 # \u200b\u5982\u679c\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5efa\u8bae\u200b\u5c06\u200bTrain\u200b\u548c\u200bEval\u200b\u7684\u200bloader\u200b\u90e8\u5206\u200b\u7684\u200bnum_workers\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b`/dev/shm insufficient`\u200b\u7684\u200b\u62a5\u9519\u200b\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o Global.pretrained_model=./pretrained_models/ch_PP-OCRv3_det_distill_train/student\n</code></pre> <p>\u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_det_student/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 47.50% 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune 65.20% 2 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b 80.00%"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#434-finetune_teacher","title":"4.3.4 \u200b\u57fa\u4e8e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bFinetune_teacher\u200b\u6a21\u578b","text":"<p>\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u4ece\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bbest_accuracy.pdparams\u200b\u4e2d\u200b\u63d0\u53d6\u200bteacher\u200b\u53c2\u6570\u200b\uff0c\u200b\u7ec4\u5408\u6210\u200b\u9002\u5408\u200bdml\u200b\u8bad\u7ec3\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\uff0c\u200b\u63d0\u53d6\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>cd ./pretrained_models/\n# transform teacher params in best_accuracy.pdparams into teacher_dml.paramers\nimport paddle\n\n# load pretrained model\nall_params = paddle.load(\"ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# print(all_params.keys())\n\n# keep teacher params\nt_params = {key[len(\"Teacher.\"):]: all_params[key] for key in all_params if \"Teacher.\" in key}\n\n# print(t_params.keys())\n\ns_params = {\"Student.\" + key: t_params[key] for key in t_params}\ns2_params = {\"Student2.\" + key: t_params[key] for key in t_params}\ns_params = {**s_params, **s2_params}\n# print(s_params.keys())\n\npaddle.save(s_params, \"ch_PP-OCRv3_det_distill_train/teacher_dml.pdparams\")\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bconfigs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u53c2\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\u3002 \u200b\u53e6\u5916\u200b\uff0cbatch_size\u200b\u53ef\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u663e\u5b58\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002 \u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff1a</p> <pre><code>epoch:100\nsave_epoch_step:10\neval_batch_step:[0, 50]\nsave_model_dir: ./output/ch_PP-OCR_v3_det_teacher/\npretrained_model: ./pretrained_models/ch_PP-OCRv3_det_distill_train/teacher_dml\nlearning_rate: 0.00025\nnum_workers: 0 # \u200b\u5982\u679c\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5efa\u8bae\u200b\u5c06\u200bTrain\u200b\u548c\u200bEval\u200b\u7684\u200bloader\u200b\u90e8\u5206\u200b\u7684\u200bnum_workers\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b`/dev/shm insufficient`\u200b\u7684\u200b\u62a5\u9519\u200b\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml -o Global.pretrained_model=./pretrained_models/ch_PP-OCRv3_det_distill_train/teacher_dml\n</code></pre> <p>\u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_det_teacher/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 47.50% 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune 65.20% 2 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b 80.00% 3 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b 84.80%"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#435-cmlstudent","title":"4.3.5 \u200b\u91c7\u7528\u200bCML\u200b\u84b8\u998f\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200bstudent\u200b\u6a21\u578b\u200b\u7cbe\u5ea6","text":"<p>\u200b\u9700\u8981\u200b\u4ece\u200b4.3.3\u200b\u548c\u200b4.3.4\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200bbest_accuracy.pdparams\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u5404\u81ea\u200b\u4ee3\u8868\u200bstudent\u200b\u548c\u200bteacher\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u7ec4\u5408\u6210\u200b\u9002\u5408\u200bcml\u200b\u8bad\u7ec3\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\uff0c\u200b\u63d0\u53d6\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># transform teacher params and student parameters into cml model\nimport paddle\n\nall_params = paddle.load(\"./pretrained_models/ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# print(all_params.keys())\n\nt_params = paddle.load(\"./output/ch_PP-OCR_v3_det_teacher/best_accuracy.pdparams\")\n# print(t_params.keys())\n\ns_params = paddle.load(\"./output/ch_PP-OCR_v3_det_student/best_accuracy.pdparams\")\n# print(s_params.keys())\n\nfor key in all_params:\n    # teacher is OK\n    if \"Teacher.\" in key:\n        new_key = key.replace(\"Teacher\", \"Student\")\n        #print(\"{} &gt;&gt; {}\\n\".format(key, new_key))\n        assert all_params[key].shape == t_params[new_key].shape\n        all_params[key] = t_params[new_key]\n\n    if \"Student.\" in key:\n        new_key = key.replace(\"Student.\", \"\")\n        #print(\"{} &gt;&gt; {}\\n\".format(key, new_key))\n        assert all_params[key].shape == s_params[new_key].shape\n        all_params[key] = s_params[new_key]\n\n    if \"Student2.\" in key:\n        new_key = key.replace(\"Student2.\", \"\")\n        print(\"{} &gt;&gt; {}\\n\".format(key, new_key))\n        assert all_params[key].shape == s_params[new_key].shape\n        all_params[key] = s_params[new_key]\n\npaddle.save(all_params, \"./pretrained_models/ch_PP-OCRv3_det_distill_train/teacher_cml_student.pdparams\")\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model=./pretrained_models/ch_PP-OCRv3_det_distill_train/teacher_cml_student Global.save_model_dir=./output/ch_PP-OCR_v3_det_finetune/\n</code></pre> <p>\u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_det_finetune/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 47.50% 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune 65.20% 2 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b 80.00% 3 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfintune\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b 84.80% 4 \u200b\u57fa\u4e8e\u200b2\u200b\u548c\u200b3\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200bfintune 82.70% <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#436","title":"4.3.6 \u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u63a8\u7406","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8f6c\u6362\u6210\u200binference\u200b\u6a21\u578b\u200b\u3002inference \u200b\u6a21\u578b\u200b\u4f1a\u200b\u989d\u5916\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u3001\u200b\u52a0\u901f\u200b\u63a8\u7406\u200b\u4e0a\u200b\u6027\u80fd\u4f18\u8d8a\u200b\uff0c\u200b\u7075\u6d3b\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u9002\u5408\u200b\u4e8e\u200b\u5b9e\u9645\u200b\u7cfb\u7edf\u96c6\u6210\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#4361","title":"4.3.6.1 \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u5bfc\u51fa\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u8f6c\u5316\u200b\u4e3a\u200b\u63a8\u7406\u6a21\u578b\u200b\npython tools/export_model.py \\\n-c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml \\\n-o Global.pretrained_model=./output/ch_PP-OCR_v3_det_finetune/best_accuracy \\\n-o Global.save_inference_dir=\"./inference/det_ppocrv3\"\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#4362","title":"4.3.6.2 \u200b\u6a21\u578b\u200b\u63a8\u7406","text":"<p>\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code># \u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b\npython tools/infer/predict_det.py --image_dir=\"train_data/icdar2015/text_localization/test/1.jpg\" --det_model_dir=\"./inference/det_ppocrv3/Student\"\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#5","title":"5. \u200b\u6587\u5b57\u200b\u8bc6\u522b","text":"<p>\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u8bc6\u522b\u200b\u51fa\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u6587\u5b57\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4e00\u822c\u200b\u8f93\u5165\u200b\u6765\u81ea\u200b\u4e8e\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u5f97\u5230\u200b\u7684\u200b\u6587\u672c\u6846\u200b\u622a\u200b\u53d6\u51fa\u200b\u7684\u200b\u56fe\u50cf\u200b\u6587\u5b57\u200b\u533a\u57df\u200b\u3002\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4e00\u822c\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u5f85\u200b\u8bc6\u522b\u200b\u6587\u672c\u200b\u5f62\u72b6\u200b\u5206\u4e3a\u200b\u89c4\u5219\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u548c\u200b\u4e0d\u89c4\u5219\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4e24\u5927\u7c7b\u200b\u3002\u200b\u89c4\u5219\u200b\u6587\u672c\u200b\u4e3b\u8981\u200b\u6307\u200b\u5370\u5237\u200b\u5b57\u4f53\u200b\u3001\u200b\u626b\u63cf\u200b\u6587\u672c\u200b\u7b49\u200b\uff0c\u200b\u6587\u672c\u200b\u5927\u81f4\u200b\u5904\u5728\u200b\u6c34\u5e73\u7ebf\u200b\u4f4d\u7f6e\u200b\uff1b\u200b\u4e0d\u89c4\u5219\u200b\u6587\u672c\u200b\u5f80\u5f80\u200b\u4e0d\u200b\u5728\u200b\u6c34\u5e73\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u5b58\u5728\u200b\u5f2f\u66f2\u200b\u3001\u200b\u906e\u6321\u200b\u3001\u200b\u6a21\u7cca\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002\u200b\u4e0d\u89c4\u5219\u200b\u6587\u672c\u200b\u573a\u666f\u200b\u5177\u6709\u200b\u5f88\u5927\u200b\u7684\u200b\u6311\u6218\u6027\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u76ee\u524d\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u9886\u57df\u200b\u7684\u200b\u4e3b\u8981\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u3002\u200b\u672c\u200b\u9879\u76ee\u200b\u57fa\u4e8e\u200bPP-OCRv3\u200b\u7b97\u6cd5\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#51-pp-ocrv3","title":"5.1 PP-OCRv3\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b\u4ecb\u7ecd","text":"<p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPP-OCRv3\u200b\u91c7\u7528\u200b\u4e86\u200b6\u200b\u4e2a\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u3002 </p> <p>\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u6c47\u603b\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b</li> <li>GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b</li> <li>TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b</li> <li>TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</li> <li>UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b</li> <li>UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b</li> </ul> <p>\u200b\u8be6\u7ec6\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u63cf\u8ff0\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv3\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#52","title":"5.2 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\u5c4f\u5e55\u200b\u5b57\u7b26\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\u6570\u636e\u200b\u6765\u6e90\u4e8e\u200b\u5b9e\u9645\u200b\u9879\u76ee\u200b\u4e2d\u200b\u5404\u79cd\u200b\u8ba1\u91cf\u200b\u8bbe\u5907\u200b\u7684\u200b\u6570\u663e\u200b\u5c4f\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5728\u200b\u7f51\u4e0a\u200b\u641c\u96c6\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u6570\u663e\u200b\u5c4f\u200b\uff0c\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u96c6\u200b19912\u200b\u5f20\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b4099\u200b\u5f20\u200b\u3002</p> <pre><code># \u200b\u89e3\u538b\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u5230\u200b\u6307\u5b9a\u200b\u8def\u5f84\u200b\u4e0b\u200b\nunzip ic15_data.zip -d train_data\n</code></pre> <pre><code># \u200b\u968f\u673a\u200b\u67e5\u770b\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\u56fe\u7247\u200b\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ntrain = './train_data/ic15_data/train'\n# \u200b\u4ece\u200b\u6307\u5b9a\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u9009\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\ndef get_one_image(train):\n    plt.figure()\n    files = os.listdir(train)\n    n = len(files)\n    ind = np.random.randint(0,n)\n    img_dir = os.path.join(train,files[ind])\n    image = Image.open(img_dir)\n    plt.imshow(image)\n    plt.show()\n    image = image.resize([208, 208])\n\nget_one_image(train)\n</code></pre> <p></p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#53","title":"5.3 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":""},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_3","title":"\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u9009\u62e9\u200b\u8bf7\u200b\u81ea\u884c\u200b\u9009\u62e9\u200b\u5176\u4ed6\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</p> <pre><code># \u200b\u4f7f\u7528\u200b\u8be5\u200b\u6307\u4ee4\u200b\u4e0b\u8f7d\u200b\u9700\u8981\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget -P ./pretrained_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\n# \u200b\u89e3\u538b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\ntar -xf ./pretrained_models/ch_PP-OCRv3_rec_train.tar -C pretrained_models\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_4","title":"\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bconfigs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml\uff0c\u200b\u4e3b\u8981\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u53c2\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\u3002 \u200b\u53e6\u5916\u200b\uff0cbatch_size\u200b\u53ef\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u663e\u5b58\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002 \u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff1a</p> <pre><code>  epoch_num: 100 # \u200b\u8bad\u7ec3\u200bepoch\u200b\u6570\u200b\n  save_model_dir: ./output/ch_PP-OCR_v3_rec\n  save_epoch_step: 10\n  eval_batch_step: [0, 100] # \u200b\u8bc4\u4f30\u200b\u95f4\u9694\u200b\uff0c\u200b\u6bcf\u9694\u200b100step\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\n  cal_metric_during_train: true\n  pretrained_model: ./pretrained_models/ch_PP-OCRv3_rec_train/best_accuracy  # \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\n  character_dict_path: ppocr/utils/ppocr_keys_v1.txt\n  use_space_char: true  # \u200b\u4f7f\u7528\u200b\u7a7a\u683c\u200b\n\n  lr:\n    name: Cosine # \u200b\u4fee\u6539\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u7b56\u7565\u200b\u4e3a\u200bCosine\n    learning_rate: 0.0002 # \u200b\u4fee\u6539\u200bfine-tune\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\n    warmup_epoch: 2 # \u200b\u4fee\u6539\u200bwarmup\u200b\u8f6e\u6570\u200b\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/ic15_data/ # \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\n    ext_op_transform_idx: 1\n    label_file_list:\n    - ./train_data/ic15_data/rec_gt_train.txt # \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u7b7e\u200b\n    ratio_list:\n    - 1.0\n  loader:\n    shuffle: true\n    batch_size_per_card: 64\n    drop_last: true\n    num_workers: 4\nEval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/ic15_data/ # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u8def\u5f84\u200b\n    label_file_list:\n    - ./train_data/ic15_data/rec_gt_test.txt # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u7b7e\u200b\n    ratio_list:\n    - 1.0\n  loader:\n    shuffle: false\n    drop_last: false\n    batch_size_per_card: 64\n    num_workers: 4\n</code></pre> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u6765\u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u6548\u679c\u200b:</p> <pre><code># \u200b\u8bc4\u4f30\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=\"./pretrained_models/ch_PP-OCRv3_rec_train/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b accuracy 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 70.40%"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_5","title":"\u5f00\u59cb\u200b\u8bad\u7ec3","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u4fee\u6539\u200b\u597d\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200bconfigs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u8def\u5f84\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u7b49\u200b\u90fd\u200b\u5df2\u7ecf\u200b\u8bbe\u7f6e\u200b\u5b8c\u6bd5\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u547d\u4ee4\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u3002</p> <pre><code># \u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\npython tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e2d\u200b\u6700\u597d\u200b\u7684\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u8bc4\u4f30\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u8bc4\u4f30\u200bfinetune\u200b\u6548\u679c\u200b\npython tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.checkpoints=\"./output/ch_PP-OCR_v3_rec/best_accuracy\"\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b accuracy 0 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 70.40% 1 PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bfinetune 82.20% <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u626b\u7801\u200b\u586b\u5199\u200b\u95ee\u5377\u200b\uff0c\u200b\u52a0\u5165\u200bPaddleOCR\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u7fa4\u200b\u83b7\u53d6\u200b\u5168\u90e8\u200bOCR\u200b\u5782\u7c7b\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\u3001\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u7b49\u200b\u5168\u5957\u200bOCR\u200b\u5b66\u4e60\u200b\u8d44\u6599\u200b\ud83c\udf81</p> <p></p> <p>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u7f6e\u200b\u5728\u200b\u5bf9\u5e94\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#54","title":"5.4 \u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u63a8\u7406","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8f6c\u6362\u6210\u200binference\u200b\u6a21\u578b\u200b\u3002inference \u200b\u6a21\u578b\u200b\u4f1a\u200b\u989d\u5916\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u3001\u200b\u52a0\u901f\u200b\u63a8\u7406\u200b\u4e0a\u200b\u6027\u80fd\u4f18\u8d8a\u200b\uff0c\u200b\u7075\u6d3b\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u9002\u5408\u200b\u4e8e\u200b\u5b9e\u9645\u200b\u7cfb\u7edf\u96c6\u6210\u200b\u3002</p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_6","title":"\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u5bfc\u51fa\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u8f6c\u5316\u200b\u4e3a\u200b\u63a8\u7406\u6a21\u578b\u200b\npython tools/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=\"./output/ch_PP-OCR_v3_rec/best_accuracy\" Global.save_inference_dir=\"./inference/rec_ppocrv3/\"\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#_7","title":"\u6a21\u578b\u200b\u63a8\u7406","text":"<p>\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b</p> <pre><code># \u200b\u63a8\u7406\u200b\u9884\u6d4b\u200b\npython tools/infer/predict_rec.py --image_dir=\"train_data/ic15_data/test/1_crop_0.jpg\" --rec_model_dir=\"./inference/rec_ppocrv3/Student\"\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#6","title":"6. \u200b\u7cfb\u7edf\u200b\u4e32\u8054","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e0a\u9762\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u7cfb\u7edf\u200b\u4e32\u8054\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>#\u200b\u4e32\u8054\u200b\u6d4b\u8bd5\u200b\npython3 tools/infer/predict_system.py --image_dir=\"./train_data/icdar2015/text_localization/test/142.jpg\" --det_model_dir=\"./inference/det_ppocrv3/Student\"  --rec_model_dir=\"./inference/rec_ppocrv3/Student\"\n</code></pre> <p>\u200b\u6d4b\u8bd5\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>./inference_results/</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e0b\u9762\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b</p> <pre><code>%cd /home/aistudio/PaddleOCR\n# \u200b\u663e\u793a\u200b\u7ed3\u679c\u200b\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimg_path= \"./inference_results/142.jpg\"\nimg = Image.open(img_path)\nplt.figure(\"test_img\", figsize=(30,30))\nplt.imshow(img)\nplt.show()\n</code></pre> <p></p>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#61","title":"6.1 \u200b\u540e\u5904\u7406","text":"<p>\u200b\u5982\u679c\u200b\u9700\u8981\u200b\u83b7\u53d6\u200bkey-value\u200b\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u57fa\u4e8e\u200b\u542f\u53d1\u5f0f\u200b\u7684\u200b\u89c4\u5219\u200b\uff0c\u200b\u5c06\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u5173\u952e\u5b57\u200b\u5e93\u200b\u8fdb\u884c\u200b\u5339\u914d\u200b\uff1b\u200b\u5982\u679c\u200b\u5339\u914d\u200b\u4e0a\u200b\u4e86\u200b\uff0c\u200b\u5219\u200b\u53d6\u8be5\u200b\u5b57\u200b\u6bb5\u200b\u4e3a\u200bkey, \u200b\u540e\u9762\u200b\u4e00\u4e2a\u200b\u5b57\u6bb5\u200b\u4e3a\u200bvalue\u3002</p> <pre><code>def postprocess(rec_res):\n    keys = [\"\u200b\u578b\u53f7\u200b\", \"\u200b\u5382\u5bb6\u200b\", \"\u200b\u7248\u672c\u53f7\u200b\", \"\u200b\u68c0\u5b9a\u200b\u6821\u51c6\u200b\u5206\u7c7b\u200b\", \"\u200b\u8ba1\u91cf\u200b\u5668\u5177\u200b\u7f16\u53f7\u200b\", \"\u200b\u70df\u5c18\u200b\u6d41\u91cf\u200b\",\n            \"\u200b\u7d2f\u79ef\u200b\u4f53\u79ef\u200b\", \"\u200b\u70df\u6c14\u200b\u6e29\u5ea6\u200b\", \"\u200b\u52a8\u538b\u200b\", \"\u200b\u9759\u538b\u200b\", \"\u200b\u65f6\u95f4\u200b\", \"\u200b\u8bd5\u9a8c\u53f0\u200b\u7f16\u53f7\u200b\", \"\u200b\u9884\u6d4b\u200b\u6d41\u901f\u200b\",\n            \"\u200b\u5168\u538b\u200b\", \"\u200b\u70df\u6e29\u200b\", \"\u200b\u6d41\u901f\u200b\", \"\u200b\u5de5\u51b5\u200b\u6d41\u91cf\u200b\", \"\u200b\u6807\u6746\u200b\u6d41\u91cf\u200b\", \"\u200b\u70df\u5c18\u200b\u76f4\u8bfb\u200b\u5634\u200b\", \"\u200b\u70df\u5c18\u200b\u91c7\u6837\u200b\u5634\u200b\",\n            \"\u200b\u5927\u6c14\u538b\u200b\", \"\u200b\u8ba1\u524d\u200b\u6e29\u5ea6\u200b\", \"\u200b\u8ba1\u524d\u200b\u538b\u529b\u200b\", \"\u200b\u5e72\u7403\u6e29\u5ea6\u200b\", \"\u200b\u6e7f\u7403\u6e29\u5ea6\u200b\", \"\u200b\u6d41\u91cf\u200b\", \"\u200b\u542b\u6e7f\u91cf\u200b\"]\n    key_value = []\n    if len(rec_res) &gt; 1:\n        for i in range(len(rec_res) - 1):\n            rec_str, _ = rec_res[i]\n            for key in keys:\n                if rec_str in key:\n                    key_value.append([rec_str, rec_res[i + 1][0]])\n                    break\n    return key_value\nkey_value = postprocess(filter_rec_res)\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#7-paddleserving","title":"7. PaddleServing\u200b\u90e8\u7f72","text":"<p>\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u5b89\u88c5\u200bPaddleServing\u200b\u90e8\u7f72\u200b\u76f8\u5173\u200b\u7684\u200b\u73af\u5883\u200b</p> <pre><code>python -m pip install paddle-serving-server-gpu\npython -m pip install paddle_serving_client\npython -m pip install paddle-serving-app\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#71","title":"7.1 \u200b\u8f6c\u5316\u200b\u68c0\u6d4b\u200b\u6a21\u578b","text":"<pre><code>cd deploy/pdserving/\npython -m paddle_serving_client.convert --dirname ../../inference/det_ppocrv3/Student/  \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_det_v3_serving/ \\\n                                         --serving_client ./ppocr_det_v3_client/\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#72","title":"7.2 \u200b\u8f6c\u5316\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":"<pre><code>python -m paddle_serving_client.convert --dirname ../../inference/rec_ppocrv3/Student \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_rec_v3_serving/ \\\n                                         --serving_client ./ppocr_rec_v3_client/\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#73","title":"7.3 \u200b\u542f\u52a8\u200b\u670d\u52a1","text":"<p>\u200b\u9996\u5148\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u540e\u5904\u7406\u200b\u4ee3\u7801\u200b\u52a0\u5165\u200b\u5230\u200bweb_service.py\u200b\u4e2d\u200b\uff0c\u200b\u5177\u4f53\u200b\u4fee\u6539\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u4ee3\u7801\u200b153\u200b\u884c\u200b\u540e\u9762\u200b\u589e\u52a0\u200b\u4e0b\u9762\u200b\u4ee3\u7801\u200b\ndef _postprocess(rec_res):\n    keys = [\"\u200b\u578b\u53f7\u200b\", \"\u200b\u5382\u5bb6\u200b\", \"\u200b\u7248\u672c\u53f7\u200b\", \"\u200b\u68c0\u5b9a\u200b\u6821\u51c6\u200b\u5206\u7c7b\u200b\", \"\u200b\u8ba1\u91cf\u200b\u5668\u5177\u200b\u7f16\u53f7\u200b\", \"\u200b\u70df\u5c18\u200b\u6d41\u91cf\u200b\",\n            \"\u200b\u7d2f\u79ef\u200b\u4f53\u79ef\u200b\", \"\u200b\u70df\u6c14\u200b\u6e29\u5ea6\u200b\", \"\u200b\u52a8\u538b\u200b\", \"\u200b\u9759\u538b\u200b\", \"\u200b\u65f6\u95f4\u200b\", \"\u200b\u8bd5\u9a8c\u53f0\u200b\u7f16\u53f7\u200b\", \"\u200b\u9884\u6d4b\u200b\u6d41\u901f\u200b\",\n            \"\u200b\u5168\u538b\u200b\", \"\u200b\u70df\u6e29\u200b\", \"\u200b\u6d41\u901f\u200b\", \"\u200b\u5de5\u51b5\u200b\u6d41\u91cf\u200b\", \"\u200b\u6807\u6746\u200b\u6d41\u91cf\u200b\", \"\u200b\u70df\u5c18\u200b\u76f4\u8bfb\u200b\u5634\u200b\", \"\u200b\u70df\u5c18\u200b\u91c7\u6837\u200b\u5634\u200b\",\n            \"\u200b\u5927\u6c14\u538b\u200b\", \"\u200b\u8ba1\u524d\u200b\u6e29\u5ea6\u200b\", \"\u200b\u8ba1\u524d\u200b\u538b\u529b\u200b\", \"\u200b\u5e72\u7403\u6e29\u5ea6\u200b\", \"\u200b\u6e7f\u7403\u6e29\u5ea6\u200b\", \"\u200b\u6d41\u91cf\u200b\", \"\u200b\u542b\u6e7f\u91cf\u200b\"]\n    key_value = []\n    if len(rec_res) &gt; 1:\n        for i in range(len(rec_res) - 1):\n            rec_str, _ = rec_res[i]\n            for key in keys:\n                if rec_str in key:\n                    key_value.append([rec_str, rec_res[i + 1][0]])\n                    break\n    return key_value\nkey_value = _postprocess(rec_list)\nres = {\"result\": str(key_value)}\n# res = {\"result\": str(result_list)}\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u670d\u52a1\u7aef\u200b</p> <pre><code>python web_service.py 2&gt;&amp;1 &gt;log.txt\n</code></pre>"},{"location":"en/applications/%E6%B6%B2%E6%99%B6%E5%B1%8F%E8%AF%BB%E6%95%B0%E8%AF%86%E5%88%AB.html#74","title":"7.4 \u200b\u53d1\u9001\u200b\u8bf7\u6c42","text":"<p>\u200b\u7136\u540e\u200b\u518d\u200b\u5f00\u542f\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u7ec8\u7aef\u200b\uff0c\u200b\u8fd0\u884c\u200b\u4e0b\u9762\u200b\u7684\u200b\u5ba2\u6237\u7aef\u200b\u4ee3\u7801\u200b</p> <pre><code>python pipeline_http_client.py --image_dir ../../train_data/icdar2015/text_localization/test/142.jpg\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u83b7\u53d6\u200b\u5230\u200b\u6700\u7ec8\u200b\u7684\u200bkey-value\u200b\u7ed3\u679c\u200b\uff1a</p> <pre><code>\u200b\u5927\u6c14\u538b\u200b, 100.07kPa\n\u200b\u5e72\u7403\u6e29\u5ea6\u200b, 0000\u2103\n\u200b\u8ba1\u524d\u200b\u6e29\u5ea6\u200b, 0000\u2103\n\u200b\u6e7f\u7403\u6e29\u5ea6\u200b, 0000\u2103\n\u200b\u8ba1\u524d\u200b\u538b\u529b\u200b, -0000kPa\n\u200b\u6d41\u91cf\u200b, 00.0L/min\n\u200b\u9759\u538b\u200b, 00000kPa\n\u200b\u542b\u6e7f\u91cf\u200b, 00.0 %\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html","title":"\u4e00\u79cd\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#1","title":"1. \u200b\u9879\u76ee\u200b\u4ecb\u7ecd","text":"<p>\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b(Vehicle License Plate Recognition\uff0cVLPR) \u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u9891\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u6280\u672f\u200b\u5728\u200b\u8f66\u8f86\u200b\u724c\u7167\u200b\u8bc6\u522b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u79cd\u200b\u5e94\u7528\u200b\u3002\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b\u6280\u672f\u200b\u8981\u6c42\u200b\u80fd\u591f\u200b\u5c06\u200b\u8fd0\u52a8\u200b\u4e2d\u200b\u7684\u200b\u6c7d\u8f66\u200b\u724c\u7167\u200b\u4ece\u200b\u590d\u6742\u200b\u80cc\u666f\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u5e76\u200b\u8bc6\u522b\u200b\u51fa\u6765\u200b\uff0c\u200b\u5728\u200b\u9ad8\u901f\u516c\u8def\u200b\u8f66\u8f86\u7ba1\u7406\u200b\uff0c\u200b\u505c\u8f66\u573a\u200b\u7ba1\u7406\u200b\u548c\u200b\u57ce\u5e02\u4ea4\u901a\u200b\u4e2d\u200b\u5f97\u5230\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u3002</p> <p>\u200b\u672c\u200b\u9879\u76ee\u200b\u96be\u70b9\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u8f66\u724c\u200b\u5728\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u5c3a\u5ea6\u200b\u5dee\u5f02\u200b\u5927\u200b\u3001\u200b\u5728\u200b\u8f66\u8f86\u200b\u4e0a\u200b\u7684\u200b\u60ac\u6302\u200b\u4f4d\u7f6e\u200b\u4e0d\u200b\u56fa\u5b9a\u200b</li> <li>\u200b\u8f66\u724c\u200b\u56fe\u50cf\u200b\u8d28\u91cf\u200b\u5c42\u6b21\u200b\u4e0d\u9f50\u200b: \u200b\u89d2\u5ea6\u200b\u503e\u659c\u200b\u3001\u200b\u56fe\u7247\u200b\u6a21\u7cca\u200b\u3001\u200b\u5149\u7167\u200b\u4e0d\u8db3\u200b\u3001\u200b\u8fc7\u200b\u66dd\u200b\u7b49\u200b\u95ee\u9898\u200b\u4e25\u91cd\u200b</li> <li>\u200b\u8fb9\u7f18\u200b\u548c\u200b\u7aef\u6d4b\u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u5bf9\u6a21\u578b\u200b\u5927\u5c0f\u200b\u6709\u200b\u9650\u5236\u200b\uff0c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u6709\u200b\u8981\u6c42\u200b</li> </ol> <p>\u200b\u9488\u5bf9\u200b\u4ee5\u4e0a\u200b\u95ee\u9898\u200b\uff0c \u200b\u672c\u4f8b\u200b\u9009\u7528\u200b PP-OCRv3 \u200b\u8fd9\u4e00\u200b\u5f00\u6e90\u200b\u8d85\u200b\u8f7b\u91cf\u200bOCR\u200b\u7cfb\u7edf\u200b\u8fdb\u884c\u200b\u8f66\u724c\u200b\u8bc6\u522b\u7cfb\u7edf\u200b\u7684\u200b\u5f00\u53d1\u200b\u3002\u200b\u57fa\u4e8e\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200bCCPD\u200b\u6570\u636e\u200b\u96c6\u200b\u8fbe\u5230\u200b99%\u200b\u7684\u200b\u68c0\u6d4b\u200b\u548c\u200b94%\u200b\u7684\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b12.8M(2.5M+10.3M)\u3002\u200b\u57fa\u4e8e\u200b\u91cf\u5316\u200b\u5bf9\u6a21\u578b\u200b\u4f53\u79ef\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u538b\u7f29\u200b\u5230\u200b5.8M(1M+4.8M), \u200b\u540c\u65f6\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b25%\u3002</p> <p>aistudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b: \u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b\u8303\u4f8b\u200b</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#2","title":"2. \u200b\u73af\u5883\u200b\u642d\u5efa","text":"<p>\u200b\u672c\u200b\u4efb\u52a1\u200b\u57fa\u4e8e\u200bAistudio\u200b\u5b8c\u6210\u200b, \u200b\u5177\u4f53\u200b\u73af\u5883\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b: Linux</li> <li>PaddlePaddle: 2.3</li> <li>paddleslim: 2.2.2</li> <li>PaddleOCR: Release/2.5</li> </ul> <p>\u200b\u4e0b\u8f7d\u200b PaddleOCR\u200b\u4ee3\u7801\u200b</p> <pre><code>git clone -b dygraph https://github.com/PaddlePaddle/PaddleOCR\n</code></pre> <p>\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5e93\u200b</p> <pre><code>pip install -r PaddleOCR/requirements.txt\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#3","title":"3. \u200b\u6570\u636e\u200b\u96c6\u200b\u51c6\u5907","text":"<p>\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u4e3a\u200b CCPD2020 \u200b\u65b0\u80fd\u6e90\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u4e3a\u200b</p> <p>\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u5206\u5e03\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u578b\u200b \u200b\u6570\u91cf\u200b \u200b\u8bad\u7ec3\u200b\u96c6\u200b 5769 \u200b\u9a8c\u8bc1\u200b\u96c6\u200b 1001 \u200b\u6d4b\u8bd5\u200b\u96c6\u200b 5006 <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u56fe\u7247\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b:</p> <p></p> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u8fd9\u91cc\u200b\u4e0b\u8f7d\u200b https://aistudio.baidu.com/aistudio/datasetdetail/101595</p> <p>\u200b\u4e0b\u8f7d\u200b\u597d\u200b\u6570\u636e\u200b\u96c6\u540e\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u89e3\u538b\u200b</p> <pre><code>unzip -d /home/aistudio/data /home/aistudio/data/data101595/CCPD2020.zip\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#31","title":"3.1 \u200b\u6570\u636e\u200b\u96c6\u200b\u6807\u6ce8\u200b\u89c4\u5219","text":"<p>CPPD\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u56fe\u7247\u200b\u6587\u4ef6\u540d\u200b\u5177\u6709\u200b\u7279\u6b8a\u200b\u89c4\u5219\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u53ef\u200b\u67e5\u770b\u200b\uff1ahttps://github.com/detectRecog/CCPD</p> <p>\u200b\u5177\u4f53\u200b\u89c4\u5219\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u4f8b\u5982\u200b: 025-95_113-154&amp;383_386&amp;473-386&amp;473_177&amp;454_154&amp;383_363&amp;402-0_0_22_27_27_33_16-37-15.jpg</p> <p>\u200b\u6bcf\u4e2a\u200b\u540d\u79f0\u200b\u53ef\u4ee5\u200b\u5206\u4e3a\u200b\u4e03\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u4ee5\u200b-\u200b\u7b26\u53f7\u200b\u4f5c\u4e3a\u200b\u5206\u5272\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5b57\u200b\u6bb5\u200b\u89e3\u91ca\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>025\uff1a\u200b\u8f66\u724c\u200b\u9762\u79ef\u200b\u4e0e\u200b\u6574\u4e2a\u200b\u56fe\u7247\u200b\u533a\u57df\u200b\u7684\u200b\u9762\u79ef\u200b\u6bd4\u200b\u3002025 (25%)</li> <li>95_113\uff1a\u200b\u6c34\u5e73\u200b\u503e\u659c\u200b\u7a0b\u5ea6\u200b\u548c\u200b\u5782\u76f4\u200b\u503e\u659c\u5ea6\u200b\u3002\u200b\u6c34\u5e73\u200b 95\u200b\u5ea6\u200b \u200b\u5782\u76f4\u200b 113\u200b\u5ea6\u200b</li> <li>154&amp;383_386&amp;473\uff1a\u200b\u5de6\u4e0a\u200b\u548c\u200b\u53f3\u200b\u4e0b\u200b\u9876\u70b9\u200b\u7684\u200b\u5750\u6807\u200b\u3002\u200b\u5de6\u200b\u4e0a\u200b(154,383) \u200b\u53f3\u200b\u4e0b\u200b(386,473)</li> <li>386&amp;473_177&amp;454_154&amp;383_363&amp;402\uff1a\u200b\u6574\u4e2a\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u8f66\u724c\u200b\u7684\u200b\u56db\u4e2a\u200b\u9876\u70b9\u200b\u7684\u200b\u7cbe\u786e\u200b\uff08x\uff0cy\uff09\u200b\u5750\u6807\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5750\u6807\u200b\u4ece\u200b\u53f3\u4e0b\u89d2\u200b\u9876\u70b9\u200b\u5f00\u59cb\u200b\u3002(386,473) (177,454) (154,383) (363,402)</li> <li>0_0_22_27_27_33_16\uff1aCCPD\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u53ea\u6709\u200b\u4e00\u4e2a\u200b\u8f66\u724c\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u8f66\u724c\u53f7\u7801\u200b\u7531\u200b\u4e00\u4e2a\u200b\u6c49\u5b57\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5b57\u6bcd\u200b\u548c\u200b\u4e94\u4e2a\u200b\u5b57\u6bcd\u200b\u6216\u200b\u6570\u5b57\u200b\u7ec4\u6210\u200b\u3002\u200b\u6709\u6548\u200b\u7684\u200b\u4e2d\u6587\u200b\u8f66\u724c\u200b\u7531\u200b\u4e03\u4e2a\u200b\u5b57\u7b26\u200b\u7ec4\u6210\u200b\uff1a\u200b\u7701\u200b\uff081\u200b\u4e2a\u5b57\u7b26\u200b\uff09\uff0c\u200b\u5b57\u6bcd\u200b\uff081\u200b\u4e2a\u5b57\u7b26\u200b\uff09\uff0c\u200b\u5b57\u6bcd\u200b+\u200b\u6570\u5b57\u200b\uff085\u200b\u4e2a\u5b57\u7b26\u200b\uff09\u3002\u201c 0_0_22_27_27_33_16\u201d\u200b\u662f\u200b\u6bcf\u4e2a\u200b\u5b57\u7b26\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002\u200b\u8fd9\u200b\u4e09\u4e2a\u200b\u6570\u7ec4\u200b\u5b9a\u4e49\u200b\u5982\u4e0b\u200b\uff1a\u200b\u6bcf\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5b57\u7b26\u200b\u662f\u200b\u5b57\u6bcd\u200bO\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6570\u5b57\u200b0\u3002\u200b\u6211\u4eec\u200b\u5c06\u200bO\u200b\u7528\u4f5c\u200b\u201c\u200b\u65e0\u200b\u5b57\u7b26\u200b\u201d\u200b\u7684\u200b\u7b26\u53f7\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4e2d\u6587\u200b\u8f66\u724c\u200b\u5b57\u7b26\u200b\u4e2d\u200b\u6ca1\u6709\u200bO\u3002\u200b\u56e0\u6b64\u200b\u4ee5\u4e0a\u200b\u8f66\u724c\u200b\u62fc\u200b\u8d77\u6765\u200b\u5373\u200b\u4e3a\u200b \u200b\u7696\u200bAY339S</li> <li>37\uff1a\u200b\u724c\u7167\u200b\u533a\u57df\u200b\u7684\u200b\u4eae\u5ea6\u200b\u3002 37 (37%)</li> <li>15\uff1a\u200b\u8f66\u724c\u200b\u533a\u57df\u200b\u7684\u200b\u6a21\u7cca\u200b\u5ea6\u200b\u300215 (15%)</li> </ul> <pre><code>provinces = [\"\u200b\u7696\u200b\", \"\u200b\u6caa\u200b\", \"\u200b\u6d25\u200b\", \"\u200b\u6e1d\u200b\", \"\u200b\u5180\u200b\", \"\u200b\u664b\u200b\", \"\u200b\u8499\u200b\", \"\u200b\u8fbd\u200b\", \"\u200b\u5409\u200b\", \"\u200b\u9ed1\u200b\", \"\u200b\u82cf\u200b\", \"\u200b\u6d59\u200b\", \"\u200b\u4eac\u200b\", \"\u200b\u95fd\u200b\", \"\u200b\u8d63\u200b\", \"\u200b\u9c81\u200b\", \"\u200b\u8c6b\u200b\", \"\u200b\u9102\u200b\", \"\u200b\u6e58\u200b\", \"\u200b\u7ca4\u200b\", \"\u200b\u6842\u200b\", \"\u200b\u743c\u200b\", \"\u200b\u5ddd\u200b\", \"\u200b\u8d35\u200b\", \"\u200b\u4e91\u200b\", \"\u200b\u85cf\u200b\", \"\u200b\u9655\u200b\", \"\u200b\u7518\u200b\", \"\u200b\u9752\u200b\", \"\u200b\u5b81\u200b\", \"\u200b\u65b0\u200b\", \"\u200b\u8b66\u200b\", \"\u200b\u5b66\u200b\", \"O\"]\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W','X', 'Y', 'Z', 'O']\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X','Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#32-pp-ocr","title":"3.2 \u200b\u5236\u4f5c\u200b\u7b26\u5408\u200bPP-OCR\u200b\u8bad\u7ec3\u200b\u683c\u5f0f\u200b\u7684\u200b\u6807\u6ce8\u200b\u6587\u4ef6","text":"<p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u53ef\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u5236\u4f5c\u200b\u7b26\u5408\u200bPP-OCR\u200b\u8bad\u7ec3\u200b\u683c\u5f0f\u200b\u7684\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u3002</p> <pre><code>import cv2\nimport os\nimport json\nfrom tqdm import tqdm\nimport numpy as np\n\nprovinces = [\"\u200b\u7696\u200b\", \"\u200b\u6caa\u200b\", \"\u200b\u6d25\u200b\", \"\u200b\u6e1d\u200b\", \"\u200b\u5180\u200b\", \"\u200b\u664b\u200b\", \"\u200b\u8499\u200b\", \"\u200b\u8fbd\u200b\", \"\u200b\u5409\u200b\", \"\u200b\u9ed1\u200b\", \"\u200b\u82cf\u200b\", \"\u200b\u6d59\u200b\", \"\u200b\u4eac\u200b\", \"\u200b\u95fd\u200b\", \"\u200b\u8d63\u200b\", \"\u200b\u9c81\u200b\", \"\u200b\u8c6b\u200b\", \"\u200b\u9102\u200b\", \"\u200b\u6e58\u200b\", \"\u200b\u7ca4\u200b\", \"\u200b\u6842\u200b\", \"\u200b\u743c\u200b\", \"\u200b\u5ddd\u200b\", \"\u200b\u8d35\u200b\", \"\u200b\u4e91\u200b\", \"\u200b\u85cf\u200b\", \"\u200b\u9655\u200b\", \"\u200b\u7518\u200b\", \"\u200b\u9752\u200b\", \"\u200b\u5b81\u200b\", \"\u200b\u65b0\u200b\", \"\u200b\u8b66\u200b\", \"\u200b\u5b66\u200b\", \"O\"]\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n\ndef make_label(img_dir, save_gt_folder, phase):\n    crop_img_save_dir = os.path.join(save_gt_folder, phase, 'crop_imgs')\n    os.makedirs(crop_img_save_dir, exist_ok=True)\n\n    f_det = open(os.path.join(save_gt_folder, phase, 'det.txt'), 'w', encoding='utf-8')\n    f_rec = open(os.path.join(save_gt_folder, phase, 'rec.txt'), 'w', encoding='utf-8')\n\n    i = 0\n    for filename in tqdm(os.listdir(os.path.join(img_dir, phase))):\n        str_list = filename.split('-')\n        if len(str_list) &lt; 5:\n            continue\n        coord_list = str_list[3].split('_')\n        txt_list = str_list[4].split('_')\n        boxes = []\n        for coord in coord_list:\n            boxes.append([int(x) for x in coord.split(\"&amp;\")])\n        boxes = [boxes[2], boxes[3], boxes[0], boxes[1]]\n        lp_number = provinces[int(txt_list[0])] + alphabets[int(txt_list[1])] + ''.join([ads[int(x)] for x in txt_list[2:]])\n\n        # det\n        det_info = [{'points':boxes, 'transcription':lp_number}]\n        f_det.write('{}\\t{}\\n'.format(os.path.join(phase, filename), json.dumps(det_info, ensure_ascii=False)))\n\n        # rec\n        boxes = np.float32(boxes)\n        img = cv2.imread(os.path.join(img_dir, phase, filename))\n        # crop_img = img[int(boxes[:,1].min()):int(boxes[:,1].max()),int(boxes[:,0].min()):int(boxes[:,0].max())]\n        crop_img = get_rotate_crop_image(img, boxes)\n        crop_img_save_filename = '{}_{}.jpg'.format(i,'_'.join(txt_list))\n        crop_img_save_path = os.path.join(crop_img_save_dir, crop_img_save_filename)\n        cv2.imwrite(crop_img_save_path, crop_img)\n        f_rec.write('{}/crop_imgs/{}\\t{}\\n'.format(phase, crop_img_save_filename, lp_number))\n        i+=1\n    f_det.close()\n    f_rec.close()\n\ndef get_rotate_crop_image(img, points):\n    '''\n    img_height, img_width = img.shape[0:2]\n    left = int(np.min(points[:, 0]))\n    right = int(np.max(points[:, 0]))\n    top = int(np.min(points[:, 1]))\n    bottom = int(np.max(points[:, 1]))\n    img_crop = img[top:bottom, left:right, :].copy()\n    points[:, 0] = points[:, 0] - left\n    points[:, 1] = points[:, 1] - top\n    '''\n    assert len(points) == 4, \"shape of points must be 4*2\"\n    img_crop_width = int(\n        max(\n            np.linalg.norm(points[0] - points[1]),\n            np.linalg.norm(points[2] - points[3])))\n    img_crop_height = int(\n        max(\n            np.linalg.norm(points[0] - points[3]),\n            np.linalg.norm(points[1] - points[2])))\n    pts_std = np.float32([[0, 0], [img_crop_width, 0],\n                          [img_crop_width, img_crop_height],\n                          [0, img_crop_height]])\n    M = cv2.getPerspectiveTransform(points, pts_std)\n    dst_img = cv2.warpPerspective(\n        img,\n        M, (img_crop_width, img_crop_height),\n        borderMode=cv2.BORDER_REPLICATE,\n        flags=cv2.INTER_CUBIC)\n    dst_img_height, dst_img_width = dst_img.shape[0:2]\n    if dst_img_height * 1.0 / dst_img_width &gt;= 1.5:\n        dst_img = np.rot90(dst_img)\n    return dst_img\n\nimg_dir = '/home/aistudio/data/CCPD2020/ccpd_green'\nsave_gt_folder = '/home/aistudio/data/CCPD2020/PPOCR'\n# phase = 'train' # change to val and test to make val dataset and test dataset\nfor phase in ['train','val','test']:\n    make_label(img_dir, save_gt_folder, phase)\n</code></pre> <p>\u200b\u901a\u8fc7\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u53ef\u4ee5\u200b\u5b8c\u6210\u200b\u4e86\u200b<code>\u200b\u8bad\u7ec3\u200b\u96c6\u200b</code>\uff0c<code>\u200b\u9a8c\u8bc1\u200b\u96c6\u200b</code>\u200b\u548c\u200b<code>\u200b\u6d4b\u8bd5\u200b\u96c6\u200b</code>\u200b\u7684\u200b\u5236\u4f5c\u200b\uff0c\u200b\u5236\u4f5c\u200b\u5b8c\u6210\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u4fe1\u606f\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u7c7b\u578b\u200b \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u56fe\u7247\u200b\u5730\u5740\u200b \u200b\u6807\u7b7e\u200b\u5730\u5740\u200b \u200b\u56fe\u7247\u200b\u6570\u91cf\u200b \u200b\u68c0\u6d4b\u200b \u200b\u8bad\u7ec3\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/ccpd_green/train /home/aistudio/data/CCPD2020/PPOCR/train/det.txt 5769 \u200b\u68c0\u6d4b\u200b \u200b\u9a8c\u8bc1\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/ccpd_green/val /home/aistudio/data/CCPD2020/PPOCR/val/det.txt 1001 \u200b\u68c0\u6d4b\u200b \u200b\u6d4b\u8bd5\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/ccpd_green/test /home/aistudio/data/CCPD2020/PPOCR/test/det.txt 5006 \u200b\u8bc6\u522b\u200b \u200b\u8bad\u7ec3\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/PPOCR/train/crop_imgs /home/aistudio/data/CCPD2020/PPOCR/train/rec.txt 5769 \u200b\u8bc6\u522b\u200b \u200b\u9a8c\u8bc1\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/PPOCR/val/crop_imgs /home/aistudio/data/CCPD2020/PPOCR/val/rec.txt 1001 \u200b\u8bc6\u522b\u200b \u200b\u6d4b\u8bd5\u200b\u96c6\u200b /home/aistudio/data/CCPD2020/PPOCR/test/crop_imgs /home/aistudio/data/CCPD2020/PPOCR/test/rec.txt 5006 <p>\u200b\u5728\u200b\u666e\u904d\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u9009\u62e9\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\u540e\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\u3002\u200b\u5728\u200b\u672c\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7701\u7565\u200b\u4e2d\u95f4\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u76f4\u63a5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u9009\u62e9\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u53ea\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#4","title":"4. \u200b\u5b9e\u9a8c","text":"<p>\u200b\u7531\u4e8e\u200b\u6570\u636e\u200b\u96c6\u200b\u6bd4\u8f83\u200b\u5c11\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u6a21\u578b\u200b\u66f4\u597d\u200b\u548c\u200b\u66f4\u200b\u5feb\u200b\u7684\u200b\u6536\u655b\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u9009\u7528\u200b PaddleOCR \u200b\u4e2d\u200b\u7684\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f7f\u7528\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4f5c\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002PP-OCRv3\u200b\u5728\u200bPP-OCRv2\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u7aef\u5230\u200b\u7aef\u200bHmean\u200b\u6307\u6807\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv2\u200b\u63d0\u5347\u200b5%, \u200b\u82f1\u6587\u200b\u6570\u5b57\u6a21\u578b\u200b\u7aef\u5230\u200b\u7aef\u200b\u6548\u679c\u200b\u63d0\u5347\u200b11%\u3002\u200b\u8be6\u7ec6\u200b\u4f18\u5316\u200b\u7ec6\u8282\u200b\u8bf7\u200b\u53c2\u8003\u200bPP-OCRv3\u200b\u6280\u672f\u200b\u62a5\u544a\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u8f66\u724c\u200b\u573a\u666f\u200b\u5747\u200b\u4e3a\u200b\u7aef\u4fa7\u200b\u8bbe\u5907\u200b\u90e8\u7f72\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5bf9\u200b\u901f\u5ea6\u200b\u548c\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u6709\u200b\u6bd4\u8f83\u200b\u9ad8\u200b\u7684\u200b\u8981\u6c42\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8fd8\u200b\u9700\u8981\u200b\u91c7\u7528\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u7684\u200b\u538b\u7f29\u200b\u548c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u7684\u200b\u52a0\u901f\u200b\u3002\u200b\u6a21\u578b\u200b\u91cf\u5316\u200b\u53ef\u4ee5\u200b\u5728\u200b\u57fa\u672c\u200b\u4e0d\u200b\u635f\u5931\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5c06\u200bFP32\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u8f6c\u6362\u200b\u4e3a\u200bInt8\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u51cf\u5c0f\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5927\u5c0f\u200b\u5e76\u200b\u52a0\u901f\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u4f7f\u7528\u200b\u91cf\u5316\u200b\u540e\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u79fb\u52a8\u200b\u7aef\u200b\u7b49\u200b\u90e8\u7f72\u200b\u65f6\u200b\u66f4\u200b\u5177\u5907\u200b\u901f\u5ea6\u200b\u4f18\u52bf\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u672c\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u5bf9\u4e8e\u200b\u8f66\u724c\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6709\u200b\u5982\u4e0b\u200b3\u200b\u79cd\u200b\u65b9\u6848\u200b\uff1a</p> <ol> <li>PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b</li> <li>CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u5728\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\u4e0a\u200bfine-tune</li> <li>CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u5728\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\u4e0a\u200bfine-tune\u200b\u540e\u200b\u91cf\u5316\u200b</li> </ol>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#41","title":"4.1 \u200b\u68c0\u6d4b","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#411","title":"4.1.1 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b","text":"<p>\u200b\u4ece\u200b\u4e0b\u8868\u4e2d\u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b ch_PP-OCRv3_det \u3010\u200b\u6700\u65b0\u200b\u3011\u200b\u539f\u59cb\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u4e2d\u82f1\u6587\u200b\u3001\u200b\u591a\u8bed\u79cd\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b ch_PP-OCRv3_det_cml.yml 3.8M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> <pre><code>mkdir models\ncd models\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar -xf ch_PP-OCRv3_det_distill_train.tar\ncd /home/aistudio/PaddleOCR\n</code></pre> <p>\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bch_PP-OCRv3_det_student.yml \u200b\u914d\u7f6e\u6587\u4ef6\u200b\u8fdb\u884c\u200b\u540e\u7eed\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5728\u200b\u5f00\u59cb\u200b\u8bc4\u4f30\u200b\u4e4b\u524d\u200b\u9700\u8981\u200b\u5bf9\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u90e8\u5206\u200b\u5b57\u200b\u6bb5\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5177\u4f53\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u548c\u200b\u8bad\u7ec3\u200b\u76f8\u5173\u200b:</li> <li>Global.pretrained_model: \u200b\u6307\u5411\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u76f8\u5173\u200b</li> <li>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Eval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> </ol> <p>\u200b\u4e0a\u8ff0\u200b\u5b57\u6bb5\u200b\u5747\u200b\u4e3a\u200b\u5fc5\u987b\u200b\u4fee\u6539\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6539\u52a8\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u200b\u5728\u200b\u4e0d\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6539\u53d8\u200b\u8bad\u7ec3\u200b\u7684\u200b\u53c2\u6570\u200b\u3002\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u4e0d\u200b\u6539\u53d8\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u65b9\u5f0f\u200b \u3002\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u8bc4\u4f30\u200b</p> <pre><code>python tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_det_distill_train/student.pdparams \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/det.txt]\n</code></pre> <p>\u200b\u4e0a\u8ff0\u200b\u6307\u4ee4\u200b\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200b-c \u200b\u9009\u62e9\u200b\u8bad\u7ec3\u200b\u4f7f\u7528\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u901a\u8fc7\u200b-o\u200b\u53c2\u6570\u200b\u5728\u200b\u4e0d\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6539\u53d8\u200b\u8bad\u7ec3\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 76.12%"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#412-ccpdfine-tune","title":"4.1.2 CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_1","title":"\u8bad\u7ec3","text":"<p>\u200b\u4e3a\u4e86\u200b\u8fdb\u884c\u200bfine-tune\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u548c\u200b\u6570\u636e\u200b\u96c6\u7b49\u200b\u53c2\u6570\u200b\u3002 \u200b\u5177\u4f53\u200b\u5982\u4e0b\u200b:</p> <ol> <li>\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u548c\u200b\u8bad\u7ec3\u200b\u76f8\u5173\u200b:</li> <li>Global.pretrained_model: \u200b\u6307\u5411\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b</li> <li>Global.eval_batch_step: \u200b\u6a21\u578b\u200b\u591a\u5c11\u200bstep\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u8bbe\u4e3a\u200b\u4ece\u200b\u7b2c\u200b0\u200b\u4e2a\u200bstep\u200b\u5f00\u59cb\u200b\u6bcf\u9694\u200b772\u200b\u4e2a\u200bstep\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff0c772\u200b\u4e3a\u200b\u4e00\u4e2a\u200bepoch\u200b\u603b\u200b\u7684\u200bstep\u200b\u6570\u200b\u3002</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b:</li> <li>Optimizer.lr.name: \u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u5668\u200b\u8bbe\u200b\u4e3a\u200b\u5e38\u91cf\u200b Const</li> <li>Optimizer.lr.learning_rate: \u200b\u505a\u200b fine-tune \u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\uff0c\u200b\u6b64\u5904\u200b\u5b66\u4e60\u200b\u7387\u8bbe\u200b\u4e3a\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b0.05\u200b\u500d\u200b</li> <li>Optimizer.lr.warmup_epoch: warmup_epoch\u200b\u8bbe\u200b\u4e3a\u200b0</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u76f8\u5173\u200b:</li> <li>Train.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Train.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> <li>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Eval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> </ol> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u542f\u52a8\u200b\u5728\u200bCCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200bfine-tune\u3002</p> <pre><code>python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_det_distill_train/student.pdparams \\\n    Global.save_model_dir=output/CCPD/det \\\n    Global.eval_batch_step=\"[0, 772]\" \\\n    Optimizer.lr.name=Const \\\n    Optimizer.lr.learning_rate=0.0005 \\\n    Optimizer.lr.warmup_epoch=0 \\\n    Train.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Train.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/train/det.txt] \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/det.txt]\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200b<code>-o</code>\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4fee\u6539\u200b\u4e86\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_2","title":"\u8bc4\u4f30","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b</p> <pre><code>python tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det/best_accuracy.pdparams \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/det.txt]\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u548c\u200bCCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune\uff0c\u200b\u6307\u6807\u200b\u5206\u522b\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b hmeans PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 76.12% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 99.00% <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fdb\u884c\u200bfine-tune\u200b\u80fd\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\u8f66\u724c\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#413-ccpdfine-tune","title":"4.1.3 CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune+\u200b\u91cf\u5316\u200b\u8bad\u7ec3","text":"<p>\u200b\u6b64\u5904\u200b\u91c7\u7528\u200b PaddleOCR \u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u597d\u200b\u7684\u200b\u91cf\u5316\u200b\u6559\u7a0b\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u53ef\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b:</p> <pre><code>python3.7 deploy/slim/quantization/quant.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det/best_accuracy.pdparams \\\n    Global.save_model_dir=output/CCPD/det_quant \\\n    Global.eval_batch_step=\"[0, 772]\" \\\n    Optimizer.lr.name=Const \\\n    Optimizer.lr.learning_rate=0.0005 \\\n    Optimizer.lr.warmup_epoch=0 \\\n    Train.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Train.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/train/det.txt] \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/det.txt]\n</code></pre> <p>\u200b\u91cf\u5316\u200b\u540e\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b</p> \u200b\u65b9\u6848\u200b hmeans \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b(lite) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 99.00% 2.5M 223ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune+\u200b\u91cf\u5316\u200b 98.91% 1.0M 189ms <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u901a\u8fc7\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u5728\u200b\u7cbe\u5ea6\u200b\u51e0\u4e4e\u200b\u65e0\u635f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u964d\u4f4e\u200b\u6a21\u578b\u200b\u4f53\u79ef\u200b60%\u200b\u5e76\u4e14\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b15%\u3002</p> <p>\u200b\u901f\u5ea6\u200b\u6d4b\u8bd5\u200b\u57fa\u4e8e\u200bPaddleOCR lite\u200b\u6559\u7a0b\u200b\u5b8c\u6210\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#414","title":"4.1.4 \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u5bfc\u51fa\u200b</p> <p>\u200b\u975e\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b</p> <pre><code>python tools/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/det/infer\n</code></pre> <p>\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b</p> <pre><code>python deploy/slim/quantization/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det_quant/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/det/infer\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#42","title":"4.2 \u200b\u8bc6\u522b","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#421","title":"4.2.1 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b","text":"<p>\u200b\u4ece\u200b\u4e0b\u8868\u4e2d\u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b ch_PP-OCRv3_rec \u3010\u200b\u6700\u65b0\u200b\u3011\u200b\u539f\u59cb\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u4e2d\u82f1\u6587\u200b\u3001\u200b\u6570\u5b57\u200b\u8bc6\u522b\u200b ch_PP-OCRv3_rec_distillation.yml 12.4M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b</p> <pre><code>mkdir models\ncd models\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\ntar -xf ch_PP-OCRv3_rec_train.tar\ncd /home/aistudio/PaddleOCR\n</code></pre> <p>PaddleOCR\u200b\u63d0\u4f9b\u200b\u7684\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u91c7\u7528\u200b\u84b8\u998f\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u56e0\u6b64\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e2d\u200b\u4f1a\u200b\u5305\u542b\u200b<code>Teacher</code>\u200b\u548c\u200b<code>Student</code>\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u53ef\u200b\u53c2\u8003\u200bknowledge_distillation.md\u3002 \u200b\u56e0\u6b64\u200b\uff0c\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u63d0\u53d6\u200b<code>Student</code>\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff1a</p> <pre><code>import paddle\n# \u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nall_params = paddle.load(\"models/ch_PP-OCRv3_rec_train/best_accuracy.pdparams\")\n# \u200b\u67e5\u770b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(all_params.keys())\n# \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u63d0\u53d6\u200b\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# \u200b\u67e5\u770b\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u7684\u200bkeys\nprint(s_params.keys())\n# \u200b\u4fdd\u5b58\u200b\npaddle.save(s_params, \"models/ch_PP-OCRv3_rec_train/student.pdparams\")\n</code></pre> <p>\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bch_PP-OCRv3_rec.yml \u200b\u914d\u7f6e\u6587\u4ef6\u200b\u8fdb\u884c\u200b\u540e\u7eed\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5728\u200b\u5f00\u59cb\u200b\u8bc4\u4f30\u200b\u4e4b\u524d\u200b\u9700\u8981\u200b\u5bf9\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u90e8\u5206\u200b\u5b57\u200b\u6bb5\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5177\u4f53\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u548c\u200b\u8bad\u7ec3\u200b\u76f8\u5173\u200b:</li> <li>Global.pretrained_model: \u200b\u6307\u5411\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u76f8\u5173\u200b</li> <li>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Eval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> </ol> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u8bc4\u4f30\u200b</p> <pre><code>python tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_rec_train/student.pdparams \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/rec.txt]\n</code></pre> <p>\u200b\u5982\u9700\u200b\u83b7\u53d6\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <p>\u200b\u8bc4\u4f30\u200b\u90e8\u5206\u200b\u65e5\u5fd7\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>[2022/05/12 19:52:02] ppocr INFO: load pretrain successful from models/ch_PP-OCRv3_rec_train/best_accuracy\neval model:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:15&lt;00:00,  2.57it/s]\n[2022/05/12 19:52:17] ppocr INFO: metric eval ***************\n[2022/05/12 19:52:17] ppocr INFO: acc:0.0\n[2022/05/12 19:52:17] ppocr INFO: norm_edit_dis:0.8656084923002452\n[2022/05/12 19:52:17] ppocr INFO: Teacher_acc:0.000399520574511545\n[2022/05/12 19:52:17] ppocr INFO: Teacher_norm_edit_dis:0.8657902943394548\n[2022/05/12 19:52:17] ppocr INFO: fps:1443.1801978719905\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 0% <p>\u200b\u4ece\u200b\u8bc4\u4f30\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0cacc\u200b\u975e\u5e38\u4f4e\u200b\uff0c\u200b\u4f46\u662f\u200bnorm_edit_dis\u200b\u5f88\u200b\u9ad8\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u731c\u6d4b\u200b\u662f\u200b\u6a21\u578b\u200b\u5927\u90e8\u5206\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u662f\u200b\u5bf9\u200b\u7684\u200b\uff0c\u200b\u53ea\u6709\u200b\u5c11\u90e8\u5206\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b\u9519\u8bef\u200b\u3002\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200binfer\u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u9a8c\u8bc1\u200b\uff1a</p> <pre><code>python tools/infer_rec.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_rec_train/student.pdparams \\\n    Global.infer_img=/home/aistudio/data/CCPD2020/PPOCR/test/crop_imgs/0_0_0_3_32_30_31_30_30.jpg\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\u90e8\u5206\u200b\u65e5\u5fd7\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>[2022/05/01 08:51:57] ppocr INFO: train with paddle 2.2.2 and device CUDAPlace(0)\nW0501 08:51:57.127391 11326 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\nW0501 08:51:57.132315 11326 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n[2022/05/01 08:52:00] ppocr INFO: load pretrain successful from models/ch_PP-OCRv3_rec_train/student\n[2022/05/01 08:52:00] ppocr INFO: infer_img: /home/aistudio/data/CCPD2020/PPOCR/test/crop_imgs/0_0_3_32_30_31_30_30.jpg\n[2022/05/01 08:52:00] ppocr INFO:      result: {\"Student\": {\"label\": \"\u200b\u7696\u200bA\u00b7D86766\", \"score\": 0.9552637934684753}, \"Teacher\": {\"label\": \"\u200b\u7696\u200bA\u00b7D86766\", \"score\": 0.9917094707489014}}\n[2022/05/01 08:52:00] ppocr INFO: success!\n</code></pre> <p>\u200b\u4ece\u200binfer\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u8f66\u724c\u200b\u4e2d\u200b\u7684\u200b\u6587\u5b57\u200b\u5927\u90e8\u5206\u200b\u90fd\u200b\u8bc6\u522b\u200b\u6b63\u786e\u200b\uff0c\u200b\u53ea\u662f\u200b\u591a\u200b\u8bc6\u522b\u200b\u51fa\u200b\u4e86\u200b\u4e00\u4e2a\u200b<code>\u00b7</code>\u3002\u200b\u9488\u5bf9\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6709\u200b\u5982\u4e0b\u200b\u4e24\u79cd\u200b\u65b9\u6848\u200b\uff1a</p> <ol> <li>\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code>\u3002</li> <li>\u200b\u8fdb\u884c\u200b fine-tune\u3002</li> </ol>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#422","title":"4.2.2 \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b+\u200b\u6539\u52a8\u200b\u540e\u5904\u7406","text":"<p>\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code>\uff0c\u200b\u5728\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u6539\u52a8\u200b\u6bd4\u8f83\u7b80\u5355\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u5728\u200b ppocr/postprocess/rec_postprocess.py \u200b\u6587\u4ef6\u200b\u7684\u200b76\u200b\u884c\u200b\u6dfb\u52a0\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b:</p> <pre><code>text = text.replace('\u00b7','')\n</code></pre> <p>\u200b\u6539\u52a8\u200b\u524d\u540e\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b:</p> \u200b\u65b9\u6848\u200b acc PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 0.20% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b+\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 90.97% <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u53bb\u6389\u200b\u591a\u4f59\u200b\u7684\u200b<code>\u00b7</code>\u200b\u80fd\u200b\u5927\u5e45\u63d0\u9ad8\u200b\u7cbe\u5ea6\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#423-ccpdfine-tune","title":"4.2.3 CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_3","title":"\u8bad\u7ec3","text":"<p>\u200b\u4e3a\u4e86\u200b\u8fdb\u884c\u200bfine-tune\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u548c\u200b\u6570\u636e\u200b\u96c6\u7b49\u200b\u53c2\u6570\u200b\u3002 \u200b\u5177\u4f53\u200b\u5982\u4e0b\u200b:</p> <ol> <li>\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u548c\u200b\u8bad\u7ec3\u200b\u76f8\u5173\u200b:</li> <li>Global.pretrained_model: \u200b\u6307\u5411\u200bPP-OCRv3\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5730\u5740\u200b</li> <li>Global.eval_batch_step: \u200b\u6a21\u578b\u200b\u591a\u5c11\u200bstep\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u8bbe\u4e3a\u200b\u4ece\u200b\u7b2c\u200b0\u200b\u4e2a\u200bstep\u200b\u5f00\u59cb\u200b\u6ca1\u9694\u200b45\u200b\u4e2a\u200bstep\u200b\u8bc4\u4f30\u200b\u4e00\u6b21\u200b\uff0c45\u200b\u4e3a\u200b\u4e00\u4e2a\u200bepoch\u200b\u603b\u200b\u7684\u200bstep\u200b\u6570\u200b\u3002</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b</li> <li>Optimizer.lr.name: \u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u5668\u200b\u8bbe\u200b\u4e3a\u200b\u5e38\u91cf\u200b Const</li> <li>Optimizer.lr.learning_rate: \u200b\u505a\u200b fine-tune \u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\uff0c\u200b\u6b64\u5904\u200b\u5b66\u4e60\u200b\u7387\u8bbe\u200b\u4e3a\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b0.05\u200b\u500d\u200b</li> <li>Optimizer.lr.warmup_epoch: warmup_epoch\u200b\u8bbe\u200b\u4e3a\u200b0</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u76f8\u5173\u200b</li> <li>Train.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Train.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> <li>Eval.dataset.data_dir\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u76ee\u5f55\u200b</li> <li>Eval.dataset.label_file_list\uff1a\u200b\u6307\u5411\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b</li> </ol> <p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b fine-tune</p> <pre><code>python tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_rec_train/student.pdparams \\\n    Global.save_model_dir=output/CCPD/rec/ \\\n    Global.eval_batch_step=\"[0, 90]\" \\\n    Optimizer.lr.name=Const \\\n    Optimizer.lr.learning_rate=0.0005 \\\n    Optimizer.lr.warmup_epoch=0 \\\n    Train.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Train.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/train/rec.txt] \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/rec.txt]\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_4","title":"\u8bc4\u4f30","text":"<p>\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b</p> <pre><code>python tools/eval.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec/best_accuracy.pdparams \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/rec.txt]\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u548c\u200bCCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune\uff0c\u200b\u6307\u6807\u200b\u5206\u522b\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u65b9\u6848\u200b acc PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 0.00% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b+\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 90.97% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 94.54% <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fdb\u884c\u200bfine-tune\u200b\u80fd\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b\u7684\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#424-ccpdfine-tune","title":"4.2.4 CCPD\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u200bfine-tune+\u200b\u91cf\u5316\u200b\u8bad\u7ec3","text":"<p>\u200b\u6b64\u5904\u200b\u91c7\u7528\u200b PaddleOCR \u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u597d\u200b\u7684\u200b\u91cf\u5316\u200b\u6559\u7a0b\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u53ef\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u542f\u52a8\u200b:</p> <pre><code>python3.7 deploy/slim/quantization/quant.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec/best_accuracy.pdparams \\\n    Global.save_model_dir=output/CCPD/rec_quant/ \\\n    Global.eval_batch_step=\"[0, 90]\" \\\n    Optimizer.lr.name=Const \\\n    Optimizer.lr.learning_rate=0.0005 \\\n    Optimizer.lr.warmup_epoch=0 \\\n    Train.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Train.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/train/rec.txt] \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/rec.txt]\n</code></pre> <p>\u200b\u91cf\u5316\u200b\u540e\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b</p> \u200b\u65b9\u6848\u200b acc \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b(lite) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 94.54% 10.3M 4.2ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune + \u200b\u91cf\u5316\u200b 93.40% 4.8M 1.8ms <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u91cf\u5316\u200b\u540e\u80fd\u200b\u964d\u4f4e\u200b\u6a21\u578b\u200b\u4f53\u79ef\u200b53%\u200b\u5e76\u4e14\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b57%\uff0c\u200b\u4f46\u662f\u200b\u7531\u4e8e\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u8fc7\u5c11\u200b\uff0c\u200b\u91cf\u5316\u200b\u5e26\u6765\u200b\u4e86\u200b1%\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e0b\u964d\u200b\u3002</p> <p>\u200b\u901f\u5ea6\u200b\u6d4b\u8bd5\u200b\u57fa\u4e8e\u200bPaddleOCR lite\u200b\u6559\u7a0b\u200b\u5b8c\u6210\u200b\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#425","title":"4.2.5 \u200b\u6a21\u578b\u200b\u5bfc\u51fa","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u5bfc\u51fa\u200b\u3002</p> <p>\u200b\u975e\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b</p> <pre><code>python tools/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/rec/infer\n</code></pre> <p>\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b</p> <pre><code>python deploy/slim/quantization/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec_quant/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/rec_quant/infer\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#43-end2end","title":"4.3 \u200b\u8ba1\u7b97\u200bEnd2End\u200b\u6307\u6807","text":"<p>\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u53ef\u200b\u901a\u8fc7\u200b PaddleOCR\u200b\u5185\u7f6e\u200b\u811a\u672c\u200b \u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5177\u4f53\u6b65\u9aa4\u200b\u5982\u4e0b\u200b\uff1a</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#1_1","title":"1. \u200b\u5bfc\u51fa\u200b\u6a21\u578b","text":"<p>\u200b\u901a\u8fc7\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u5bfc\u51fa\u200b\u3002\u200b\u6ce8\u610f\u200b\uff0c\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u914d\u7f6e\u200beval\u200b\u6570\u636e\u200b\u96c6\u200b</p> <pre><code># \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\n\n# \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_det_distill_train/student.pdparams \\\n    Global.save_inference_dir=output/ch_PP-OCRv3_det_distill_train/infer\n\n# \u200b\u975e\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\npython tools/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/det/infer\n\n# \u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\npython deploy/slim/quantization/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o \\\n    Global.pretrained_model=output/CCPD/det_quant/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/det_quant/infer \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/ccpd_green \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/det.txt] \\\n    Eval.loader.num_workers=0\n\n# \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\n\n# \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython tools/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=models/ch_PP-OCRv3_rec_train/student.pdparams \\\n    Global.save_inference_dir=output/ch_PP-OCRv3_rec_train/infer\n\n# \u200b\u975e\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\npython tools/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/rec/infer\n\n# \u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\npython deploy/slim/quantization/export_model.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec.yml -o \\\n    Global.pretrained_model=output/CCPD/rec_quant/best_accuracy.pdparams \\\n    Global.save_inference_dir=output/CCPD/rec_quant/infer \\\n    Eval.dataset.data_dir=/home/aistudio/data/CCPD2020/PPOCR \\\n    Eval.dataset.label_file_list=[/home/aistudio/data/CCPD2020/PPOCR/test/rec.txt]\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#2_1","title":"2. \u200b\u7528\u200b\u5bfc\u51fa\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b","text":"<p>\u200b\u6b64\u5904\u200b\uff0c\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0cfintune\u200b\u6a21\u578b\u200b\u548c\u200b\u91cf\u5316\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\npython3 tools/infer/predict_system.py --det_model_dir=models/ch_PP-OCRv3_det_distill_train/infer --rec_model_dir=models/ch_PP-OCRv3_rec_train/infer --det_limit_side_len=736 --det_limit_type=min --image_dir=/home/aistudio/data/CCPD2020/ccpd_green/test/ --draw_img_save_dir=infer/pretrain --use_dilation=true\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune\npython3 tools/infer/predict_system.py --det_model_dir=output/CCPD/det/infer --rec_model_dir=output/CCPD/rec/infer --det_limit_side_len=736 --det_limit_type=min --image_dir=/home/aistudio/data/CCPD2020/ccpd_green/test/ --draw_img_save_dir=infer/fine-tune --use_dilation=true\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune +\u200b\u91cf\u5316\u200b\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune +\u200b\u91cf\u5316\u200b \u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u548c\u200b\u8bc4\u4f30\u200b\npython3 tools/infer/predict_system.py --det_model_dir=output/CCPD/det_quant/infer --rec_model_dir=output/CCPD/rec_quant/infer --det_limit_side_len=736 --det_limit_type=min --image_dir=/home/aistudio/data/CCPD2020/ccpd_green/test/ --draw_img_save_dir=infer/quant --use_dilation=true\n</code></pre>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#3-label","title":"3. \u200b\u8f6c\u6362\u200blabel\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u6307\u6807","text":"<p>\u200b\u5c06\u200bgt\u200b\u548c\u200b\u4e0a\u200b\u4e00\u6b65\u200b\u4fdd\u5b58\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u4e3a\u7aef\u200b\u5bf9\u200b\u7aef\u200b\u8bc4\u6d4b\u200b\u9700\u8981\u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u8f6c\u6362\u200b\u540e\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u8ba1\u7b97\u200b</p> <pre><code>python3 tools/end2end/convert_ppocr_label.py --mode=gt --label_path=/home/aistudio/data/CCPD2020/PPOCR/test/det.txt --save_folder=end2end/gt\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u548c\u200b\u8bc4\u4f30\u200b\npython3 tools/end2end/convert_ppocr_label.py --mode=pred --label_path=infer/pretrain/system_results.txt --save_folder=end2end/pretrain\npython3 tools/end2end/eval_end2end.py end2end/gt end2end/pretrain\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b`\u00b7` \u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u548c\u200b\u8bc4\u4f30\u200b\n# \u200b\u9700\u200b\u624b\u52a8\u200b\u4fee\u6539\u200b\u540e\u200b\u5904\u7406\u51fd\u6570\u200b\npython3 tools/end2end/convert_ppocr_label.py --mode=pred --label_path=infer/post/system_results.txt --save_folder=end2end/post\npython3 tools/end2end/eval_end2end.py end2end/gt end2end/post\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune \u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u548c\u200b\u8bc4\u4f30\u200b\npython3 tools/end2end/convert_ppocr_label.py --mode=pred --label_path=infer/fine-tune/system_results.txt --save_folder=end2end/fine-tune\npython3 tools/end2end/eval_end2end.py end2end/gt end2end/fine-tune\n\n# PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune +\u200b\u91cf\u5316\u200b\uff0cPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune +\u200b\u91cf\u5316\u200b \u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u548c\u200b\u8bc4\u4f30\u200b\npython3 tools/end2end/convert_ppocr_label.py --mode=pred --label_path=infer/quant/system_results.txt --save_folder=end2end/quant\npython3 tools/end2end/eval_end2end.py end2end/gt end2end/quant\n</code></pre> <p>\u200b\u65e5\u5fd7\u200b\u5982\u4e0b\u200b:</p> <pre><code>The convert label saved in end2end/gt\nThe convert label saved in end2end/pretrain\nstart testing...\nhit, dt_count, gt_count 2 5988 5006\ncharacter_acc: 70.42%\navg_edit_dist_field: 2.37\navg_edit_dist_img: 2.37\nprecision: 0.03%\nrecall: 0.04%\nfmeasure: 0.04%\nThe convert label saved in end2end/post\nstart testing...\nhit, dt_count, gt_count 4224 5988 5006\ncharacter_acc: 81.59%\navg_edit_dist_field: 1.47\navg_edit_dist_img: 1.47\nprecision: 70.54%\nrecall: 84.38%\nfmeasure: 76.84%\nThe convert label saved in end2end/fine-tune\nstart testing...\nhit, dt_count, gt_count 4286 4898 5006\ncharacter_acc: 94.16%\navg_edit_dist_field: 0.47\navg_edit_dist_img: 0.47\nprecision: 87.51%\nrecall: 85.62%\nfmeasure: 86.55%\nThe convert label saved in end2end/quant\nstart testing...\nhit, dt_count, gt_count 4349 4951 5006\ncharacter_acc: 94.13%\navg_edit_dist_field: 0.47\navg_edit_dist_img: 0.47\nprecision: 87.84%\nrecall: 86.88%\nfmeasure: 87.36%\n</code></pre> <p>\u200b\u5404\u4e2a\u200b\u65b9\u6848\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b \u200b\u6307\u6807\u200b PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 0.04% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 78.27% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune 87.14% PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b 88.00% <p>\u200b\u4ece\u200b\u7ed3\u679c\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5bf9\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0d\u200b\u505a\u200b\u4fee\u6539\u200b\uff0c\u200b\u53ea\u200b\u6839\u636e\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5177\u4f53\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u4fee\u6539\u200b\u5c31\u200b\u80fd\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5230\u200b78.27%\uff0c\u200b\u5728\u200bCCPD\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b fine-tune \u200b\u540e\u200b\u6307\u6807\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b87.14%, \u200b\u5728\u200b\u7ecf\u8fc7\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u4e4b\u540e\u200b\uff0c\u200b\u7531\u4e8e\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200brecall\u200b\u53d8\u9ad8\u200b\uff0c\u200b\u6307\u6807\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b88%\u3002\u200b\u4f46\u662f\u200b\u8fd9\u4e2a\u200b\u7ed3\u679c\u200b\u4ecd\u65e7\u200b\u4e0d\u200b\u7b26\u5408\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b+\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u771f\u5b9e\u6027\u200b\u80fd\u200b(99%*94%=93%)\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b base case \u200b\u8fdb\u884c\u200b\u5177\u4f53\u5206\u6790\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5f88\u591a\u200b\u4e0d\u200b\u7b26\u5408\u200b\u8f66\u724c\u200b\u6807\u6ce8\u200b\u7684\u200b\u6587\u5b57\u200b\u88ab\u200b\u8bc6\u522b\u200b\u51fa\u6765\u200b, \u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u7b80\u5355\u200b\u7684\u200b\u8fc7\u6ee4\u200b\u6765\u200b\u63d0\u5347\u200bprecision</p> <p>\u200b\u4e3a\u4e86\u200b\u5feb\u901f\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b <code>tools/end2end/convert_ppocr_label.py</code> \u200b\u811a\u672c\u200b\u7684\u200b 58 \u200b\u884c\u200b\u52a0\u5165\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5bf9\u975e\u200b8\u200b\u4e2a\u5b57\u7b26\u200b\u7684\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u8fc7\u6ee4\u200b</p> <pre><code>if len(txt) != 8: # \u200b\u8f66\u724c\u200b\u5b57\u7b26\u4e32\u200b\u957f\u5ea6\u200b\u4e3a\u200b8\n    continue\n</code></pre> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u901a\u8fc7\u200b\u53ef\u89c6\u5316\u200bbox\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u6709\u200b\u5f88\u591a\u200b\u6846\u200b\u90fd\u200b\u662f\u200b\u7ad6\u76f4\u200b\u7ffb\u8f6c\u200b\u4e4b\u540e\u200b\u7684\u200b\u6846\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6ca1\u6709\u200b\u5b8c\u5168\u200b\u6846\u4f4f\u200b\u8f66\u724c\u200b\u8fb9\u754c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u6846\u200b\u7684\u200b\u7ad6\u76f4\u200b\u7ffb\u8f6c\u200b\u4ee5\u53ca\u200b\u8f7b\u5fae\u200b\u6269\u5927\u200b\uff0c\u200b\u793a\u610f\u56fe\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u4fee\u6539\u200b\u524d\u540e\u200b\u4e2a\u200b\u65b9\u6848\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u5404\u4e2a\u200b\u65b9\u6848\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b base A:\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u8fc7\u6ee4\u200b B:use_dilation C:flip_box best PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 0.04% 0.08% 0.02% 0.05% 0.00%(A) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 78.27% 90.84% 78.61% 79.43% 91.66%(A+B+C) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune 87.14% 90.40% 87.66% 89.98% 92.50%(A+B+C) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b 88.00% 90.54% 88.50% 89.46% 92.02%(A+B+C) <p>\u200b\u4ece\u200b\u7ed3\u679c\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5bf9\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0d\u200b\u505a\u200b\u4fee\u6539\u200b\uff0c\u200b\u53ea\u200b\u6839\u636e\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5177\u4f53\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u4fee\u6539\u200b\u5c31\u200b\u80fd\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5230\u200b91.66%\uff0c\u200b\u5728\u200bCCPD\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b fine-tune \u200b\u540e\u200b\u6307\u6807\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b92.5%, \u200b\u5728\u200b\u7ecf\u8fc7\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u4e4b\u540e\u200b\uff0c\u200b\u6307\u6807\u200b\u53d8\u4e3a\u200b92.02%\u3002</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#44","title":"4.4 \u200b\u90e8\u7f72","text":""},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#paddle-inference-python","title":"\u57fa\u4e8e\u200b Paddle Inference \u200b\u7684\u200bpython\u200b\u63a8\u7406","text":"<p>\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5206\u522b\u200b fine-tune \u200b\u5e76\u200b\u5bfc\u51fa\u200b\u4e3a\u200binference\u200b\u6a21\u578b\u200b\u4e4b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u57fa\u4e8e\u200b Paddle Inference \u200b\u8fdb\u884c\u200b\u7aef\u5230\u200b\u7aef\u200b\u63a8\u7406\u200b\u5e76\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\u3002</p> <pre><code>python tools/infer/predict_system.py \\\n    --det_model_dir=output/CCPD/det/infer/ \\\n    --rec_model_dir=output/CCPD/rec/infer/ \\\n    --image_dir=\"/home/aistudio/data/CCPD2020/ccpd_green/test/04131106321839081-92_258-159&amp;509_530&amp;611-527&amp;611_172&amp;599_159&amp;509_530&amp;525-0_0_3_32_30_31_30_30-109-106.jpg\" \\\n    --rec_image_shape=3,48,320\n</code></pre> <p>\u200b\u63a8\u7406\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b</p> <p></p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_5","title":"\u7aef\u4fa7\u200b\u90e8\u7f72","text":"<p>\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\u6211\u4eec\u200b\u91c7\u7528\u200b\u57fa\u4e8e\u200b PaddleLite \u200b\u7684\u200b cpp \u200b\u63a8\u7406\u200b\u3002Paddle Lite\u200b\u662f\u200b\u98de\u6868\u200b\u8f7b\u91cf\u5316\u200b\u63a8\u7406\u200b\u5f15\u64ce\u200b\uff0c\u200b\u4e3a\u200b\u624b\u673a\u200b\u3001IOT\u200b\u7aef\u200b\u63d0\u4f9b\u200b\u9ad8\u6548\u200b\u63a8\u7406\u200b\u80fd\u529b\u200b\uff0c\u200b\u5e76\u200b\u5e7f\u6cdb\u200b\u6574\u5408\u200b\u8de8\u5e73\u53f0\u200b\u786c\u4ef6\u200b\uff0c\u200b\u4e3a\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\u53ca\u200b\u5e94\u7528\u200b\u843d\u5730\u200b\u95ee\u9898\u200b\u63d0\u4f9b\u200b\u8f7b\u91cf\u5316\u200b\u7684\u200b\u90e8\u7f72\u200b\u65b9\u6848\u200b\u3002\u200b\u5177\u4f53\u200b\u53ef\u200b\u53c2\u8003\u200b PaddleOCR lite\u200b\u6559\u7a0b\u200b</p>"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#45","title":"4.5 \u200b\u5b9e\u9a8c\u200b\u603b\u7ed3","text":"<p>\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u76f4\u63a5\u200b\u8bc4\u4f30\u200b\u548c\u200b fine-tune \u200b\u548c\u200b fine-tune +\u200b\u91cf\u5316\u200b3\u200b\u79cd\u200b\u65b9\u6848\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5e76\u200b\u57fa\u4e8e\u200bPaddleOCR lite\u200b\u6559\u7a0b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u901f\u5ea6\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u6307\u6807\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u68c0\u6d4b\u200b</li> </ul> \u200b\u65b9\u6848\u200b hmeans \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b(lite) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 76.12% 2.5M 233ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 99.00% 2.5M 233ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune + \u200b\u91cf\u5316\u200b 98.91% 1.0M 189ms <ul> <li>\u200b\u8bc6\u522b\u200b</li> </ul> \u200b\u65b9\u6848\u200b acc \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b(lite) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b 0.00% 10.3M 4.2ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u63a5\u200b\u9884\u6d4b\u200b+\u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 90.97% 10.3M 4.2ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune 94.54% 10.3M 4.2ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b fine-tune + \u200b\u91cf\u5316\u200b 93.40% 4.8M 1.8ms <ul> <li>\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\uff1a</li> </ul> \u200b\u65b9\u6848\u200b fmeasure \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b(lite) PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b 0.08% 12.8M 298ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b + \u200b\u540e\u5904\u7406\u200b\u53bb\u6389\u200b\u591a\u200b\u8bc6\u522b\u200b\u7684\u200b<code>\u00b7</code> 91.66% 12.8M 298ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune 92.50% 12.8M 298ms PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b  PP-OCRv3\u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u8bc6\u522b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b+fine-tune+\u200b\u91cf\u5316\u200b 92.02% 5.80M 224ms"},{"location":"en/applications/%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB.html#_6","title":"\u7ed3\u8bba","text":"<p>PP-OCRv3\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5728\u200b\u672a\u200b\u7ecf\u8fc7\u200bfine-tune\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5728\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4e5f\u200b\u6709\u200b\u4e00\u5b9a\u200b\u7684\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u7ecf\u8fc7\u200b fine-tune \u200b\u540e\u200b\u80fd\u591f\u200b\u6781\u5927\u200b\u7684\u200b\u63d0\u5347\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fbe\u5230\u200b99%\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u540e\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u51e0\u4e4e\u200b\u65e0\u635f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u538b\u7f29\u200b60%\u3002</p> <p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200b\u672a\u200b\u7ecf\u8fc7\u200bfine-tune\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5728\u200b\u8f66\u724c\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b0\uff0c\u200b\u4f46\u662f\u200b\u7ecf\u8fc7\u200b\u5206\u6790\u200b\u53ef\u4ee5\u200b\u77e5\u9053\u200b\uff0c\u200b\u6a21\u578b\u200b\u5927\u90e8\u5206\u200b\u5b57\u7b26\u200b\u90fd\u200b\u9884\u6d4b\u200b\u6b63\u786e\u200b\uff0c\u200b\u4f46\u662f\u200b\u4f1a\u591a\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u7279\u6b8a\u5b57\u7b26\u200b\uff0c\u200b\u53bb\u6389\u200b\u8fd9\u4e2a\u200b\u7279\u6b8a\u5b57\u7b26\u200b\u540e\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fbe\u5230\u200b90%\u3002PP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200b\u7ecf\u8fc7\u200b fine-tune \u200b\u540e\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\uff0c\u200b\u8fbe\u5230\u200b94.4%\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u540e\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u538b\u7f29\u200b53%\uff0c\u200b\u4f46\u662f\u200b\u7531\u4e8e\u200b\u6570\u636e\u91cf\u200b\u591a\u5c11\u200b\uff0c\u200b\u5e26\u6765\u200b\u4e86\u200b1%\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u4ece\u7aef\u200b\u5230\u200b\u7aef\u200b\u7ed3\u679c\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5bf9\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0d\u200b\u505a\u200b\u4fee\u6539\u200b\uff0c\u200b\u53ea\u200b\u6839\u636e\u200b\u573a\u666f\u200b\u4e0b\u200b\u7684\u200b\u5177\u4f53\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u4fee\u6539\u200b\u5c31\u200b\u80fd\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u5230\u200b91.66%\uff0c\u200b\u5728\u200bCCPD\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b fine-tune \u200b\u540e\u200b\u6307\u6807\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b92.5%, \u200b\u5728\u200b\u7ecf\u8fc7\u200b\u91cf\u5316\u200b\u8bad\u7ec3\u200b\u4e4b\u540e\u200b\uff0c\u200b\u6307\u6807\u200b\u8f7b\u5fae\u200b\u4e0b\u964d\u200b\u5230\u200b92.02%\u200b\u4f46\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u964d\u4f4e\u200b54%\u3002</p>"},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html","title":"\u9ad8\u7cbe\u5ea6\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSVTR","text":""},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>PP-OCRv3\u200b\u662f\u200b\u767e\u5ea6\u200b\u5f00\u6e90\u200b\u7684\u200b\u8d85\u200b\u8f7b\u91cf\u7ea7\u200b\u573a\u666f\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u6a21\u578b\u5e93\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8d85\u200b\u8f7b\u91cf\u200b\u7684\u200b\u573a\u666f\u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSVTR_LCNet\u200b\u4f7f\u7528\u200b\u4e86\u200bSVTR\u200b\u7b97\u6cd5\u200b\u7ed3\u6784\u200b\u3002\u200b\u4e3a\u4e86\u200b\u4fdd\u8bc1\u200b\u901f\u5ea6\u200b\uff0cSVTR_LCNet\u200b\u5c06\u200bSVTR\u200b\u6a21\u578b\u200b\u7684\u200bLocal Blocks\u200b\u66ff\u6362\u200b\u4e3a\u200bLCNet\uff0c\u200b\u4f7f\u7528\u200b\u4e24\u5c42\u200bGlobal Blocks\u3002\u200b\u5728\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0cPP-OCRv3\u200b\u8bc6\u522b\u200b\u4e3b\u8981\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\uff08\u200b\u8be6\u7ec6\u200b\u6280\u672f\u200b\u62a5\u544a\u200b\uff09\uff1a</p> <ul> <li>GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1b</li> <li>UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\uff1b</li> <li>UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002</li> </ul> <p>\u200b\u5176\u4e2d\u200b UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b \u200b\u4f7f\u7528\u200b\u4e86\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200bSVTR\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u7684\u200b\u5237\u5e93\u200b\uff0c\u200b\u8be5\u200b\u6a21\u578b\u200b\u5728\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u5bf9\u6bd4\u200b\u5982\u4e0b\u200b\u8868\u200b\u3002</p> \u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b \u200b\u6a21\u578b\u200b UIM \u200b\u7cbe\u5ea6\u200b PP-OCRv3 SVTR_LCNet w/o 78.40% PP-OCRv3 SVTR_LCNet w 79.40% SVTR SVTR-Tiny - 82.50% <p>aistudio\u200b\u9879\u76ee\u200b\u94fe\u63a5\u200b: \u200b\u9ad8\u7cbe\u5ea6\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200bSVTR</p>"},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html#2-svtr","title":"2. SVTR\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u4f7f\u7528","text":""},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html#_1","title":"\u73af\u5883\u200b\u51c6\u5907","text":"<p>\u200b\u672c\u200b\u4efb\u52a1\u200b\u57fa\u4e8e\u200bAistudio\u200b\u5b8c\u6210\u200b, \u200b\u5177\u4f53\u200b\u73af\u5883\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b: Linux</li> <li>PaddlePaddle: 2.3</li> <li>PaddleOCR: dygraph</li> </ul> <p>\u200b\u4e0b\u8f7d\u200bPaddleOCR\u200b\u4ee3\u7801\u200b</p> <pre><code>git clone -b dygraph https://github.com/PaddlePaddle/PaddleOCR\n</code></pre> <p>\u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\u5e93\u200b</p> <pre><code>pip install -r PaddleOCR/requirements.txt -i https://mirror.baidu.com/pypi/simple\n</code></pre>"},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html#_2","title":"\u5feb\u901f\u200b\u4f7f\u7528","text":"<p>\u200b\u83b7\u53d6\u200bSVTR\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bf7\u200b\u52a0\u5165\u200bPaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff0c\u200b\u83b7\u53d6\u200b20G OCR\u200b\u5b66\u4e60\u200b\u5927\u793c\u5305\u200b\uff08\u200b\u5185\u542b\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300b\u200b\u7535\u5b50\u4e66\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u56de\u653e\u200b\u89c6\u9891\u200b\u3001\u200b\u524d\u6cbf\u200b\u8bba\u6587\u200b\u7b49\u200b\u91cd\u78c5\u200b\u8d44\u6599\u200b\uff09</p> <ul> <li>PaddleX\u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b\u9891\u9053\u200b\uff1ahttps://aistudio.baidu.com/community/channel/610</li> </ul> <pre><code># \u200b\u89e3\u538b\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\ntar xf svtr_ch_high_accuracy.tar\n</code></pre> <p>\u200b\u9884\u6d4b\u200b\u4e2d\u6587\u200b\u6587\u672c\u200b\uff0c\u200b\u4ee5\u4e0b\u200b\u56fe\u4e3a\u4f8b\u200b\uff1a </p> <p>\u200b\u9884\u6d4b\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code># CPU\u200b\u9884\u6d4b\u200b\npython tools/infer_rec.py -c configs/rec/rec_svtrnet_ch.yml -o Global.pretrained_model=./svtr_ch_high_accuracy/best_accuracy Global.infer_img=./doc/imgs_words/ch/word_1.jpg Global.use_gpu=False\n\n# GPU\u200b\u9884\u6d4b\u200b\n#python tools/infer_rec.py -c configs/rec/rec_svtrnet_ch.yml -o Global.pretrained_model=./svtr_ch_high_accuracy/best_accuracy Global.infer_img=./doc/imgs_words/ch/word_1.jpg Global.use_gpu=True\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6700\u540e\u200b\u6253\u5370\u200b\u7ed3\u679c\u200b\u4e3a\u200b</p> <ul> <li>result: \u200b\u97e9\u56fd\u200b\u5c0f\u9986\u200b    0.9853458404541016</li> </ul> <p>0.9853458404541016\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u7f6e\u4fe1\u5ea6\u200b\u3002</p>"},{"location":"en/applications/%E9%AB%98%E7%B2%BE%E5%BA%A6%E4%B8%AD%E6%96%87%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html#_3","title":"\u63a8\u7406\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e0e\u200b\u9884\u6d4b","text":"<p>inference \u200b\u6a21\u578b\u200b\uff08paddle.jit.save\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\uff09 \u200b\u4e00\u822c\u200b\u662f\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u628a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u548c\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4fdd\u5b58\u200b\u5728\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u56fa\u5316\u200b\u6a21\u578b\u200b\uff0c\u200b\u591a\u200b\u7528\u4e8e\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u573a\u666f\u200b\u3002 \u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200bcheckpoints\u200b\u6a21\u578b\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u7684\u200b\u53ea\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u591a\u200b\u7528\u4e8e\u200b\u6062\u590d\u200b\u8bad\u7ec3\u200b\u7b49\u200b\u3002 \u200b\u4e0e\u200bcheckpoints\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\uff0cinference \u200b\u6a21\u578b\u200b\u4f1a\u200b\u989d\u5916\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u90e8\u7f72\u200b\u3001\u200b\u52a0\u901f\u200b\u63a8\u7406\u200b\u4e0a\u200b\u6027\u80fd\u4f18\u8d8a\u200b\uff0c\u200b\u7075\u6d3b\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u9002\u5408\u200b\u4e8e\u200b\u5b9e\u9645\u200b\u7cfb\u7edf\u96c6\u6210\u200b\u3002</p> <p>\u200b\u8fd0\u884c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8f6c\u200binference\u200b\u6a21\u578b\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>python tools/export_model.py -c configs/rec/rec_svtrnet_ch.yml -o Global.pretrained_model=./svtr_ch_high_accuracy/best_accuracy Global.save_inference_dir=./inference/svtr_ch\n</code></pre> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>inference/svtr_ch/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre> <p>inference\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\uff0c\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># CPU\u200b\u9884\u6d4b\u200b\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/ch/word_1.jpg\" --rec_algorithm='SVTR' --rec_model_dir=./inference/svtr_ch/ --rec_image_shape='3, 32, 320'  --rec_char_dict_path=ppocr/utils/ppocr_keys_v1.txt --use_gpu=False\n\n# GPU\u200b\u9884\u6d4b\u200b\n#python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/ch/word_1.jpg\" --rec_algorithm='SVTR' --rec_model_dir=./inference/svtr_ch/ --rec_image_shape='3, 32, 320'  --rec_char_dict_path=ppocr/utils/ppocr_keys_v1.txt --use_gpu=True\n</code></pre> <p>\u200b\u6ce8\u610f\u200b</p> <ul> <li>\u200b\u4f7f\u7528\u200bSVTR\u200b\u7b97\u6cd5\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u6307\u5b9a\u200b--rec_algorithm='SVTR'</li> <li>\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u5b57\u5178\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b--rec_char_dict_path=ppocr/utils/ppocr_keys_v1.txt\u200b\u4fee\u6539\u200b\u4e3a\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u5b57\u5178\u200b</li> <li>--rec_image_shape='3, 32, 320' \u200b\u8be5\u200b\u53c2\u6570\u200b\u4e0d\u80fd\u200b\u53bb\u6389\u200b</li> </ul>"},{"location":"en/community/code_and_doc.html","title":"Appendix","text":"<p>This appendix contains python, document specifications and Pull Request process.</p>"},{"location":"en/community/code_and_doc.html#appendix-1python-code-specification","title":"Appendix 1\uff1aPython Code Specification","text":"<p>The Python code of PaddleOCR follows PEP8 Specification, some of the key concerns include the following</p> <ul> <li> <p>Space</p> <ul> <li>Spaces should be added after commas, semicolons, colons, not before them</li> </ul> <pre><code># true:\nprint(x, y)\n\n# false:\nprint(x , y)\n</code></pre> <ul> <li>When specifying a keyword parameter or default parameter value in a function, do not use spaces on both sides of it</li> </ul> <pre><code># true:\ndef complex(real, imag=0.0)\n# false:\ndef complex(real, imag = 0.0)\n</code></pre> </li> <li> <p>comment</p> <ul> <li>Inline comments: inline comments are indicated by the<code>#</code>sign. Two spaces should be left between code and<code>#</code>, and one space should be left between<code>#</code>and comments, for example</li> </ul> <pre><code>x = x + 1  # Compensate for border\n</code></pre> <ul> <li> <p>Functions and methods: The definition of each function should include the following:</p> <ul> <li>Function description: Utility, input and output of function</li> <li>Args: Name and description of each parameter</li> <li>Returns: The meaning and type of the return value</li> </ul> <pre><code>def fetch_bigtable_rows(big_table, keys, other_silly_variable=None):\n    \"\"\"Fetches rows from a Bigtable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by big_table.  Silly things may happen if\n    other_silly_variable is not None.\n\n    Args:\n        big_table: An open Bigtable Table instance.\n        keys: A sequence of strings representing the key of each table row\n            to fetch.\n        other_silly_variable: Another optional variable, that has a much\n            longer name than the other args, and which does nothing.\n\n    Returns:\n        A dict mapping keys to the corresponding table row data\n        fetched. Each row is represented as a tuple of strings. For\n        example:\n\n        {'Serak': ('Rigel VII', 'Preparer'),\n        'Zim': ('Irk', 'Invader'),\n        'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        If a key from the keys argument is missing from the dictionary,\n        then that row was not found in the table.\n    \"\"\"\n    pass\n</code></pre> </li> </ul> </li> </ul>"},{"location":"en/community/code_and_doc.html#appendix-2-document-specification","title":"Appendix 2: Document Specification","text":""},{"location":"en/community/code_and_doc.html#21-overall-description","title":"2.1 Overall Description","text":"<ul> <li> <p>Document Location: If you add new features to your original Markdown file, please Do not re-create a new file. If you don't know where to add it, you can first PR the code and then ask the official in commit.</p> </li> <li> <p>New Markdown Document Name: Describe the content of the document in English, typically a combination of lowercase letters and underscores, such as <code>add_New_Algorithm.md</code></p> </li> <li> <p>New Markdown Document Format: Catalog - Body - FAQ</p> </li> </ul> <p>The directory generation method can use this site Automatically extract directories after copying MD contents, and then add `</p> <ul> <li>English and Chinese: Any changes or additions to the document need to be made in both Chinese and English documents.</li> </ul>"},{"location":"en/community/code_and_doc.html#22-format-specification","title":"2.2 Format Specification","text":"<ul> <li> <p>Title format: The document title format follows the format of: Arabic decimal point combination-space-title (for example, <code>2.1 XXXX</code>, <code>2.XXXX</code>)</p> </li> <li> <p>Code block: Displays code in code block format that needs to be run, describing the meaning of command parameters before the code block. for example:</p> </li> </ul> <p>Pipeline of detection + direction Classify + recognition: Vertical text can be recognized after set direction classifier parameters<code>--use_angle_cls true</code>.</p> <pre><code>paddleocr --image_dir ./imgs/11.jpg --use_angle_cls true\n</code></pre> <ul> <li> <p>Variable Rrferences: If code variables or command parameters are referenced in line, they need to be represented in line code, for example, above <code>--use_angle_cls true</code> with one space in front and one space in back</p> </li> <li> <p>Uniform naming: e.g. PP-OCRv2, PP-OCR mobile, <code>paddleocr</code> whl package, PPOCRLabel, Paddle Lite, etc.</p> </li> <li> <p>Supplementary notes: Supplementary notes by reference format <code>&gt;</code>.</p> </li> <li> <p>Picture: If a picture is added to the description document, specify the naming of the picture (describing its content) and add the picture under <code>doc/</code>.</p> </li> <li> <p>Title: Capitalize the first letter of each word in the title.</p> </li> </ul>"},{"location":"en/community/code_and_doc.html#appendix-3-pull-request-description","title":"Appendix 3: Pull Request Description","text":""},{"location":"en/community/code_and_doc.html#31-paddleocr-branch-description","title":"3.1 PaddleOCR Branch Description","text":"<p>PaddleOCR will maintain two branches in the future, one for each:</p> <ul> <li>release/x.x family branch: stable release version branch, also the default branch. PaddleOCR releases a new release branch based on feature updates and adapts to the release version of Paddle. As versions iterate, more and more release/x.x family branches are maintained by default with the latest version of the release branch.</li> <li>dygraph branch: For the development branch, adapts the dygraph version of the Paddle dynamic graph to primarily develop new functionality. If you need to redevelop, choose the dygraph branch. To ensure that the dygraph branch pulls out the release/x.x branch when needed, the code for the dygraph branch can only use the valid API in the latest release branch of Paddle. That is, if a new API has been developed in the Paddle dygraph branch but has not yet appeared in the release branch code, do not use it in Paddle OCR. In addition, performance optimization, parameter tuning, policy updates that do not involve API can be developed normally.</li> </ul> <p>The historical branch of PaddleOCR will no longer be maintained in the future. These branches will continue to be maintained, considering that some of you may still be using them:</p> <p>Develop branch: This branch was used for the development and testing of static diagrams and is currently compatible with version &gt;=1.7. If you have special needs, you can also use this branch to accommodate older versions of Paddle, but you won't update your code until you fix the bug.</p> <p>PaddleOCR welcomes you to actively contribute code to repo. Here are some basic processes for contributing code.</p>"},{"location":"en/community/code_and_doc.html#32-paddleocr-code-submission-process-and-specification","title":"3.2 PaddleOCR Code Submission Process And Specification","text":"<p>If you are familiar with Git use, you can jump directly to Some Conventions For Submitting Code in 3.2.10</p>"},{"location":"en/community/code_and_doc.html#321-create-your-remote-repo","title":"3.2.1 Create Your <code>Remote Repo</code>","text":"<p>In PaddleOCR GitHub Home Click the <code>Fork</code> button in the upper left corner to create a <code>remote repo</code>in your personal directory, such as <code>https://github.com/ {your_name}/PaddleOCR</code>.</p> <p></p> <p>Clone <code>Remote repo</code></p> <pre><code># pull code of develop branch\ngit clone https://github.com/{your_name}/PaddleOCR.git -b dygraph\ncd PaddleOCR\n</code></pre> <p>Clone failures are mostly due to network reasons, try again later or configure the proxy</p>"},{"location":"en/community/code_and_doc.html#322-login-and-connect-using-token","title":"3.2.2 Login And Connect Using Token","text":"<p>Start by viewing the information for the current <code>remote repo</code>.</p> <pre><code>git remote -v\n# origin    https://github.com/{your_name}/PaddleOCR.git (fetch)\n# origin    https://github.com/{your_name}/PaddleOCR.git (push)\n</code></pre> <p>Only the information of the clone <code>remote repo</code>, i.e. the PaddleOCR under your username, is available. Due to the change in Github's login method, you need to reconfigure the <code>remote repo</code> address by means of a Token. The token is generated as follows:</p> <ol> <li> <p>Find Personal Access Tokens: Click on your avatar in the upper right corner of the Github page and choose Settings --&gt; Developer settings --&gt; Personal access tokens,</p> </li> <li> <p>Click Generate new token: Fill in the token name in Note, such as 'paddle'. In Select scopes, select repo (required), admin:repo_hook, delete_repo, etc. You can check them according to your needs. Then click Generate token to generate the token, and finally copy the generated token.</p> </li> </ol> <p>Delete the original origin configuration</p> <pre><code>git remote rm origin\n</code></pre> <p>Change the remote branch to <code>https://oauth2:{token}@github.com/{your_name}/PaddleOCR.git</code>. For example, if the token value is 12345 and your user name is PPOCR, run the following command</p> <pre><code>git remote add origin https://oauth2:12345@github.com/PPOCR/PaddleOCR.git\n</code></pre> <p>This establishes a connection to our own <code>remote repo</code>. Next we create a remote host of the original PaddleOCR repo, named upstream.</p> <pre><code>git remote add upstream https://github.com/PaddlePaddle/PaddleOCR.git\n</code></pre> <p>Use <code>git remote -v</code> to view current <code>remote warehouse</code> information, output as follows, found to include two origin and two upstream of <code>remote repo</code> .</p> <pre><code>origin    https://github.com/{your_name}/PaddleOCR.git (fetch)\norigin    https://github.com/{your_name}/PaddleOCR.git (push)\nupstream    https://github.com/PaddlePaddle/PaddleOCR.git (fetch)\nupstream    https://github.com/PaddlePaddle/PaddleOCR.git (push)\n</code></pre> <p>This is mainly to keep the local repository up to date when subsequent pull request (PR) submissions are made.</p>"},{"location":"en/community/code_and_doc.html#323-create-local-branch","title":"3.2.3 Create Local Branch","text":"<p>First get the latest code of upstream, then create a new_branch branch based on the dygraph of the upstream repo (upstream).</p> <pre><code>git fetch upstream\ngit checkout -b new_branch upstream/dygraph\n</code></pre> <p>If for a newly forked PaddleOCR project, the user's remote repo (origin) has the same branch updates as the upstream repository (upstream), you can also create a new local branch based on the default branch of the origin repo or a specified branch with the following command</p> <pre><code># Create new_branch branch on user remote repo (origin) based on develop branch\ngit checkout -b new_branch origin/develop\n# Create new_branch branch based on upstream remote repo develop branch\n# If you need to create a new branch from upstream,\n# you need to first use git fetch upstream to get upstream code\ngit checkout -b new_branch upstream/develop\n</code></pre> <p>The final switch to the new branch is displayed with the following output information.</p> <p>Branch new_branch set up to track remote branch develop from upstream. Switched to a new branch 'new_branch'</p> <p>After switching branches, file changes can be made on this branch</p>"},{"location":"en/community/code_and_doc.html#324-use-pre-commit-hook","title":"3.2.4 Use Pre-Commit Hook","text":"<p>Paddle developers use the pre-commit tool to manage Git pre-submit hooks. It helps us format the source code (C++, Python) and automatically check for basic things (such as having only one EOL per file, not adding large files to Git) before committing it.</p> <p>The pre-commit test is part of the unit test in Travis-CI. PR that does not satisfy the hook cannot be submitted to PaddleOCR. Install it first and run it in the current directory\uff1a</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <ol> <li> <p>Paddle uses clang-format to adjust the C/C++ source code format. Make sure the <code>clang-format</code> version is above 3.8.</p> </li> <li> <p>Yapf installed through pip install pre-commit is slightly different from conda install-c conda-forge pre-commit, and PaddleOCR developers use <code>pip install pre-commit</code>.</p> </li> </ol>"},{"location":"en/community/code_and_doc.html#325-modify-and-submit-code","title":"3.2.5 Modify And Submit Code","text":"<p>If you make some changes on <code>README.Md</code> on PaddleOCR, you can view the changed file through <code>git status</code>, and then add the changed file using <code>git add</code>\u3002</p> <pre><code>git status # View change files\ngit add README.md\npre-commit\n</code></pre> <p>Repeat these steps until the pre-comit format check does not error. As shown below.</p> <p></p> <p>Use the following command to complete the submission.</p> <pre><code>git commit -m \"your commit info\"\n</code></pre>"},{"location":"en/community/code_and_doc.html#326-keep-local-repo-up-to-date","title":"3.2.6 Keep Local Repo Up To Date","text":"<p>Get the latest code for upstream and update the current branch. Here the upstream comes from section 2.2, <code>Connecting to a remote repo</code>.</p> <pre><code>git fetch upstream\n# If you want to commit to another branch, you need to pull code from another branch of upstream, here is develop\ngit pull upstream develop\n</code></pre>"},{"location":"en/community/code_and_doc.html#327-push-to-remote-repo","title":"3.2.7 Push To Remote Repo","text":"<pre><code>git push origin new_branch\n</code></pre>"},{"location":"en/community/code_and_doc.html#327-submit-pull-request","title":"3.2.7 Submit Pull Request","text":"<p>Click the new pull request to select the local branch and the target branch, as shown in the following figure. In the description of PR, fill in the functions completed by the PR. Next, wait for review, and if you need to modify something, update the corresponding branch in origin with the steps above.</p> <p></p>"},{"location":"en/community/code_and_doc.html#328-sign-cla-agreement-and-pass-unit-tests","title":"3.2.8 Sign CLA Agreement And Pass Unit Tests","text":"<p>Signing the CLA When submitting a Pull Request to PaddlePaddle for the first time, you need to sign a CLA (Contributor License Agreement) agreement to ensure that your code can be incorporated as follows:</p> <ol> <li> <p>Please check the Check section in PR, find the license/cla, and click on the right detail to enter the CLA website</p> </li> <li> <p>Click Sign in with GitHub to agree on the CLA website and when clicked, it will jump back to your Pull Request page</p> </li> </ol>"},{"location":"en/community/code_and_doc.html#329-delete-branch","title":"3.2.9 Delete Branch","text":"<ul> <li>Remove remote branch</li> </ul> <p>After PR is merged into the main repo, we can delete the branch of the remote repofrom the PR page.   You can also use <code>git push origin:branch name</code> to delete remote branches, such as:</p> <pre><code>git push origin :new_branch\n</code></pre> <ul> <li>Delete local branch</li> </ul> <pre><code># Switch to the development branch, otherwise the current branch cannot be deleted\ngit checkout develop\n\n# Delete new_ Branch Branch\ngit branch -D new_branch\n</code></pre>"},{"location":"en/community/code_and_doc.html#3210-some-conventions-for-submitting-code","title":"3.2.10 Some Conventions For Submitting Code","text":"<p>In order for official maintainers to better focus on the code itself when reviewing it, please follow the following conventions each time you submit your code:</p> <p>1\uff09Please ensure that the unit tests in Travis-CI pass smoothly. If not, indicate that there is a problem with the submitted code, and the official maintainer generally does not review it.</p> <p>2\uff09Before submitting a Pull Request.</p> <ul> <li>Note the number of commits.</li> </ul> <p>Reason: If you only modify one file and submit more than a dozen commits, each commit will only make a few modifications, which can be very confusing to the reviewer. The reviewer needs to look at each commit individually to see what changes have been made, and does not exclude the fact that changes between commits overlap each other.</p> <p>Suggestion: Keep as few commits as possible each time you submit, and supplement your last commit with git commit --amend. For multiple commits that have been Push to a remote warehouse, you can refer to squash commits after push.</p> <ul> <li>Note the name of each commit: it should reflect the content of the current commit, not be too arbitrary.</li> </ul> <p>3\uff09 If you have solved a problem, add in the first comment box of the Pull Request:fix #issue_number\uff0cThis will automatically close the corresponding Issue when the Pull Request is merged. Key words include:close, closes, closed, fix, fixes, fixed, resolve, resolves, resolved,please choose the right vocabulary. Detailed reference Closing issues via commit messages.</p> <p>In addition, in response to the reviewer's comments, you are requested to abide by the following conventions:</p> <p>1\uff09 Each review comment from an official maintainer would like a response, which would better enhance the contribution of the open source community.</p> <ul> <li>If you agree to the review opinion and modify it accordingly, give a simple Done.</li> <li>If you disagree with the review, please give your own reasons for refuting.</li> </ul> <p>2\uff09If there are many reviews:</p> <ul> <li>Please give an overview of the changes.</li> <li>Please reply with `start a review', not directly. The reason is that each reply sends an e-mail message, which can cause a mail disaster.</li> </ul>"},{"location":"en/community/community_contribution.html","title":"\u793e\u533a\u200b\u8d21\u732e","text":"<p>\u200b\u611f\u8c22\u200b\u5927\u5bb6\u200b\u957f\u4e45\u4ee5\u6765\u200b\u5bf9\u200bPaddleOCR\u200b\u7684\u200b\u652f\u6301\u200b\u548c\u200b\u5173\u6ce8\u200b\uff0c\u200b\u4e0e\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u5171\u540c\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u4e13\u4e1a\u200b\u3001\u200b\u548c\u8c10\u200b\u3001\u200b\u76f8\u4e92\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u662f\u200bPaddleOCR\u200b\u7684\u200b\u76ee\u6807\u200b\u3002\u200b\u672c\u200b\u6587\u6863\u200b\u5c55\u793a\u200b\u4e86\u200b\u5df2\u6709\u200b\u7684\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u3001\u200b\u5bf9\u4e8e\u200b\u5404\u7c7b\u200b\u8d21\u732e\u200b\u8bf4\u660e\u200b\u3001\u200b\u65b0\u200b\u7684\u200b\u673a\u4f1a\u200b\u4e0e\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5e0c\u671b\u200b\u8d21\u732e\u200b\u6d41\u7a0b\u200b\u66f4\u52a0\u200b\u9ad8\u6548\u200b\u3001\u200b\u8def\u5f84\u200b\u66f4\u52a0\u200b\u6e05\u6670\u200b\u3002</p> <p>PaddleOCR\u200b\u5e0c\u671b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bAI\u200b\u7684\u200b\u529b\u91cf\u200b\u52a9\u529b\u200b\u4efb\u4f55\u200b\u4e00\u4f4d\u200b\u6709\u200b\u68a6\u60f3\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u5b9e\u73b0\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u4eab\u53d7\u200b\u521b\u9020\u200b\u4ef7\u503c\u200b\u5e26\u6765\u200b\u7684\u200b\u6109\u60a6\u200b\u3002</p> <p> </p>"},{"location":"en/community/community_contribution.html#1","title":"1. \u200b\u793e\u533a\u200b\u8d21\u732e","text":""},{"location":"en/community/community_contribution.html#11-paddleocr","title":"1.1 \u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u793e\u533a\u200b\u9879\u76ee","text":"\u7c7b\u522b\u200b \u200b\u9879\u76ee\u200b \u200b\u63cf\u8ff0\u200b \u200b\u5f00\u53d1\u8005\u200b \u200b\u901a\u7528\u200b\u5de5\u5177\u200b FastOCRLabel \u200b\u5b8c\u6574\u200b\u7684\u200bC#\u200b\u7248\u672c\u200b\u6807\u6ce8\u200bGUI \u200b\u5305\u5efa\u5f3a\u200b \u200b\u901a\u7528\u200b\u5de5\u5177\u200b DangoOCR\u200b\u79bb\u7ebf\u200b\u7248\u200b \u200b\u901a\u7528\u578b\u200b\u684c\u9762\u200b\u7ea7\u200b\u5373\u65f6\u200b\u7ffb\u8bd1\u200bGUI PantsuDango \u200b\u901a\u7528\u200b\u5de5\u5177\u200b scr2txt \u200b\u622a\u5c4f\u200b\u8f6c\u200b\u6587\u5b57\u200bGUI lstwzd \u200b\u901a\u7528\u200b\u5de5\u5177\u200b ocr_sdk OCR java SDK\u200b\u5de5\u5177\u7bb1\u200b Calvin \u200b\u901a\u7528\u200b\u5de5\u5177\u200b iocr IOCR \u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u677f\u200b\u8bc6\u522b\u200b(\u200b\u652f\u6301\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b) Calvin \u200b\u901a\u7528\u200b\u5de5\u5177\u200b Lmdb Dataset Format Conversion Tool \u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e2d\u200blmdb\u200b\u6570\u636e\u683c\u5f0f\u200b\u8f6c\u6362\u200b\u5de5\u5177\u200b OneYearIsEnough \u200b\u901a\u7528\u200b\u5de5\u5177\u200b \u200b\u7528\u200bpaddleocr\u200b\u6253\u9020\u200b\u4e00\u6b3e\u200b\u201c\u200b\u76d7\u5e55\u200b\u7b14\u8bb0\u200b\u201d \u200b\u7528\u200bPaddleOCR\u200b\u8bb0\u7b14\u8bb0\u200b kjf4096 \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b AI Studio\u200b\u9879\u76ee\u200b \u200b\u82f1\u6587\u200b\u89c6\u9891\u200b\u81ea\u52a8\u200b\u751f\u6210\u200b\u5b57\u5e55\u200b \u200b\u53f6\u6708\u200b\u6c34\u72d0\u200b \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b id_card_ocr \u200b\u8eab\u4efd\u8bc1\u200b\u590d\u5370\u4ef6\u200b\u8bc6\u522b\u200b baseli \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b Paddle_Table_Image_Reader \u200b\u80fd\u770b\u61c2\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\u7684\u200b\u6570\u636e\u200b\u52a9\u624b\u200b thunder95 \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b AI Studio\u200b\u9879\u76ee\u200b OCR\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u5bf9\u200b\u624b\u5199\u4f53\u200b\u8fdb\u884c\u200b\u8fc7\u6ee4\u200b daassh \u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8c03\u4f18\u200b AI Studio\u200b\u9879\u76ee\u200b \u200b\u7535\u8868\u200b\u8bfb\u6570\u200b\u548c\u200b\u7f16\u53f7\u200b\u8bc6\u522b\u200b \u200b\u6df1\u6e0a\u200b\u4e0a\u200b\u7684\u200b\u5751\u200b \u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8c03\u4f18\u200b AI Studio\u200b\u9879\u76ee\u200b LCD\u200b\u6db2\u6676\u200b\u5b57\u7b26\u200b\u68c0\u6d4b\u200b Dream\u200b\u62d2\u6770\u200b \u200b\u524d\u540e\u200b\u5904\u7406\u200b paddleOCRCorrectOutputs \u200b\u83b7\u53d6\u200bOCR\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u7684\u200bkey-value yuranusduke \u200b\u524d\u200b\u5904\u7406\u200b optlab OCR\u200b\u524d\u200b\u5904\u7406\u200b\u5de5\u5177\u7bb1\u200b\uff0c\u200b\u57fa\u4e8e\u200bQt\u200b\u548c\u200bLeptonica\u3002 GreatV \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCRSharp PaddleOCR\u200b\u7684\u200b.NET\u200b\u5c01\u88c5\u200b\u4e0e\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u3002 raoyutian \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleSharp PaddleOCR\u200b\u7684\u200b.NET\u200b\u5c01\u88c5\u200b\u4e0e\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\uff0c\u200b\u652f\u6301\u200b\u8de8\u5e73\u53f0\u200b\u3001GPU sdcb \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Streamlit-Demo \u200b\u4f7f\u7528\u200bStreamlit\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-PyWebIO-Demo \u200b\u4f7f\u7528\u200bPyWebIO\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Paddlejs-Vue-Demo \u200b\u4f7f\u7528\u200bPaddle.js\u200b\u548c\u200bVue\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Paddlejs-React-Demo \u200b\u4f7f\u7528\u200bPaddle.js\u200b\u548c\u200bReact\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5b66\u672f\u524d\u6cbf\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b AI Studio\u200b\u9879\u76ee\u200b StarNet-MobileNetV3\u200b\u7b97\u6cd5\u200b\u2013\u200b\u4e2d\u6587\u200b\u8bad\u7ec3\u200b xiaoyangyang2 \u200b\u5b66\u672f\u524d\u6cbf\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b ABINet-paddle ABINet\u200b\u7b97\u6cd5\u200b\u524d\u5411\u200b\u8fd0\u7b97\u200b\u7684\u200bpaddle\u200b\u5b9e\u73b0\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u5404\u200b\u90e8\u5206\u200b\u7684\u200b\u5b9e\u73b0\u200b\u7ec6\u8282\u200b\u5206\u6790\u200b Huntersdeng"},{"location":"en/community/community_contribution.html#12-paddleocr","title":"1.2 \u200b\u4e3a\u200bPaddleOCR\u200b\u65b0\u589e\u200b\u529f\u80fd","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b authorfu \u200b\u8d21\u732e\u200bAndroid(#340)\u200b\u548c\u200bxiadeye \u200b\u8d21\u732e\u200bIOS\u200b\u7684\u200bdemo\u200b\u4ee3\u7801\u200b(#325)</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b tangmq \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200bDocker\u200b\u5316\u200b\u90e8\u7f72\u200b\u670d\u52a1\u200b\uff0c\u200b\u652f\u6301\u200b\u5feb\u901f\u200b\u53d1\u5e03\u200b\u53ef\u200b\u8c03\u7528\u200b\u7684\u200bRestful API\u200b\u670d\u52a1\u200b(#507)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b lijinhan \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200bjava SpringBoot \u200b\u8c03\u7528\u200bOCR Hubserving\u200b\u63a5\u53e3\u200b\u5b8c\u6210\u200b\u5bf9\u200bOCR\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u7684\u200b\u4f7f\u7528\u200b(#1027)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Evezerest\uff0c ninetailskim\uff0c edencfc\uff0c BeyondYourself\uff0c 1084667371 \u200b\u8d21\u732e\u200b\u4e86\u200bPPOCRLabel \u200b\u7684\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b bupt906 \u200b\u8d21\u732e\u200bMicroNet\u200b\u7ed3\u6784\u200b\u4ee3\u7801\u200b(#5251)\u200b\u548c\u200b\u8d21\u732e\u200bOneCycle\u200b\u5b66\u4e60\u200b\u7387\u200b\u7b56\u7565\u200b\u4ee3\u7801\u200b(#5252)</li> </ul>"},{"location":"en/community/community_contribution.html#13","title":"1.3 \u200b\u4ee3\u7801\u200b\u4fee\u590d","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b zhangxin(Blog) \u200b\u8d21\u732e\u200b\u65b0\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u65b9\u5f0f\u200b\u3001\u200b\u6dfb\u52a0\u200b.gitgnore\u3001\u200b\u5904\u7406\u200b\u624b\u52a8\u200b\u8bbe\u7f6e\u200bPYTHONPATH\u200b\u73af\u5883\u53d8\u91cf\u200b\u7684\u200b\u95ee\u9898\u200b(#210)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b lyl120117 \u200b\u8d21\u732e\u200b\u6253\u5370\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u4ee3\u7801\u200b(#304)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b BeyondYourself \u200b\u7ed9\u200bPaddleOCR\u200b\u63d0\u4e86\u200b\u5f88\u591a\u200b\u975e\u5e38\u200b\u68d2\u200b\u7684\u200b\u5efa\u8bae\u200b\uff0c\u200b\u5e76\u200b\u7b80\u5316\u200b\u4e86\u200bPaddleOCR\u200b\u7684\u200b\u90e8\u5206\u200b\u4ee3\u7801\u200b\u98ce\u683c\u200b(so many commits)\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#14","title":"1.4 \u200b\u6587\u6863\u200b\u4f18\u5316\u200b\u4e0e\u200b\u7ffb\u8bd1","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b RangeKing\uff0cHustBestCat\uff0cv3fc\uff0c1084667371 \u200b\u8d21\u732e\u200b\u7ffb\u8bd1\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300bnotebook\u200b\u7535\u5b50\u4e66\u200b\u82f1\u6587\u7248\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b thunderstudying\uff0cRangeKing\uff0clivingbody\uff0c WZMIAOMIAO\uff0chaigang1975 \u200b\u8865\u5145\u200b\u591a\u4e2a\u200b\u82f1\u6587\u200bmarkdown\u200b\u6587\u6863\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b fanruinet \u200b\u6da6\u8272\u200b\u548c\u200b\u4fee\u590d\u200b35\u200b\u7bc7\u200b\u82f1\u6587\u200b\u6587\u6863\u200b(#5205)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Khanh Tran \u200b\u548c\u200b Karl Horky \u200b\u8d21\u732e\u200b\u4fee\u6539\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#15","title":"1.5 \u200b\u591a\u200b\u8bed\u8a00\u200b\u8bed\u6599","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b xiangyubo \u200b\u8d21\u732e\u200b\u624b\u5199\u200b\u4e2d\u6587\u200bOCR\u200b\u6570\u636e\u200b\u96c6\u200b(#321)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Mejans \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200b\u65b0\u200b\u8bed\u8a00\u200b\u5965\u514b\u200b\u897f\u5766\u8bed\u200bOccitan\u200b\u7684\u200b\u5b57\u5178\u200b\u548c\u200b\u8bed\u6599\u200b(#954)\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#2","title":"2. \u200b\u8d21\u732e\u200b\u8bf4\u660e","text":""},{"location":"en/community/community_contribution.html#21","title":"2.1 \u200b\u65b0\u589e\u200b\u529f\u80fd\u200b\u7c7b","text":"<p>PaddleOCR\u200b\u975e\u5e38\u200b\u6b22\u8fce\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u4ee5\u200bPaddleOCR\u200b\u4e3a\u200b\u6838\u5fc3\u200b\u7684\u200b\u5404\u79cd\u200b\u670d\u52a1\u200b\u3001\u200b\u90e8\u7f72\u200b\u5b9e\u4f8b\u200b\u4e0e\u200b\u8f6f\u4ef6\u5e94\u7528\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u8ba4\u8bc1\u200b\u7684\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u4f1a\u200b\u88ab\u200b\u6dfb\u52a0\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u8868\u4e2d\u200b\uff0c\u200b\u4e3a\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u589e\u52a0\u200b\u66dd\u5149\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200bPaddleOCR\u200b\u7684\u200b\u8363\u8000\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li> <p>\u200b\u9879\u76ee\u200b\u5f62\u5f0f\u200b\uff1a\u200b\u5b98\u65b9\u200b\u793e\u533a\u200b\u8ba4\u8bc1\u200b\u7684\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u5e94\u6709\u200b\u826f\u597d\u200b\u7684\u200b\u89c4\u8303\u200b\u548c\u200b\u7ed3\u6784\u200b\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u8fd8\u5e94\u200b\u914d\u5907\u200b\u4e00\u4e2a\u200b\u8be6\u7ec6\u200b\u7684\u200bREADME.md\uff0c\u200b\u8bf4\u660e\u200b\u9879\u76ee\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u3002\u200b\u901a\u8fc7\u200b\u5728\u200brequirements.txt\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u589e\u52a0\u200b\u4e00\u884c\u200b <code>paddleocr</code> \u200b\u53ef\u4ee5\u200b\u81ea\u52a8\u200b\u6536\u5f55\u200b\u5230\u200bPaddleOCR\u200b\u7684\u200busedby\u200b\u4e2d\u200b\u3002</p> </li> <li> <p>\u200b\u5408\u5165\u200b\u65b9\u5f0f\u200b\uff1a\u200b\u5982\u679c\u200b\u662f\u200b\u5bf9\u200bPaddleOCR\u200b\u73b0\u6709\u200b\u5de5\u5177\u200b\u7684\u200b\u66f4\u65b0\u200b\u5347\u7ea7\u200b\uff0c\u200b\u5219\u200b\u4f1a\u5408\u200b\u5165\u4e3b\u200brepo\u3002\u200b\u5982\u679c\u200b\u4e3a\u200bPaddleOCR\u200b\u62d3\u5c55\u200b\u4e86\u200b\u65b0\u200b\u529f\u80fd\u200b\uff0c\u200b\u8bf7\u200b\u5148\u200b\u4e0e\u200b\u5b98\u65b9\u200b\u4eba\u5458\u200b\u8054\u7cfb\u200b\uff0c\u200b\u786e\u8ba4\u200b\u9879\u76ee\u200b\u662f\u5426\u200b\u5408\u200b\u5165\u4e3b\u200brepo\uff0c\u200b\u5373\u4f7f\u200b\u65b0\u200b\u529f\u80fd\u200b\u672a\u5408\u200b\u5165\u4e3b\u200brepo\uff0c\u200b\u6211\u4eec\u200b\u540c\u6837\u200b\u4e5f\u200b\u4f1a\u200b\u4ee5\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u9879\u76ee\u200b\u589e\u52a0\u200b\u66dd\u5149\u200b\u3002</p> </li> </ul>"},{"location":"en/community/community_contribution.html#22","title":"2.2 \u200b\u4ee3\u7801\u4f18\u5316","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u65f6\u200b\u9047\u5230\u200b\u4e86\u200b\u4ee3\u7801\u200bbug\u3001\u200b\u529f\u80fd\u200b\u4e0d\u200b\u7b26\u5408\u200b\u9884\u671f\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u60a8\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li> <p>Python\u200b\u4ee3\u7801\u200b\u89c4\u8303\u200b\u53ef\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b1\uff1aPython\u200b\u4ee3\u7801\u200b\u89c4\u8303\u200b\u3002</p> </li> <li> <p>\u200b\u63d0\u4ea4\u200b\u4ee3\u7801\u200b\u524d\u200b\u8bf7\u200b\u518d\u4e09\u200b\u786e\u8ba4\u200b\u4e0d\u4f1a\u200b\u5f15\u5165\u200b\u65b0\u200b\u7684\u200bbug\uff0c\u200b\u5e76\u200b\u5728\u200bPR\u200b\u4e2d\u200b\u63cf\u8ff0\u200b\u4f18\u5316\u200b\u70b9\u200b\u3002\u200b\u5982\u679c\u200b\u8be5\u200bPR\u200b\u89e3\u51b3\u200b\u4e86\u200b\u67d0\u4e2a\u200bissue\uff0c\u200b\u8bf7\u200b\u5728\u200bPR\u200b\u4e2d\u200b\u8fde\u63a5\u200b\u5230\u200b\u8be5\u200bissue\u3002\u200b\u6240\u6709\u200b\u7684\u200bPR\u200b\u90fd\u200b\u5e94\u8be5\u200b\u9075\u5b88\u200b\u9644\u5f55\u200b3\u200b\u4e2d\u200b\u7684\u200b3.2.10 \u200b\u63d0\u4ea4\u200b\u4ee3\u7801\u200b\u7684\u200b\u4e00\u4e9b\u200b\u7ea6\u5b9a\u200b\u3002</p> </li> <li> <p>\u200b\u8bf7\u200b\u5728\u200b\u63d0\u4ea4\u200b\u4e4b\u524d\u200b\u53c2\u8003\u200b\u4e0b\u65b9\u200b\u7684\u200b\u9644\u5f55\u200b3\uff1aPull Request\u200b\u8bf4\u660e\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5bf9\u200bgit\u200b\u7684\u200b\u63d0\u4ea4\u200b\u6d41\u7a0b\u200b\u4e0d\u200b\u719f\u6089\u200b\uff0c\u200b\u540c\u6837\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b3\u200b\u7684\u200b3.2\u200b\u8282\u200b\u3002</p> </li> </ul>"},{"location":"en/community/community_contribution.html#23","title":"2.3 \u200b\u6587\u6863\u200b\u4f18\u5316","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u65f6\u200b\u9047\u5230\u200b\u4e86\u200b\u6587\u6863\u200b\u8868\u8ff0\u200b\u4e0d\u200b\u6e05\u695a\u200b\u3001\u200b\u63cf\u8ff0\u200b\u7f3a\u5931\u200b\u3001\u200b\u94fe\u63a5\u200b\u5931\u6548\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u60a8\u200b\u7684\u200b\u4fee\u6539\u200b\u3002\u200b\u6587\u6863\u200b\u4e66\u5199\u200b\u89c4\u8303\u200b\u8bf7\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b2\uff1a\u200b\u6587\u6863\u200b\u89c4\u8303\u200b\u3002</p>"},{"location":"en/community/community_contribution.html#3","title":"3. \u200b\u66f4\u200b\u591a\u200b\u8d21\u732e\u200b\u673a\u4f1a","text":"<p>\u200b\u6211\u4eec\u200b\u975e\u5e38\u200b\u9f13\u52b1\u200b\u5f00\u53d1\u8005\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u5b9e\u73b0\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5217\u51fa\u200b\u4e00\u4e9b\u200b\u7ecf\u8fc7\u200b\u5206\u6790\u200b\u540e\u200b\u8ba4\u4e3a\u200b\u6709\u200b\u4ef7\u503c\u200b\u7684\u200b\u62d3\u5c55\u200b\u65b9\u5411\u200b\uff0c\u200b\u6574\u4f53\u200b\u6536\u96c6\u200b\u5728\u200b\u793e\u533a\u200b\u9879\u76ee\u200b\u5e38\u89c4\u8d5b\u200b\u4e2d\u200b\u3002</p>"},{"location":"en/community/community_contribution.html#4","title":"4. \u200b\u8054\u7cfb\u200b\u6211\u4eec","text":"<p>\u200b\u6211\u4eec\u200b\u975e\u5e38\u200b\u6b22\u8fce\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u5728\u200b\u6709\u200b\u610f\u5411\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u4ee3\u7801\u200b\u3001\u200b\u6587\u6863\u200b\u3001\u200b\u8bed\u6599\u200b\u7b49\u200b\u5185\u5bb9\u200b\u524d\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u5927\u5927\u964d\u4f4e\u200bPR\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u6c9f\u901a\u200b\u6210\u672c\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u89c9\u5f97\u200b\u67d0\u4e9b\u200b\u60f3\u6cd5\u200b\u4e2a\u4eba\u200b\u96be\u4ee5\u5b9e\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bSIG\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5b9a\u5411\u200b\u4e3a\u200b\u9879\u76ee\u200b\u62db\u52df\u200b\u5fd7\u540c\u9053\u5408\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u4e00\u8d77\u200b\u5171\u5efa\u200b\u3002\u200b\u901a\u8fc7\u200bSIG\u200b\u6e20\u9053\u200b\u8d21\u732e\u200b\u7684\u200b\u9879\u76ee\u200b\u5c06\u4f1a\u200b\u83b7\u5f97\u200b\u6df1\u5c42\u6b21\u200b\u7684\u200b\u7814\u53d1\u200b\u652f\u6301\u200b\u4e0e\u200b\u8fd0\u8425\u200b\u8d44\u6e90\u200b\uff08\u200b\u5982\u200b\u516c\u4f17\u200b\u53f7\u200b\u5ba3\u4f20\u200b\u3001\u200b\u76f4\u64ad\u200b\u8bfe\u200b\u7b49\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u63a8\u8350\u200b\u7684\u200b\u8d21\u732e\u200b\u6d41\u7a0b\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u901a\u8fc7\u200b\u5728\u200bgithub issue\u200b\u7684\u200b\u9898\u76ee\u200b\u4e2d\u200b\u589e\u52a0\u200b  <code>\u3010third-party\u3011</code> \u200b\u6807\u8bb0\u200b\uff0c\u200b\u8bf4\u660e\u200b\u9047\u5230\u200b\u7684\u200b\u95ee\u9898\u200b\uff08\u200b\u4ee5\u53ca\u200b\u89e3\u51b3\u200b\u7684\u200b\u601d\u8def\u200b\uff09\u200b\u6216\u200b\u60f3\u200b\u62d3\u5c55\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u7b49\u5f85\u200b\u503c\u73ed\u4eba\u5458\u200b\u56de\u590d\u200b\u3002\u200b\u4f8b\u5982\u200b <code>\u3010third-party\u3011\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200bIOS\u200b\u793a\u4f8b\u200b</code></li> <li>\u200b\u4e0e\u200b\u6211\u4eec\u200b\u6c9f\u901a\u200b\u786e\u8ba4\u200b\u6280\u672f\u200b\u65b9\u6848\u200b\u6216\u200bbug\u3001\u200b\u4f18\u5316\u200b\u70b9\u200b\u51c6\u786e\u65e0\u8bef\u200b\u540e\u200b\u8fdb\u884c\u200b\u529f\u80fd\u200b\u65b0\u589e\u200b\u6216\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u6587\u6863\u200b\u9075\u5faa\u200b\u76f8\u5173\u200b\u89c4\u8303\u200b\u3002</li> <li>PR\u200b\u94fe\u63a5\u200b\u5230\u200b\u4e0a\u8ff0\u200bissue\uff0c\u200b\u7b49\u5f85\u200breview\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#5","title":"5. \u200b\u81f4\u8c22\u200b\u4e0e\u200b\u540e\u7eed","text":"<ul> <li>\u200b\u5408\u5165\u200b\u4ee3\u7801\u200b\u4e4b\u540e\u200b\u4f1a\u200b\u5728\u200b\u672c\u200b\u6587\u6863\u200b\u7b2c\u4e00\u8282\u200b\u4e2d\u200b\u66f4\u65b0\u200b\u4fe1\u606f\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u94fe\u63a5\u200b\u4e3a\u200bgithub\u200b\u540d\u5b57\u200b\u53ca\u200b\u4e3b\u9875\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u4e3b\u9875\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002</li> <li>\u200b\u65b0\u589e\u200b\u91cd\u8981\u200b\u529f\u80fd\u200b\u7c7b\u200b\uff0c\u200b\u4f1a\u200b\u5728\u200b\u7528\u6237\u7fa4\u200b\u5e7f\u800c\u544a\u4e4b\u200b\uff0c\u200b\u4eab\u53d7\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u8363\u8a89\u200b\u65f6\u523b\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u6709\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u4f46\u200b\u672a\u200b\u51fa\u73b0\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u5217\u8868\u200b\u4e2d\u200b\uff0c\u200b\u8bf7\u200b\u6309\u7167\u200b <code>4. \u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b</code> \u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\u3002</li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html","title":"DATA ANNOTATION TOOLS","text":"<p>There are the commonly used data annotation tools, which will be continuously updated. Welcome to contribute tools~</p>"},{"location":"en/data_anno_synth/data_annotation.html#1-labelimg","title":"1. labelImg","text":"<ul> <li>Tool description: Rectangular label</li> <li>Tool address:  https://github.com/tzutalin/labelImg</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html#2-rolabelimg","title":"2. roLabelImg","text":"<ul> <li>Tool description: Label tool rewritten based on labelImg, supporting rotating rectangular label</li> <li>Tool address:   https://github.com/cgvict/roLabelImg</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html#3-labelme","title":"3. labelme","text":"<ul> <li>Tool description: Support four points, polygons, circles and other labels</li> <li>Tool address:   https://github.com/wkentaro/labelme</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_synthesis.html","title":"DATA SYNTHESIS TOOLS","text":"<p>In addition to open source data, users can also use synthesis tools to synthesize data. There are the commonly used data synthesis tools, which will be continuously updated. Welcome to contribute tools~</p> <ul> <li>Text_renderer</li> <li>SynthText</li> <li>SynthText_Chinese_version</li> <li>TextRecognitionDataGenerator</li> <li>SynthText3D</li> <li>UnrealText</li> <li>SynthTIGER</li> </ul>"},{"location":"en/data_anno_synth/overview.html","title":"Overview","text":"<ul> <li>Semi-automatic Annotation Tool: PPOCRLabel: https://github.com/PFCCLab/PPOCRLabel/blob/main/README_ch.md</li> <li>Data Synthesis Tool: Style-Text: https://github.com/PFCCLab/StyleText/blob/main/README_ch.md</li> </ul>"},{"location":"en/datasets/datasets.html","title":"General Chinese and English OCR dataset","text":"<p>This is a collection of commonly used Chinese datasets, which is being updated continuously. You are welcome to contribute to this list\uff5e</p> <p>In addition to opensource data, users can also use synthesis tools to synthesize data themselves. Current available synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator, etc.</p>"},{"location":"en/datasets/datasets.html#1-icdar2019-lsvt","title":"1. ICDAR2019-LSVT","text":"<ul> <li>Data sources\uff1ahttps://ai.baidu.com/broad/introduction?dataset=lsvt</li> <li> <p>Introduction\uff1a A total of 45w Chinese street view images, including 5w (2w test + 3w training) fully labeled data (text coordinates + text content), 40w weakly labeled data (text content only), as shown in the following figure:     </p> <p>(a) Fully labeled data</p> <p></p> <p>(b) Weakly labeled data - Download link\uff1ahttps://ai.baidu.com/broad/download?dataset=lsvt</p> </li> </ul>"},{"location":"en/datasets/datasets.html#2-icdar2017-rctw-17","title":"2. ICDAR2017-RCTW-17","text":"<ul> <li>Data sources\uff1ahttps://rctw.vlrlab.net/</li> <li>Introduction\uff1aIt contains 12000 + images, most of them are collected in the wild through mobile camera. Some are screenshots. These images show a variety of scenes, including street views, posters, menus, indoor scenes and screenshots of mobile applications.     </li> <li>Download link\uff1ahttps://rctw.vlrlab.net/dataset/</li> </ul>"},{"location":"en/datasets/datasets.html#3-chinese-street-view-text-recognition","title":"3. Chinese Street View Text Recognition","text":"<ul> <li>Data sources\uff1ahttps://aistudio.baidu.com/aistudio/competition/detail/8</li> <li> <p>Introduction\uff1aA total of 290000 pictures are included, of which 210000 are used as training sets (with labels) and 80000 are used as test sets (without labels). The dataset is collected from the Chinese street view, and is formed by by cutting out the text line area (such as shop signs, landmarks, etc.) in the street view picture. All the images are preprocessed: by using affine transform, the text area is proportionally mapped to a picture with a height of 48 pixels, as shown in the figure:</p> <p> (a) Label: \u200b\u9b45\u6d3e\u200b\u96c6\u6210\u200b\u540a\u9876\u200b  (b) Label: \u200b\u6bcd\u5a74\u200b\u7528\u54c1\u200b\u8fde\u9501\u200b - Download link https://aistudio.baidu.com/aistudio/datasetdetail/8429</p> </li> </ul>"},{"location":"en/datasets/datasets.html#4-chinese-document-text-recognition","title":"4. Chinese Document Text Recognition","text":"<ul> <li>Data sources\uff1ahttps://github.com/YCG09/chinese_ocr</li> <li>Introduction\uff1a</li> <li>A total of 3.64 million pictures are divided into training set and validation set according to 99:1.</li> <li>Using Chinese corpus (news + classical Chinese), the data is randomly generated through changes in font, size, grayscale, blur, perspective, stretching, etc.</li> <li>5990 characters including Chinese characters, English letters, numbers and punctuation\uff08Characters set: https://github.com/YCG09/chinese_ocr/blob/master/train/char_std_5990.txt \uff09</li> <li>Each sample is fixed with 10 characters, and the characters are randomly intercepted from the sentences in the corpus</li> <li>Image resolution is 280x32      </li> <li>Download link\uff1ahttps://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS1Pw (Password: lu7m)</li> </ul>"},{"location":"en/datasets/datasets.html#5icdar2019-art","title":"5\u3001ICDAR2019-ArT","text":"<ul> <li>Data source\uff1ahttps://ai.baidu.com/broad/introduction?dataset=art</li> <li>Introduction\uff1aIt includes 10166 images, 5603 in training sets and 4563 in test sets. It is composed of three parts: total text, scut-ctw1500 and Baidu curved scene text, including text with various shapes such as horizontal, multi-directional and curved.     </li> <li>Download link\uff1ahttps://ai.baidu.com/broad/download?dataset=art</li> </ul>"},{"location":"en/datasets/handwritten_datasets.html","title":"Handwritten OCR dataset","text":"<p>Here we have sorted out the commonly used handwritten OCR dataset datasets, which are being updated continuously. We welcome you to contribute datasets ~</p> <ul> <li>Institute of automation, Chinese Academy of Sciences - handwritten Chinese dataset</li> <li>NIST handwritten single character dataset - English</li> </ul>"},{"location":"en/datasets/handwritten_datasets.html#institute-of-automation-chinese-academy-of-sciences-handwritten-chinese-dataset","title":"Institute of automation, Chinese Academy of Sciences - handwritten Chinese dataset","text":"<ul> <li>Data source: http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html</li> <li>Data introduction:</li> <li> <p>It includes online and offline handwritten data,<code>HWDB1.0~1.2</code> has totally 3895135 handwritten single character samples, which belong to 7356 categories (7185 Chinese characters and 171 English letters, numbers and symbols);<code>HWDB2.0~2.2</code> has totally 5091 pages of images, which are divided into 52230 text lines and 1349414 words. All text and text samples are stored as grayscale images. Some sample words are shown below.</p> <p></p> </li> <li> <p>Download address:http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html</p> </li> <li>\u200b\u4f7f\u7528\u200b\u5efa\u8bae\u200b:Data for single character, white background, can form a large number of text lines for training. White background can be processed into transparent state, which is convenient to add various backgrounds. For the case of semantic needs, it is suggested to extract single character from real corpus to form text lines.</li> </ul>"},{"location":"en/datasets/handwritten_datasets.html#nist-handwritten-single-character-dataset-englishnist-handprinted-forms-and-characters-database","title":"NIST handwritten single character dataset - English(NIST Handprinted Forms and Characters Database)","text":"<ul> <li>Data source: https://www.nist.gov/srd/nist-special-database-19</li> <li> <p>Data introduction: NIST19 dataset is suitable for handwritten document and character recognition model training. It is extracted from the handwritten sample form of 3600 authors and contains 810000 character images in total. Nine of them are shown below.</p> <p></p> </li> <li> <p>Download address: https://www.nist.gov/srd/nist-special-database-19</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html","title":"Key Information Extraction Dataset","text":""},{"location":"en/datasets/kie_datasets.html#key-information-extraction-dataset","title":"Key Information Extraction dataset","text":"<p>Here are the common datasets key information extraction, which are being updated continuously. Welcome to contribute datasets.</p>"},{"location":"en/datasets/kie_datasets.html#1-funsd-dataset","title":"1. FUNSD dataset","text":"<ul> <li>Data source: https://guillaumejaume.github.io/FUNSD/</li> <li> <p>Data Introduction: The FUNSD dataset is a dataset for form comprehension. It contains 199 real, fully annotated scanned images, including market reports, advertisements, and academic reports, etc., and is divided into 149 training set and 50 test set. The FUNSD dataset is suitable for many types of DocVQA tasks, such as field-level entity classification, field-level entity connection, etc. Part of the image and the annotation box visualization are shown below:</p> <p></p> <p></p> <p>In the figure, the orange area represents <code>header</code>, the light blue area represents <code>question</code>, the green area represents <code>answer</code>, and the pink area represents <code>other</code>.</p> </li> <li> <p>Download address: https://guillaumejaume.github.io/FUNSD/download/</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html#2-xfund-dataset","title":"2. XFUND dataset","text":"<ul> <li>Data source: https://github.com/doc-analysis/XFUND</li> <li> <p>Data introduction: XFUND is a multilingual form comprehension dataset, which contains form data in 7 different languages, and all are manually annotated in the form of key-value pairs. The data for each language contains 199 form data, which are divided into 149 training sets and 50 test sets. Part of the image and the annotation box visualization are shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://github.com/doc-analysis/XFUND/releases/tag/v1.0</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html#3-wildreceipt-dataset","title":"3. wildreceipt dataset","text":"<ul> <li>Data source: https://arxiv.org/abs/2103.14470</li> <li> <p>Data introduction: wildreceipt is an English receipt dataset, which contains 26 different categories. There are 1267 training images and 472 evaluation images, in which 50,000 textlines and boxes are annotated. Part of the image and the annotation box visualization are shown below.</p> <p></p> <p></p> </li> </ul> <p>Note\uff1a Boxes with category <code>Ignore</code> or <code>Others</code> are not visualized here.</p> <ul> <li>Download address\uff1a</li> <li>Offical dataset: link</li> <li>Dataset converted for PaddleOCR training process: link</li> </ul>"},{"location":"en/datasets/layout_datasets.html","title":"Layout Analysis Dataset","text":""},{"location":"en/datasets/layout_datasets.html#layout-analysis-dataset","title":"Layout Analysis Dataset","text":"<p>Here are the common datasets of layout anlysis, which are being updated continuously. Welcome to contribute datasets.</p> <p>Most of the layout analysis datasets are object detection datasets. In addition to open source datasets, you can also label or synthesize datasets using tools such as labelme and so on.</p>"},{"location":"en/datasets/layout_datasets.html#1-publaynet-dataset","title":"1. PubLayNet dataset","text":"<ul> <li>Data source: https://github.com/ibm-aur-nlp/PubLayNet</li> <li> <p>Data introduction: The PubLayNet dataset contains 350000 training images and 11000 validation images. There are 5 categories in total, namely: <code>text, title, list, table, figure</code>. Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://developer.ibm.com/exchanges/data/all/publaynet/</p> </li> <li>Note: When using this dataset, you need to follow CDLA-Permissive license.</li> </ul>"},{"location":"en/datasets/layout_datasets.html#2cdla-dataset","title":"2\u3001CDLA dataset","text":"<ul> <li>Data source: https://github.com/buptlihang/CDLA</li> <li> <p>Data introduction: CDLA dataset contains 5000 training images and 1000 validation images with 10 categories, which are <code>Text, Title, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation</code>. Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://github.com/buptlihang/CDLA</p> </li> <li>Note: When you train detection model on CDLA dataset using PaddleDetection, you need to remove the label <code>__ignore__</code> and <code>_background_</code>.</li> </ul>"},{"location":"en/datasets/layout_datasets.html#3tablebank-dataet","title":"3\u3001TableBank dataet","text":"<ul> <li>Data source: https://doc-analysis.github.io/tablebank-page/index.html</li> <li> <p>Data introduction: TableBank dataset contains 2 types of document: Latex (187199 training images, 7265 validation images and 5719 testing images) and Word (73383 training images 2735 validation images and 2281 testing images). Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Data source: https://doc-analysis.github.io/tablebank-page/index.html</p> </li> <li>Note: When using this dataset, you need to follow Apache-2.0 license.</li> </ul>"},{"location":"en/datasets/ocr_datasets.html","title":"OCR datasets","text":"<p>Here is a list of public datasets commonly used in OCR, which are being continuously updated. Welcome to contribute datasets~</p>"},{"location":"en/datasets/ocr_datasets.html#1-text-detection","title":"1. Text detection","text":""},{"location":"en/datasets/ocr_datasets.html#11-paddleocr-text-detection-format-annotation","title":"1.1 PaddleOCR text detection format annotation","text":"<p>The annotation file formats supported by the PaddleOCR text detection algorithm are as follows, separated by \"\\t\":</p> <pre><code>\" Image file name             Image annotation information encoded by json.dumps\"\nch4_test_images/img_61.jpg    [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> <p>The image annotation after json.dumps() encoding is a list containing multiple dictionaries.</p> <p>The <code>points</code> in the dictionary represent the coordinates (x, y) of the four points of the text box, arranged clockwise from the point at the upper left corner.</p> <p><code>transcription</code> represents the text of the current text box. When its content is \"###\" it means that the text box is invalid and will be skipped during training.</p> <p>If you want to train PaddleOCR on other datasets, please build the annotation file according to the above format.</p>"},{"location":"en/datasets/ocr_datasets.html#12-public-dataset","title":"1.2 Public dataset","text":"dataset Image download link PaddleOCR format annotation download link ICDAR 2015 https://rrc.cvc.uab.es/?ch=4&amp;com=downloads train / test ctw1500 https://paddleocr.bj.bcebos.com/dataset/ctw1500.zip Included in the downloaded image zip total text https://paddleocr.bj.bcebos.com/dataset/total_text.tar Included in the downloaded image zip"},{"location":"en/datasets/ocr_datasets.html#121-icdar-2015","title":"1.2.1 ICDAR 2015","text":"<p>The icdar2015 dataset contains train set which has 1000 images obtained with wearable cameras and test set which has 500 images obtained with wearable cameras. The icdar2015 dataset can be downloaded from the link in the table above. Registration is required for downloading.</p> <p>After registering and logging in, download the part marked in the red box in the figure below. And, the content downloaded by <code>Training Set Images</code> should be saved as the folder <code>icdar_c4_train_imgs</code>, and the content downloaded by <code>Test Set Images</code> is saved as the folder <code>ch4_test_images</code></p> <p></p> <p>Decompress the downloaded dataset to the working directory, assuming it is decompressed under PaddleOCR/train_data/. Then download the PaddleOCR format annotation file from the table above.</p> <p>PaddleOCR also provides a data format conversion script, which can convert the official website label to the PaddleOCR format. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># Convert the label file downloaded from the official website to train_icdar2015_label.txt\npython gen_label.py --mode=\"det\" --root_path=\"/path/to/icdar_c4_train_imgs/\"  \\\n                    --input_path=\"/path/to/ch4_training_localization_transcription_gt\" \\\n                    --output_label=\"/path/to/train_icdar2015_label.txt\"\n</code></pre> <p>After decompressing the data set and downloading the annotation file, PaddleOCR/train_data/ has two folders and two files, which are:</p> <pre><code>/PaddleOCR/train_data/icdar2015/text_localization/\n  \u2514\u2500 icdar_c4_train_imgs/         Training data of icdar dataset\n  \u2514\u2500 ch4_test_images/             Testing data of icdar dataset\n  \u2514\u2500 train_icdar2015_label.txt    Training annotation of icdar dataset\n  \u2514\u2500 test_icdar2015_label.txt     Test annotation of icdar dataset\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#2-text-recognition","title":"2. Text recognition","text":""},{"location":"en/datasets/ocr_datasets.html#21-paddleocr-text-recognition-format-annotation","title":"2.1 PaddleOCR text recognition format annotation","text":"<p>The text recognition algorithm in PaddleOCR supports two data formats:</p> <ul> <li><code>lmdb</code> is used to train data sets stored in lmdb format, use lmdb_dataset.py to load;</li> <li><code>common dataset</code> is used to train data sets stored in text files, use simple_dataset.py to load.</li> </ul> <p>If you want to use your own data for training, please refer to the following to organize your data.</p>"},{"location":"en/datasets/ocr_datasets.html#training-set","title":"Training set","text":"<p>It is recommended to put the training images in the same folder, and use a txt file (rec_gt_train.txt) to store the image path and label. The contents of the txt file are as follows:</p> <ul> <li>Note: by default, the image path and image label are split with \\t, if you use other methods to split, it will cause training error</li> </ul> <pre><code>\" Image file name           Image annotation \"\n\ntrain_data/rec/train/word_001.jpg   \u200b\u7b80\u5355\u200b\u53ef\u200b\u4f9d\u8d56\u200b\ntrain_data/rec/train/word_002.jpg   \u200b\u7528\u200b\u79d1\u6280\u200b\u8ba9\u200b\u590d\u6742\u200b\u7684\u200b\u4e16\u754c\u200b\u66f4\u200b\u7b80\u5355\u200b\n...\n</code></pre> <p>The final training set should have the following file structure:</p> <pre><code>|-train_data\n  |-rec\n    |- rec_gt_train.txt\n    |- train\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#test-set","title":"Test set","text":"<p>Similar to the training set, the test set also needs to be provided a folder containing all images (test) and a rec_gt_test.txt. The structure of the test set is as follows:</p> <pre><code>|-train_data\n  |-rec\n    |-ic15_data\n        |- rec_gt_test.txt\n        |- test\n            |- word_001.jpg\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#22-public-dataset","title":"2.2 Public dataset","text":"dataset Image download link PaddleOCR format annotation download link en benchmark(MJ, SJ, IIIT, SVT, IC03, IC13, IC15, SVTP, and CUTE.) DTRB LMDB format, which can be loaded directly with lmdb_dataset.py ICDAR 2015 http://rrc.cvc.uab.es/?ch=4&amp;com=downloads train/ test Multilingual datasets Baidu network disk Extraction code: frgi  google drive Included in the downloaded image zip"},{"location":"en/datasets/ocr_datasets.html#21-icdar-2015","title":"2.1 ICDAR 2015","text":"<p>The ICDAR 2015 dataset can be downloaded from the link in the table above for quick validation. The lmdb format dataset required by en benchmark can also be downloaded from the table above.</p> <p>Then download the PaddleOCR format annotation file from the table above.</p> <p>PaddleOCR also provides a data format conversion script, which can convert the ICDAR official website label to the data format supported by PaddleOCR. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># Convert the label file downloaded from the official website to rec_gt_label.txt\npython gen_label.py --mode=\"rec\" --input_path=\"{path/of/origin/label}\" --output_label=\"rec_gt_label.txt\"\n</code></pre> <p>The data format is as follows, (a) is the original picture, (b) is the Ground Truth text file corresponding to each picture:</p> <p></p>"},{"location":"en/datasets/ocr_datasets.html#3-data-storage-path","title":"3. Data storage path","text":"<p>The default storage path for PaddleOCR training data is <code>PaddleOCR/train_data</code>, if you already have a dataset on your disk, just create a soft link to the dataset directory:</p> <pre><code># linux and mac os\nln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/dataset\n# windows\nmklink /d &lt;path/to/paddle_ocr&gt;/train_data/dataset &lt;path/to/dataset&gt;\n</code></pre>"},{"location":"en/datasets/table_datasets.html","title":"Table Recognition Datasets","text":"<p>Here are the commonly used table recognition datasets, which are being updated continuously. Welcome to contribute datasets~</p>"},{"location":"en/datasets/table_datasets.html#dataset-summary","title":"Dataset Summary","text":"dataset Image download link PPOCR format annotation download link PubTabNet https://github.com/ibm-aur-nlp/PubTabNet jsonl format, which can be loaded directly with pubtab_dataset.py TAL Table Recognition Competition Dataset https://ai.100tal.com/dataset jsonl format, which can be loaded directly with pubtab_dataset.py WTW Chinese scene table dataset https://github.com/wangwen-whu/WTW-Dataset Conversion is required to load with pubtab_dataset.py"},{"location":"en/datasets/table_datasets.html#1-pubtabnet","title":"1. PubTabNet","text":"<ul> <li> <p>Data Introduction\uff1aThe training set of the PubTabNet dataset contains 500,000 images and the validation set contains 9000 images. Part of the image visualization is shown below.</p> <p></p> <p></p> </li> <li> <p>illustrate\uff1aWhen using this dataset, the CDLA-Permissive protocol is required.</p> </li> </ul>"},{"location":"en/datasets/table_datasets.html#2-tal-table-recognition-competition-dataset","title":"2. TAL Table Recognition Competition Dataset","text":"<ul> <li> <p>Data Introduction\uff1aThe training set of the TAL table recognition competition dataset contains 16,000 images. The validation set does not give trainable annotations.</p> <p></p> <p></p> </li> </ul>"},{"location":"en/datasets/table_datasets.html#3-wtw-chinese-scene-table-dataset","title":"3. WTW Chinese scene table dataset","text":"<ul> <li> <p>Data Introduction\uff1aThe WTW Chinese scene table dataset consists of two parts: table detection and table data. The dataset contains images of two scenes, scanned and photographed.</p> <p></p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html","title":"Vertical multi-language OCR dataset","text":"<p>Here we have sorted out the commonly used vertical multi-language OCR dataset datasets, which are being updated continuously. We welcome you to contribute datasets \uff5e</p> <ul> <li>Chinese urban license plate dataset</li> <li>Bank credit card dataset</li> <li>Captcha dataset-Captcha</li> <li>multi-language dataset</li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#chinese-urban-license-plate-dataset","title":"Chinese urban license plate dataset","text":"<ul> <li> <p>Data source\uff1aCCPD</p> </li> <li> <p>Data introduction: It contains more than 250000 vehicle license plate images and vehicle license plate detection and recognition information labeling. It contains the following license plate image information in different scenes.</p> </li> <li> <p>CCPD-Base: General license plate picture</p> </li> <li>CCPD-DB: The brightness of license plate area is bright, dark or uneven</li> <li>CCPD-FN: The license plate is farther or closer to the camera location</li> <li>CCPD-Rotate: License plate includes rotation (horizontal 20~50 degrees, vertical-10~10 degrees)</li> <li>CCPD-Tilt: License plate includes rotation (horizontal 15~45 degrees, vertical 15~45 degrees)</li> <li>CCPD-Blur: The license plate contains blurring due to camera lens jitter</li> <li>CCPD-Weather: The license plate is photographed on rainy, snowy or foggy days</li> <li>CCPD-Challenge: So far, some of the most challenging images in license plate detection and recognition tasks</li> <li> <p>CCPD-NP: Pictures of new cars without license plates.</p> <p></p> </li> <li> <p>Download address</p> </li> <li>Baidu cloud download address (extracted code is hm0U): https://pan.baidu.com/s/1i5AOjAbtkwb17Zy-NQGqkw</li> <li>Google drive download address:https://drive.google.com/file/d/1rdEsCUcIUaYOVRkx5IMTRNA7PcGMmSgc/view</li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#bank-credit-card-dataset","title":"Bank credit card dataset","text":"<ul> <li> <p>Data source: source</p> </li> <li> <p>Data introduction: There are three types of training data</p> </li> <li>1.Sample card data of China Merchants Bank: including card image data and annotation data, a total of 618 pictures</li> <li>2.Single character data: including pictures and annotation data, 37 pictures in total.</li> <li> <p>3.There are only other bank cards, no more detailed information, a total of 50 pictures.</p> </li> <li> <p>The demo image is shown as follows. The annotation information is stored in excel, and the demo image below is marked as</p> <ul> <li>Top 8 card number: 62257583</li> <li>Card type: card of our bank</li> <li>End of validity: 07/41</li> <li>Chinese phonetic alphabet of card users: MICHAEL</li> </ul> <p></p> </li> <li> <p>Download address: cmb2017-2.zip</p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#captcha-dataset-captcha","title":"Captcha dataset-Captcha","text":"<ul> <li>Data source: captcha</li> <li> <p>Data introduction: This is a toolkit for data synthesis. You can output captcha images according to the input text. Use the toolkit to generate several demo images as follows.</p> <p></p> </li> <li> <p>Download address: The dataset is generated and has no download address.</p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#multi-language-datasetmulti-lingual-scene-text-detection-and-recognition","title":"multi-language dataset(Multi-lingual scene text detection and recognition)","text":"<ul> <li>Data source: source</li> <li>Data introduction: Multi language detection dataset MLT contains both language recognition and detection tasks.</li> <li>In the detection task, the training set contains 10000 images in 10 languages, and each language contains 1000 training images. The test set contains 10000 images.</li> <li>In the recognition task, the training set contains 111998 samples.</li> <li>Download address: The training set is large and can be downloaded in two parts. It can only be downloaded after registering on the website: source</li> </ul>"},{"location":"en/model/index.html","title":"\u6982\u89c8","text":""},{"location":"en/model/index.html#pp-ocr","title":"PP-OCR \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u5217\u8868\u200b\uff08\u200b\u66f4\u65b0\u200b\u4e2d\u200b\uff09","text":"\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u63a8\u8350\u200b\u573a\u666f\u200b \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b \u200b\u65b9\u5411\u200b\u5206\u7c7b\u5668\u200b \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv4 \u200b\u6a21\u578b\u200b\uff0815.8M\uff09 ch_PP-OCRv4_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\uff0816.2M\uff09 ch_PP-OCRv3_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\uff0813.4M\uff09 en_PP-OCRv3_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <ul> <li>\u200b\u8d85\u200b\u8f7b\u91cf\u200b OCR \u200b\u7cfb\u5217\u200b\u66f4\u200b\u591a\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5305\u62ec\u200b\u591a\u200b\u8bed\u8a00\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPP-OCR \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u6587\u6863\u200b\u5206\u6790\u200b\u76f8\u5173\u200b\u6a21\u578b\u200b\u53c2\u8003\u200bPP-Structure \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</li> </ul>"},{"location":"en/model/index.html#paddleocr","title":"PaddleOCR \u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u6a21\u578b","text":"\u884c\u4e1a\u200b \u200b\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6587\u6863\u200b\u8bf4\u660e\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u5236\u9020\u200b \u200b\u6570\u7801\u7ba1\u200b\u8bc6\u522b\u200b \u200b\u6570\u7801\u7ba1\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u3001\u200b\u6f0f\u200b\u8bc6\u522b\u200b\u8c03\u4f18\u200b \u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u6570\u7801\u7ba1\u200b\u5b57\u7b26\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b \u200b\u91d1\u878d\u200b \u200b\u901a\u7528\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b \u200b\u591a\u200b\u6a21\u6001\u200b\u901a\u7528\u200b\u8868\u5355\u200b\u7ed3\u6784\u5316\u200b\u63d0\u53d6\u200b \u200b\u591a\u200b\u6a21\u6001\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b \u200b\u4ea4\u901a\u200b \u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b \u200b\u591a\u89d2\u5ea6\u200b\u56fe\u50cf\u5904\u7406\u200b\u3001\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\u3001\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b \u200b\u8f7b\u91cf\u7ea7\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b <ul> <li>\u200b\u66f4\u200b\u591a\u200b\u5236\u9020\u200b\u3001\u200b\u91d1\u878d\u200b\u3001\u200b\u4ea4\u901a\u200b\u884c\u4e1a\u200b\u7684\u200b\u4e3b\u8981\u200b OCR \u200b\u5782\u7c7b\u200b\u5e94\u7528\u200b\u6a21\u578b\u200b\uff08\u200b\u5982\u200b\u7535\u8868\u200b\u3001\u200b\u6db2\u6676\u5c4f\u200b\u3001\u200b\u9ad8\u7cbe\u5ea6\u200b SVTR \u200b\u6a21\u578b\u200b\u7b49\u200b\uff09\uff0c\u200b\u53ef\u200b\u53c2\u8003\u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</li> </ul>"},{"location":"en/model/hardware/install_other_devices.html","title":"\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u98de\u6868","text":"<p>\u200b\u672c\u200b\u6587\u6863\u200b\u4e3b\u8981\u200b\u9488\u5bf9\u200b\u6607\u200b\u817e\u200b NPU \u200b\u786c\u4ef6\u5e73\u53f0\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5b89\u88c5\u200b\u98de\u6868\u200b\u3002</p>"},{"location":"en/model/hardware/install_other_devices.html#1-npu","title":"1. \u200b\u6607\u200b\u817e\u200b NPU \u200b\u98de\u6868\u200b\u5b89\u88c5","text":""},{"location":"en/model/hardware/install_other_devices.html#11","title":"1.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>\u200b\u5f53\u524d\u200b PaddleOCR \u200b\u652f\u6301\u200b\u6607\u200b\u817e\u200b 910B \u200b\u82af\u7247\u200b\uff0c\u200b\u6607\u200b\u817e\u200b\u9a71\u52a8\u200b\u7248\u672c\u200b\u4e3a\u200b 23.0.3\u3002\u200b\u8003\u8651\u200b\u5230\u200b\u73af\u5883\u200b\u5dee\u5f02\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u98de\u6868\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u5b8c\u6210\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u3002</p>"},{"location":"en/model/hardware/install_other_devices.html#_2","title":"\u62c9\u53d6\u200b\u955c\u50cf","text":"<p>\u200b\u6b64\u200b\u955c\u50cf\u200b\u4ec5\u4e3a\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\uff0c\u200b\u955c\u50cf\u200b\u4e2d\u200b\u4e0d\u200b\u5305\u542b\u200b\u9884\u200b\u7f16\u8bd1\u200b\u7684\u200b\u98de\u6868\u200b\u5b89\u88c5\u5305\u200b\uff0c\u200b\u955c\u50cf\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u9ed8\u8ba4\u200b\u5b89\u88c5\u200b\u4e86\u200b\u6607\u200b\u817e\u200b\u7b97\u5b50\u200b\u5e93\u200b CANN-8.0.RC1\u3002</p> <pre><code># \u200b\u9002\u7528\u200b\u4e8e\u200b X86 \u200b\u67b6\u6784\u200b\uff0c\u200b\u6682\u65f6\u200b\u4e0d\u200b\u63d0\u4f9b\u200b Arch64 \u200b\u67b6\u6784\u200b\u955c\u50cf\u200b\ndocker pull registry.baidubce.com/device/paddle-npu:cann80RC1-ubuntu20-x86_64-gcc84-py39\n</code></pre>"},{"location":"en/model/hardware/install_other_devices.html#_3","title":"\u542f\u52a8\u200b\u5bb9\u5668","text":"<p>ASCEND_RT_VISIBLE_DEVICES \u200b\u6307\u5b9a\u200b\u53ef\u89c1\u200b\u7684\u200b NPU \u200b\u5361\u53f7\u200b</p> <pre><code>docker run -it --name paddle-npu-dev -v $(pwd):/work \\\n    --privileged --network=host --shm-size=128G -w=/work \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -e ASCEND_RT_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" \\\n    registry.baidubce.com/device/paddle-npu:cann80RC1-ubuntu20-x86_64-gcc84-py39 /bin/bash\n</code></pre>"},{"location":"en/model/hardware/install_other_devices.html#12-paddle","title":"1.2 \u200b\u5b89\u88c5\u200b paddle \u200b\u5305","text":"<p>\u200b\u5f53\u524d\u200b\u63d0\u4f9b\u200b Python3.9 \u200b\u7684\u200b wheel \u200b\u5b89\u88c5\u5305\u200b\u3002\u200b\u5982\u200b\u6709\u200b\u5176\u4ed6\u200b Python \u200b\u7248\u672c\u200b\u9700\u6c42\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u98de\u6868\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u81ea\u884c\u200b\u7f16\u8bd1\u200b\u5b89\u88c5\u200b\u3002</p>"},{"location":"en/model/hardware/install_other_devices.html#1-python39-wheel","title":"1. \u200b\u4e0b\u8f7d\u5b89\u88c5\u200b Python3.9 \u200b\u7684\u200b wheel \u200b\u5b89\u88c5\u5305","text":"<pre><code># \u200b\u6ce8\u610f\u200b\u9700\u8981\u200b\u5148\u200b\u5b89\u88c5\u200b\u98de\u6868\u200b cpu \u200b\u7248\u672c\u200b\npip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddle-device/npu/paddlepaddle-0.0.0-cp39-cp39-linux_x86_64.whl\npip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddle-device/npu/paddle_custom_npu-0.0.0-cp39-cp39-linux_x86_64.whl\n</code></pre>"},{"location":"en/model/hardware/install_other_devices.html#2","title":"2. \u200b\u9a8c\u8bc1\u200b\u5b89\u88c5\u5305","text":"<p>\u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u4e4b\u540e\u200b\uff0c\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u3002</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>\u200b\u9884\u671f\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b</p> <pre><code>Running verify PaddlePaddle program ...\nPaddlePaddle works well on 1 npu.\nPaddlePaddle works well on 8 npus.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/model/hardware/supported_models.html","title":"PaddleOCR\u200b\u6a21\u578b\u200b\u5217\u8868","text":"<p>\u200b\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u65b9\u5f0f\u200b\u8bf7\u200b\u53c2\u8003\u200b\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u6587\u6863\u200b</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6607\u200b\u817e\u200bNPU PP-OCRv4 \u221a"},{"location":"en/ppocr/environment.html","title":"Environment Preparation","text":"<p>Windows and Mac users are recommended to use Anaconda to build a Python environment, and Linux users are recommended to use docker to build a Python environment.</p> <p>Recommended working environment:</p> <ul> <li>PaddlePaddle &gt;= 2.1.2</li> <li>Python 3.7</li> <li>CUDA 10.1 / CUDA 10.2</li> <li>cuDNN 7.6</li> </ul> <p>If you already have a Python environment installed, you can skip to PaddleOCR Quick Start.</p>"},{"location":"en/ppocr/environment.html#1-python-environment-setup","title":"1. Python Environment Setup","text":""},{"location":"en/ppocr/environment.html#11-windows","title":"1.1 Windows","text":""},{"location":"en/ppocr/environment.html#111-install-anaconda","title":"1.1.1 Install Anaconda","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install python environment first, here we choose python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment.</p> </li> <li> <p>Anaconda download.</p> </li> <li> <p>Address: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> <li> <p>Most Win10 computers are 64-bit operating systems, choose x86_64 version; if the computer is a 32-bit operating system, choose x86.exe</p> <p></p> </li> <li> <p>After the download is complete, double-click the installer to enter the graphical interface</p> </li> <li> <p>The default installation location is C drive, it is recommended to change the installation location to D drive.</p> <p></p> </li> <li> <p>Check Conda to add environment variables and ignore the warning that     </p> </li> </ul>"},{"location":"en/ppocr/environment.html#112-opening-the-terminal-and-creating-the-conda-environment","title":"1.1.2 Opening the terminal and creating the Conda environment","text":"<ul> <li>Open Anaconda Prompt terminal: bottom left Windows Start Menu -&gt; Anaconda3 -&gt; Anaconda Prompt start console</li> </ul> <ul> <li>Create a new Conda environment</li> </ul> <pre><code># Enter the following command at the command line to create an environment named paddle_env\n# Here to speed up the download, use the Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # This is a one line command\n</code></pre> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> <ul> <li>To activate the Conda environment you just created, enter the following command at the command line.</li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n# View the current location of python\nwhere python\n</code></pre> <p></p> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/ppocr/environment.html#12-mac","title":"1.2 Mac","text":""},{"location":"en/ppocr/environment.html#121-installing-anaconda","title":"1.2.1 Installing Anaconda","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install the python environment first, here we choose the python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment</p> </li> <li> <p>Anaconda download:.</p> </li> <li> <p>Address: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> </ul> <p></p> <ul> <li> <p>Select <code>Anaconda3-2021.05-MacOSX-x86_64.pkg</code> at the bottom to download</p> </li> <li> <p>After downloading, double click on the .pkg file to enter the graphical interface</p> </li> <li> <p>Just follow the default settings, it will take a while to install</p> </li> <li> <p>It is recommended to install a code editor such as VSCode or PyCharm</p> </li> </ul>"},{"location":"en/ppocr/environment.html#122-open-a-terminal-and-create-a-conda-environment","title":"1.2.2 Open a terminal and create a Conda environment","text":"<ul> <li> <p>Open the terminal</p> </li> <li> <p>Press command and spacebar at the same time, type \"terminal\" in the focus search, double click to enter terminal</p> </li> <li> <p>Add Conda to the environment variables</p> </li> <li> <p>Environment variables are added so that the system can recognize the Conda command</p> </li> <li> <p>Open <code>~/.bash_profile</code> in the terminal by typing the following command.</p> <pre><code>vim ~/.bash_profile\n</code></pre> </li> <li> <p>Add Conda as an environment variable in <code>~/.bash_profile</code>.</p> <pre><code># Press i first to enter edit mode\n# In the first line type.\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# If you customized the installation location during installation, change ~/opt/anaconda3/bin to the bin folder in the customized installation directory\n\n# The modified ~/.bash_profile file should look like this (where xxx is the username)\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !!! Contents within this block are managed by 'conda init' !!!\n__conda_setup=\"$('/Users/xxx/opt/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n        eval \"$__conda_setup\"\nelse\n        if [ -f \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\" ]; then\n                . \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\"\n        else\n                export PATH=\"/Users/xxx/opt/anaconda3/bin:$PATH\"\n        fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <ul> <li>When you are done, press <code>esc</code> to exit edit mode, then type <code>:wq!</code> and enter to save and exit</li> </ul> </li> <li> <p>Verify that the Conda command is recognized.</p> <ul> <li>Enter <code>source ~/.bash_profile</code> in the terminal to update the environment variables</li> <li>Enter <code>conda info --envs</code> in the terminal again, if it shows that there is a base environment, then Conda has been added to the environment variables</li> </ul> </li> <li> <p>Create a new Conda environment</p> </li> </ul> <pre><code># Enter the following command at the command line to create an environment called paddle_env\n# Here to speed up the download, use Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n</code></pre> <ul> <li> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> </li> <li> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> </li> <li> <p>To activate the Conda environment you just created, enter the following command at the command line.</p> </li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n# View the current location of python\nwhere python\n</code></pre> <p></p> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/ppocr/environment.html#13-linux","title":"1.3 Linux","text":"<p>Linux users can choose to run either Anaconda or Docker. If you are familiar with Docker and need to train the PaddleOCR model, it is recommended to use the Docker environment, where the development process of PaddleOCR is run. If you are not familiar with Docker, you can also use Anaconda to run the project.</p>"},{"location":"en/ppocr/environment.html#131-anaconda-environment-configuration","title":"1.3.1 Anaconda environment configuration","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install the python environment first, here we choose the python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment</p> </li> <li> <p>Download Anaconda.</p> </li> <li> <p>Download at: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> </ul> <p></p> <ul> <li> <p>Select the appropriate version for your operating system</p> <ul> <li>Type <code>uname -m</code> in the terminal to check the command set used by your system</li> </ul> </li> <li> <p>Download method 1: Download locally, then transfer the installation package to the Linux server</p> </li> <li> <p>Download method 2: Directly use Linux command line to download</p> <pre><code># First install wget\nsudo apt-get install wget # Ubuntu\nsudo yum install wget # CentOS\n</code></pre> <pre><code># Then use wget to download from Tsinghua source\n# If you want to download Anaconda3-2021.05-Linux-x86_64.sh, the download command is as follows\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2021.05-Linux-x86_64.sh\n# If you want to download another version, you need to change the file name after the last 1 / to the version you want to download\n</code></pre> </li> <li> <p>To install Anaconda.</p> </li> <li> <p>Type <code>sh Anaconda3-2021.05-Linux-x86_64.sh</code> at the command line</p> <ul> <li>If you downloaded a different version, replace the file name of the command with the name of the file you downloaded</li> </ul> </li> <li> <p>Just follow the installation instructions</p> <ul> <li>You can exit by typing q when viewing the license</li> </ul> </li> <li> <p>Add conda to the environment variables</p> </li> <li> <p>If you have already added conda to the environment variable path during the installation, you can skip this step</p> </li> <li> <p>Open <code>~/.bashrc</code> in a terminal.</p> <pre><code># Enter the following command in the terminal.\nvim ~/.bashrc\n</code></pre> </li> <li> <p>Add conda as an environment variable in <code>~/.bashrc</code>.</p> <pre><code># Press i first to enter edit mode # In the first line enter.\nexport PATH=\"~/anaconda3/bin:$PATH\"\n# If you customized the installation location during installation, change ~/anaconda3/bin to the bin folder in the customized installation directory\n</code></pre> <pre><code># The modified ~/.bash_profile file should look like this (where xxx is the username)\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !!! Contents within this block are managed by 'conda init' !!!\n__conda_setup=\"$('/Users/xxx/opt/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/Users/xxx/opt/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <ul> <li>When you are done, press <code>esc</code> to exit edit mode, then type <code>:wq!</code> and enter to save and exit</li> </ul> </li> <li> <p>Verify that the Conda command is recognized.</p> <ul> <li>Enter <code>source ~/.bash_profile</code> in the terminal to update the environment variables</li> <li>Enter <code>conda info --envs</code> in the terminal again, if it shows that there is a base environment, then Conda has been added to the environment variables</li> </ul> </li> <li> <p>Create a new Conda environment</p> </li> </ul> <pre><code># Enter the following command at the command line to create an environment called paddle_env\n# Here to speed up the download, use Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n</code></pre> <ul> <li> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> </li> <li> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> </li> <li> <p>To activate the Conda environment you just created, enter the following command at the command line.</p> </li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n</code></pre> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/ppocr/environment.html#132-docker-environment-preparation","title":"1.3.2 Docker environment preparation","text":"<p>The first time you use this docker image, it will be downloaded automatically. Please be patient.</p> <pre><code># Switch to the working directory\ncd /home/Projects\n# You need to create a docker container for the first run, and do not need to run the current command when you run it again\n# Create a docker container named ppocr and map the current directory to the /paddle directory of the container\n\n# If using CPU, use docker instead of nvidia-docker to create docker\nsudo docker run --name ppocr -v $PWD:/paddle --network=host -it  registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda10.2-cudnn7  /bin/bash\n\n# If using GPU, use nvidia-docker to create docker\n# docker image registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda11.2-cudnn8 is recommended for CUDA11.2 + CUDNN8.\nsudo nvidia-docker run --name ppocr -v $PWD:/paddle --shm-size=64G --network=host -it registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda10.2-cudnn7 /bin/bash\n</code></pre> <p>You can also visit DockerHub to get the image that fits your machine.</p> <pre><code># ctrl+P+Q to exit docker, to re-enter docker using the following command:\nsudo docker container exec -it ppocr /bin/bash\n</code></pre>"},{"location":"en/ppocr/installation.html","title":"Quick Installation","text":""},{"location":"en/ppocr/installation.html#quick-installation","title":"Quick Installation","text":"<p>After testing, PaddleOCR can run on glibc 2.23. You can also test other glibc versions or install glibc 2.23 for the best compatibility.</p> <p>PaddleOCR working environment:</p> <ul> <li>PaddlePaddle 2.0.0</li> <li>Python 3.7</li> <li>glibc 2.23</li> </ul> <p>It is recommended to use the docker provided by us to run PaddleOCR. Please refer to the docker tutorial link.</p> <p>If you want to directly run the prediction code on Mac or Windows, you can start from step 2.</p>"},{"location":"en/ppocr/installation.html#1-recommended-prepare-a-docker-environment","title":"1. (Recommended) Prepare a docker environment","text":"<p>For the first time you use this docker image, it will be downloaded automatically. Please be patient.</p> <pre><code># Switch to the working directory\ncd /home/Projects\n# You need to create a docker container for the first run, and do not need to run the current command when you run it again\n# Create a docker container named ppocr and map the current directory to the /paddle directory of the container\n\n#If using CPU, use docker instead of nvidia-docker to create docker\nsudo docker run --name ppocr -v $PWD:/paddle --network=host -it  paddlepaddle/paddle:latest-dev-cuda10.1-cudnn7-gcc82  /bin/bash\n</code></pre> <p>With CUDA10, please run the following command to create a container. It is recommended to set a shared memory greater than or equal to 32G through the --shm-size parameter:</p> <pre><code>sudo nvidia-docker run --name ppocr -v $PWD:/paddle --shm-size=64G --network=host -it paddlepaddle/paddle:latest-dev-cuda10.1-cudnn7-gcc82 /bin/bash\n</code></pre> <p>You can also visit DockerHub to get the image that fits your machine.</p> <pre><code># ctrl+P+Q to exit docker, to re-enter docker using the following command:\nsudo docker container exec -it ppocr /bin/bash\n</code></pre>"},{"location":"en/ppocr/installation.html#2-install-paddlepaddle-20","title":"2. Install PaddlePaddle 2.0","text":"<pre><code>pip3 install --upgrade pip\n\n# If you have cuda9 or cuda10 installed on your machine, please run the following command to install\npython3 -m pip install paddlepaddle-gpu==2.0.0 -i https://mirror.baidu.com/pypi/simple\n\n# If you only have cpu on your machine, please run the following command to install\npython3 -m pip install paddlepaddle==2.0.0 -i https://mirror.baidu.com/pypi/simple\n</code></pre> <p>For more software version requirements, please refer to the instructions in Installation Document for operation.</p>"},{"location":"en/ppocr/installation.html#3-clone-paddleocr-repo","title":"3. Clone PaddleOCR repo","text":"<pre><code># Recommend\ngit clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If you cannot pull successfully due to network problems, you can switch to the mirror hosted on Gitee:\n\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: The mirror on Gitee may not keep in synchronization with the latest update with the project on GitHub. There might be a delay of 3-5 days. Please try GitHub at first.\n</code></pre>"},{"location":"en/ppocr/installation.html#4-install-third-party-libraries","title":"4. Install third-party libraries","text":"<pre><code>cd PaddleOCR\npip3 install -r requirements.txt\n</code></pre> <p>If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows.</p> <p>Please try to download Shapely whl file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely.</p> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/ppocr/model_list.html","title":"OCR Model List\uff08V3, updated on 2022.4.28\uff09","text":"<p>Note</p> <ol> <li>Compared with model v2, the 3rd version of the detection model has an improvement in accuracy, and the 2.1 version of the recognition model has optimizations in accuracy and speed with CPU.</li> <li>Compared with models 1.1, which are trained with static graph programming paradigm, models 2.0 or higher are the dynamic graph trained version and achieve close performance.</li> <li>All models in this tutorial are from the PaddleOCR series, for more introduction to algorithms and models based on the public dataset, you can refer to algorithm overview tutorial.</li> </ol> <p>The downloadable models provided by PaddleOCR include the <code>inference model</code>, <code>trained model</code>, <code>pre-trained model</code> and <code>nb model</code>. The differences between the models are as follows:</p> model type model format description inference model inference.pdmodel\u3001inference.pdiparams Used for inference based on Paddle inference engine\uff0cdetail trained model, pre-trained model *.pdparams\u3001*.pdopt\u3001*.states The checkpoints model saved in the training process, which stores the parameters of the model, is mostly used for model evaluation and continuous training. nb model *.nb Model optimized by Paddle-Lite, which is suitable for mobile-side deployment scenarios (Paddle-Lite is needed for nb model deployment). <p>The relationship of the above models is as follows.</p> <p></p>"},{"location":"en/ppocr/model_list.html#1-text-detection-model","title":"1. Text Detection Model","text":""},{"location":"en/ppocr/model_list.html#1-chinese-detection-model","title":"1. Chinese Detection Model","text":"model name description config model size download ch_PP-OCRv3_det_slim [New] slim quantization with distillation lightweight model, supporting Chinese, English, multilingual text detection ch_PP-OCRv3_det_cml.yml 1.1M inference model / trained model / nb model ch_PP-OCRv3_det [New] Original lightweight model, supporting Chinese, English, multilingual text detection ch_PP-OCRv3_det_cml.yml 3.8M inference model / trained model ch_PP-OCRv2_det_slim [New] slim quantization with distillation lightweight model, supporting Chinese, English, multilingual text detection ch_PP-OCRv2_det_cml.yml 3.0M inference model ch_PP-OCRv2_det [New] Original lightweight model, supporting Chinese, English, multilingual text detection ch_PP-OCRv2_det_cml.yml 3.0M inference model / trained model ch_ppocr_mobile_slim_v2.0_det Slim pruned lightweight model, supporting Chinese, English, multilingual text detection ch_det_mv3_db_v2.0.yml 2.6M inference model ch_ppocr_mobile_v2.0_det Original lightweight model, supporting Chinese, English, multilingual text detection ch_det_mv3_db_v2.0.yml 3.0M inference model / trained model ch_ppocr_server_v2.0_det General model, which is larger than the lightweight model, but achieved better performance ch_det_res18_db_v2.0.yml 47.0M inference model / trained model"},{"location":"en/ppocr/model_list.html#12-english-detection-model","title":"1.2 English Detection Model","text":"model name description config model size download en_PP-OCRv3_det_slim [New] Slim quantization with distillation lightweight detection model, supporting English ch_PP-OCRv3_det_cml.yml 1.1M inference model / trained model / nb model en_PP-OCRv3_det [New] Original lightweight detection model, supporting English ch_PP-OCRv3_det_cml.yml 3.8M inference model / trained model <ul> <li>Note: English configuration file is the same as Chinese except for training data, here we only provide one configuration file.</li> </ul>"},{"location":"en/ppocr/model_list.html#13-multilingual-detection-model","title":"1.3 Multilingual Detection Model","text":"model name description config model size download ml_PP-OCRv3_det_slim [New] Slim quantization with distillation lightweight detection model, supporting English ch_PP-OCRv3_det_cml.yml 1.1M inference model / trained model / nb model ml_PP-OCRv3_det [New] Original lightweight detection model, supporting English ch_PP-OCRv3_det_cml.yml 3.8M inference model / trained model <ul> <li>Note: English configuration file is the same as Chinese except for training data, here we only provide one configuration file.</li> </ul>"},{"location":"en/ppocr/model_list.html#2-text-recognition-model","title":"2. Text Recognition Model","text":""},{"location":"en/ppocr/model_list.html#21-chinese-recognition-model","title":"2.1 Chinese Recognition Model","text":"model name description config model size download ch_PP-OCRv3_rec_slim [New] Slim quantization with distillation lightweight model, supporting Chinese, English text recognition ch_PP-OCRv3_rec_distillation.yml 4.9M inference model / trained model / nb model ch_PP-OCRv3_rec [New] Original lightweight model, supporting Chinese, English, multilingual text recognition ch_PP-OCRv3_rec_distillation.yml 12.4M inference model / trained model ch_PP-OCRv2_rec_slim Slim quantization with distillation lightweight model, supporting Chinese, English text recognition ch_PP-OCRv2_rec.yml 9.0M inference model / trained model ch_PP-OCRv2_rec Original lightweight model, supporting Chinese, English, and multilingual text recognition ch_PP-OCRv2_rec_distillation.yml 8.5M inference model / trained model ch_ppocr_mobile_slim_v2.0_rec Slim pruned and quantized lightweight model, supporting Chinese, English and number recognition rec_chinese_lite_train_v2.0.yml 6.0M inference model / trained model ch_ppocr_mobile_v2.0_rec Original lightweight model, supporting Chinese, English and number recognition rec_chinese_lite_train_v2.0.yml 5.2M inference model / trained model / pre-trained model ch_ppocr_server_v2.0_rec General model, supporting Chinese, English and number recognition rec_chinese_common_train_v2.0.yml 94.8M inference model / trained model / pre-trained model <p>Note: The <code>trained model</code> is fine-tuned on the <code>pre-trained model</code> with real data and synthesized vertical text data, which achieved better performance in the real scene. The <code>pre-trained model</code> is directly trained on the full amount of real data and synthesized data, which is more suitable for fine-tuning your dataset.</p>"},{"location":"en/ppocr/model_list.html#22-english-recognition-model","title":"2.2 English Recognition Model","text":"model name description config model size download en_PP-OCRv3_rec_slim [New] Slim quantization with distillation lightweight model, supporting English, English text recognition en_PP-OCRv3_rec.yml 3.2M inference model / trained model / nb model en_PP-OCRv3_rec [New] Original lightweight model, supporting English, English, multilingual text recognition en_PP-OCRv3_rec.yml 9.6M inference model / trained model en_number_mobile_slim_v2.0_rec Slim pruned and quantized lightweight model, supporting English and number recognition rec_en_number_lite_train.yml 2.7M inference model / trained model en_number_mobile_v2.0_rec Original lightweight model, supporting English and number recognition rec_en_number_lite_train.yml 2.6M inference model / trained model <p>Note: Dictionary file of all English recognition models is <code>ppocr/utils/en_dict.txt</code>.</p>"},{"location":"en/ppocr/model_list.html#23-multilingual-recognition-modelupdating","title":"2.3 Multilingual Recognition Model\uff08Updating...\uff09","text":"model name dict file description config model size download korean_PP-OCRv3_rec ppocr/utils/dict/korean_dict.txt Lightweight model for Korean recognition korean_PP-OCRv3_rec.yml 11.0M inference model / trained model japan_PP-OCRv3_rec ppocr/utils/dict/japan_dict.txt Lightweight model for Japanese recognition japan_PP-OCRv3_rec.yml 11.0M inference model / trained model chinese_cht_PP-OCRv3_rec ppocr/utils/dict/chinese_cht_dict.txt Lightweight model for chinese cht chinese_cht_PP-OCRv3_rec.yml 12.0M inference model / trained model te_PP-OCRv3_rec ppocr/utils/dict/te_dict.txt Lightweight model for Telugu recognition te_PP-OCRv3_rec.yml 9.6M inference model / trained model ka_PP-OCRv3_rec ppocr/utils/dict/ka_dict.txt Lightweight model for Kannada recognition ka_PP-OCRv3_rec.yml 9.9M inference model / trained model ta_PP-OCRv3_rec ppocr/utils/dict/ta_dict.txt Lightweight model for Tamil recognition ta_PP-OCRv3_rec.yml 9.6M inference model / trained model latin_PP-OCRv3_rec ppocr/utils/dict/latin_dict.txt Lightweight model for latin recognition latin_PP-OCRv3_rec.yml 9.7M inference model / trained model arabic_PP-OCRv3_rec ppocr/utils/dict/arabic_dict.txt Lightweight model for arabic recognition arabic_PP-OCRv3_rec.yml 9.6M inference model / trained model cyrillic_PP-OCRv3_rec ppocr/utils/dict/cyrillic_dict.txt Lightweight model for cyrillic recognition cyrillic_PP-OCRv3_rec.yml 9.6M inference model / trained model devanagari_PP-OCRv3_rec ppocr/utils/dict/devanagari_dict.txt Lightweight model for devanagari recognition devanagari_PP-OCRv3_rec.yml 9.9M inference model / trained model <p>For a complete list of languages \u200b\u200band tutorials, please refer to Multi-language model</p>"},{"location":"en/ppocr/model_list.html#3-text-angle-classification-model","title":"3. Text Angle Classification Model","text":"model name description config model size download ch_ppocr_mobile_slim_v2.0_cls Slim quantized model for text angle classification cls_mv3.yml 2.1M inference model / trained model / nb model ch_ppocr_mobile_v2.0_cls Original model for text angle classification cls_mv3.yml 1.38M inference model / trained model"},{"location":"en/ppocr/model_list.html#4-paddle-lite-model","title":"4. Paddle-Lite Model","text":"<p>Paddle Lite is an updated version of Paddle-Mobile, an open-open source deep learning framework designed to make it easy to perform inference on mobile, embedded, and IoT devices. It can further optimize the inference model and generate the <code>nb model</code> used for edge devices. It's suggested to optimize the quantization model using Paddle-Lite because the <code>INT8</code> format is used for the model storage and inference.</p> <p>This chapter lists OCR nb models with PP-OCRv2 or earlier versions. You can access the latest nb models from the above tables.</p> Version Introduction Model size Detection model Text Direction model Recognition model Paddle-Lite branch PP-OCRv2 extra-lightweight chinese OCR optimized model 11.0M download link download link download link v2.10 PP-OCRv2(slim) extra-lightweight chinese OCR optimized model 4.6M download link download link download link v2.10 PP-OCRv2 extra-lightweight chinese OCR optimized model 11.0M download link download link download link v2.9 PP-OCRv2(slim) extra-lightweight chinese OCR optimized model 4.9M download link download link download link v2.9 V2.0 ppocr_v2.0 extra-lightweight chinese OCR optimized model 7.8M download link download link download link v2.9 V2.0(slim) ppovr_v2.0 extra-lightweight chinese OCR optimized model 3.3M download link download link download link v2.9"},{"location":"en/ppocr/overview.html","title":"PP-OCR","text":""},{"location":"en/ppocr/overview.html#1-introduction","title":"1. Introduction","text":"<p>PP-OCR is a self-developed practical ultra-lightweight OCR system, which is slimed and optimized based on the reimplemented academic algorithms, considering the balance between accuracy and speed.</p>"},{"location":"en/ppocr/overview.html#pp-ocr_1","title":"PP-OCR","text":"<p>PP-OCR is a two-stage OCR system, in which the text detection algorithm is DB, and the text recognition algorithm is CRNN. Besides, a text direction classifier is added between the detection and recognition modules to deal with text in different directions.</p> <p>PP-OCR pipeline is as follows:</p> <p></p> <p>PP-OCR system is in continuous optimization. At present, PP-OCR and PP-OCRv2 have been released:</p> <p>PP-OCR adopts 19 effective strategies from 8 aspects including backbone network selection and adjustment, prediction head design, data augmentation, learning rate transformation strategy, regularization parameter selection, pre-training model use, and automatic model tailoring and quantization to optimize and slim down the models of each module (as shown in the green box above). The final results are an ultra-lightweight Chinese and English OCR model with an overall size of 3.5M and a 2.8M English digital OCR model. For more details, please refer to PP-OCR technical report.</p>"},{"location":"en/ppocr/overview.html#pp-ocrv2","title":"PP-OCRv2","text":"<p>On the basis of PP-OCR, PP-OCRv2 is further optimized in five aspects. The detection model adopts CML(Collaborative Mutual Learning) knowledge distillation strategy and CopyPaste data expansion strategy. The recognition model adopts LCNet lightweight backbone network, U-DML knowledge distillation strategy and enhanced CTC loss function improvement (as shown in the red box above), which further improves the inference speed and prediction effect. For more details, please refer to PP-OCRv2 technical report.</p>"},{"location":"en/ppocr/overview.html#pp-ocrv3","title":"PP-OCRv3","text":"<p>PP-OCRv3 upgraded the detection model and recognition model in 9 aspects based on PP-OCRv2:</p> <ul> <li>PP-OCRv3 detector upgrades the CML(Collaborative Mutual Learning) text detection strategy proposed in PP-OCRv2, and further optimizes the effect of teacher model and student model respectively. In the optimization of teacher model, a pan module with large receptive field named LK-PAN is proposed and the DML distillation strategy is adopted; In the optimization of student model, a FPN module with residual attention mechanism named RSE-FPN is proposed.</li> <li>PP-OCRv3 recognizer is optimized based on text recognition algorithm SVTR. SVTR no longer adopts RNN by introducing transformers structure, which can mine the context information of text line image more effectively, so as to improve the ability of text recognition. PP-OCRv3 adopts lightweight text recognition network SVTR_LCNet, guided training of CTC by attention, data augmentation strategy TextConAug, better pre-trained model by self-supervised TextRotNet, UDML(Unified Deep Mutual Learning), and UIM (Unlabeled Images Mining) to accelerate the model and improve the effect.</li> </ul> <p>PP-OCRv3 pipeline is as follows:</p> <p></p> <p>For more details, please refer to PP-OCRv3 technical report.</p>"},{"location":"en/ppocr/overview.html#2-features","title":"2. Features","text":"<ul> <li>Ultra lightweight PP-OCRv3 series models: detection (3.6M) + direction classifier (1.4M) + recognition 12M) = 17.0M</li> <li>Ultra lightweight PP-OCRv2 series models: detection (3.1M) + direction classifier (1.4M) + recognition 8.5M) = 13.0M</li> <li>Ultra lightweight PP-OCR mobile series models: detection (3.0M) + direction classifier (1.4M) + recognition (5.0M) = 9.4M</li> <li>General PP-OCR server series models: detection (47.1M) + direction classifier (1.4M) + recognition (94.9M) = 143.4M</li> <li>Support Chinese, English, and digit recognition, vertical text recognition, and long text recognition</li> <li>Support multi-lingual recognition: about 80 languages like Korean, Japanese, German, French, etc</li> </ul>"},{"location":"en/ppocr/overview.html#3-benchmark","title":"3. benchmark","text":"<p>For the performance comparison between PP-OCR series models, please check the benchmark documentation.</p>"},{"location":"en/ppocr/overview.html#4-visualization-more","title":"4. Visualization more","text":""},{"location":"en/ppocr/overview.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/ppocr/overview.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/ppocr/overview.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/ppocr/overview.html#5-tutorial","title":"5. Tutorial","text":""},{"location":"en/ppocr/overview.html#51-quick-start","title":"5.1 Quick start","text":"<ul> <li>You can also quickly experience the ultra-lightweight OCR : Online Experience</li> <li>Mobile DEMO experience (based on EasyEdge and Paddle-Lite, supports iOS and Android systems): Sign in to the website to obtain the QR code for  installing the App</li> <li>One line of code quick use: Quick Start</li> </ul>"},{"location":"en/ppocr/overview.html#52-model-training-compression-deployment","title":"5.2 Model training / compression / deployment","text":"<p>For more tutorials, including model training, model compression, deployment, etc., please refer to tutorials\u3002</p>"},{"location":"en/ppocr/overview.html#6-model-zoo","title":"6. Model zoo","text":""},{"location":"en/ppocr/overview.html#pp-ocr-series-model-listupdate-on-20220428","title":"PP-OCR Series Model List\uff08Update on 2022.04.28\uff09","text":"Model introduction Model name Recommended scene Detection model Direction classifier Recognition model Chinese and English ultra-lightweight PP-OCRv3 model\uff0816.2M\uff09 ch_PP-OCRv3_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model English ultra-lightweight PP-OCRv3 model\uff0813.4M\uff09 en_PP-OCRv3_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model Chinese and English ultra-lightweight PP-OCRv2 model\uff0811.6M\uff09 ch_PP-OCRv2_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model Chinese and English ultra-lightweight PP-OCR model (9.4M) ch_ppocr_mobile_v2.0_xx Mobile &amp; server inference model / trained model inference model / trained model inference model / trained model Chinese and English general PP-OCR model (143.4M) ch_ppocr_server_v2.0_xx Server inference model / trained model inference model / trained model inference model / trained model <p>For more model downloads (including multiple languages), please refer to PP-OCR series model downloads.</p>"},{"location":"en/ppocr/quick_start.html","title":"PaddleOCR Quick Start","text":"<p>Note: This tutorial mainly introduces the usage of PP-OCR series models, please refer to PP-Structure Quick Start for the quick use of document analysis related functions.</p>"},{"location":"en/ppocr/quick_start.html#1-installation","title":"1. Installation","text":""},{"location":"en/ppocr/quick_start.html#11-install-paddlepaddle","title":"1.1 Install PaddlePaddle","text":"<p>If you do not have a Python environment, please refer to Environment Preparation.</p> <ul> <li>If you have CUDA 11 installed on your machine, please run the following command to install</li> </ul> <pre><code>pip install paddlepaddle-gpu\n</code></pre> <ul> <li>If you have no available GPU on your machine, please run the following command to install the CPU version</li> </ul> <pre><code>python -m pip install paddlepaddle\n</code></pre> <p>For more software version requirements, please refer to the instructions in Installation Document for operation.</p>"},{"location":"en/ppocr/quick_start.html#12-install-paddleocr-whl-package","title":"1.2 Install PaddleOCR Whl Package","text":"<pre><code>pip install \"paddleocr&gt;=2.0.1\" # Recommend to use version 2.0.1+\n</code></pre> <ul> <li>For windows users: If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows. Please try to download Shapely whl file here.</li> </ul> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/ppocr/quick_start.html#2-easy-to-use","title":"2. Easy-to-Use","text":""},{"location":"en/ppocr/quick_start.html#21-use-by-command-line","title":"2.1 Use by Command Line","text":"<p>PaddleOCR provides a series of test images, click here to download, and then switch to the corresponding directory in the terminal</p> <pre><code>cd /path/to/ppocr_img\n</code></pre> <p>If you do not use the provided test image, you can replace the following <code>--image_dir</code> parameter with the corresponding test image path</p>"},{"location":"en/ppocr/quick_start.html#211-chinese-and-english-model","title":"2.1.1 Chinese and English Model","text":"<ul> <li>Detection, direction classification and recognition: set the parameter<code>--use_gpu false</code> to disable the gpu device</li> </ul> <pre><code>paddleocr --image_dir ./imgs_en/img_12.jpg --use_angle_cls true --lang en --use_gpu false\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <p>pdf file is also supported, you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>Only detection: set <code>--rec</code> to <code>false</code></li> </ul> <pre><code>paddleocr --image_dir ./imgs_en/img_12.jpg --rec false\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[397.0, 802.0], [1092.0, 802.0], [1092.0, 841.0], [397.0, 841.0]]\n[[397.0, 750.0], [1211.0, 750.0], [1211.0, 789.0], [397.0, 789.0]]\n[[397.0, 702.0], [1209.0, 698.0], [1209.0, 734.0], [397.0, 738.0]]\n......\n</code></pre> <ul> <li>Only recognition: set <code>--det</code> to <code>false</code></li> </ul> <pre><code>paddleocr --image_dir ./imgs_words_en/word_10.png --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <p>Version paddleocr uses the PP-OCRv4 model by default(<code>--ocr_version PP-OCRv4</code>). If you want to use other versions, you can set the parameter <code>--ocr_version</code>, the specific version description is as follows:</p> version name description PP-OCRv4 support Chinese and English detection and recognition, direction classifier, support multilingual recognition PP-OCRv3 support Chinese and English detection and recognition, direction classifier, support multilingual recognition PP-OCRv2 only supports Chinese and English detection and recognition, direction classifier, multilingual model is not updated PP-OCR support Chinese and English detection and recognition, direction classifier, support multilingual recognition <p>If you want to add your own trained model, you can add model links and keys in paddleocr and recompile.</p> <p>More whl package usage can be found in whl package</p>"},{"location":"en/ppocr/quick_start.html#212-multi-language-model","title":"2.1.2 Multi-language Model","text":"<p>PaddleOCR currently supports 80 languages, which can be switched by modifying the <code>--lang</code> parameter.</p> <pre><code>paddleocr --image_dir ./doc/imgs_en/254.jpg --lang=en\n</code></pre> <p></p> <p></p> <p>The result is a list, each item contains a text box, text and recognition confidence</p> <pre><code>[[[67.0, 51.0], [327.0, 46.0], [327.0, 74.0], [68.0, 80.0]], ('PHOCAPITAL', 0.9944712519645691)]\n[[[72.0, 92.0], [453.0, 84.0], [454.0, 114.0], [73.0, 122.0]], ('107 State Street', 0.9744491577148438)]\n[[[69.0, 135.0], [501.0, 125.0], [501.0, 156.0], [70.0, 165.0]], ('Montpelier Vermont', 0.9357033967971802)]\n......\n</code></pre> <p>Commonly used multilingual abbreviations include</p> Language Abbreviation Language Abbreviation Language Abbreviation Chinese &amp; English ch French fr Japanese japan English en German german Korean korean Chinese Traditional chinese_cht Italian it Russian ru <p>A list of all languages and their corresponding abbreviations can be found in Multi-Language Model Tutorial</p>"},{"location":"en/ppocr/quick_start.html#22-use-by-code","title":"2.2 Use by Code","text":""},{"location":"en/ppocr/quick_start.html#221-chinese-english-model-and-multilingual-model","title":"2.2.1 Chinese &amp; English Model and Multilingual Model","text":"<ul> <li>detection, angle classification and recognition:</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = './imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n  [[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n  [[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n  ......\n</code></pre> <p>Visualization of results</p> <p></p> <p>If the input is a PDF file, you can refer to the following code for visualization</p> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nPAGE_NUM = 10 # Set the recognition page number\npdf_path = 'default.pdf'\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", page_num=PAGE_NUM)  # need to run only once to download and load model into memory\n# ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", page_num=PAGE_NUM,use_gpu=0) # To Use GPU,uncomment this line and comment the above one.\nresult = ocr.ocr(pdf_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    if res == None: # Skip when empty result detected to avoid TypeError:NoneType\n        print(f\"[DEBUG] Empty page {idx+1} detected, skip it.\")\n        continue\n    for line in res:\n        print(line)\n\n# draw the result\nimport fitz\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimgs = []\nwith fitz.open(pdf_path) as pdf:\n    for pg in range(0, PAGE_NUM):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.get_pixmap(matrix=mat, alpha=False)\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\nfor idx in range(len(result)):\n    res = result[idx]\n    if res == None:\n        continue\n    image = imgs[idx]\n    boxes = [line[0] for line in res]\n    txts = [line[1][0] for line in res]\n    scores = [line[1][1] for line in res]\n    im_show = draw_ocr(image, boxes, txts, scores, font_path='doc/fonts/simfang.ttf')\n    im_show = Image.fromarray(im_show)\n    im_show.save('result_page_{}.jpg'.format(idx))\n</code></pre> <ul> <li>Detection and Recognition Using Sliding Windows</li> </ul> <p>To perform OCR using sliding windows, the following code snippet can be employed:</p> <pre><code>from paddleocr import PaddleOCR\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Initialize OCR engine\nocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n\nimg_path = \"./very_large_image.jpg\"\nslice = {'horizontal_stride': 300, 'vertical_stride': 500, 'merge_x_thres': 50, 'merge_y_thres': 35}\nresults = ocr.ocr(img_path, cls=True, slice=slice)\n\n# Load image\nimage = Image.open(img_path).convert(\"RGB\")\ndraw = ImageDraw.Draw(image)\nfont = ImageFont.truetype(\"./doc/fonts/simfang.ttf\", size=20)  # Adjust size as needed\n\n# Process and draw results\nfor res in results:\n    for line in res:\n        box = [tuple(point) for point in line[0]]\n        # Finding the bounding box\n        box = [(min(point[0] for point in box), min(point[1] for point in box)),\n               (max(point[0] for point in box), max(point[1] for point in box))]\n        txt = line[1][0]\n        draw.rectangle(box, outline=\"red\", width=2)  # Draw rectangle\n        draw.text((box[0][0], box[0][1] - 25), txt, fill=\"blue\", font=font)  # Draw text above the box\n\n# Save result\nimage.save(\"result.jpg\")\n</code></pre> <p>This example initializes the PaddleOCR instance with angle classification enabled and sets the language to English. The <code>ocr</code> method is then called with several parameters to customize the detection and recognition process, including the <code>slice</code> parameter for handling image slices.</p> <p>For a more comprehensive understanding of the slicing operation, please refer to the slice operation documentation.</p>"},{"location":"en/ppocr/quick_start.html#3-summary","title":"3. Summary","text":"<p>In this section, you have mastered the use of PaddleOCR whl package.</p> <p>PaddleX provides a high-quality ecological model of the paddle. It is a one-stop full-process high-efficiency development platform for training, pressing and pushing. Its mission is to help AI technology to be implemented quickly. The vision is to make everyone an AI Developer! Currently PP-OCRv4 has been launched on PaddleX, you can enter General OCR to experience the whole process of model training, compression and inference deployment.</p>"},{"location":"en/ppocr/visualization.html","title":"Visualization","text":""},{"location":"en/ppocr/visualization.html#pp-ocrv3","title":"PP-OCRv3","text":""},{"location":"en/ppocr/visualization.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/ppocr/visualization.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/ppocr/visualization.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/ppocr/visualization.html#pp-ocrv2","title":"PP-OCRv2","text":""},{"location":"en/ppocr/visualization.html#ch_ppocr_server_20","title":"ch_ppocr_server_2.0","text":""},{"location":"en/ppocr/visualization.html#en_ppocr_mobile_20","title":"en_ppocr_mobile_2.0","text":""},{"location":"en/ppocr/visualization.html#multilingual_ppocr_mobile_20","title":"(multilingual)_ppocr_mobile_2.0","text":""},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html","title":"PP-OCRv3","text":""},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>PP-OCRv3\u200b\u5728\u200bPP-OCRv2\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv2\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\u4ecd\u200b\u57fa\u4e8e\u200bDB\u200b\u7b97\u6cd5\u200b\u4f18\u5316\u200b\uff0c\u200b\u800c\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bCRNN\uff0c\u200b\u6362\u6210\u200b\u4e86\u200bIJCAI 2022\u200b\u6700\u65b0\u200b\u6536\u5f55\u200b\u7684\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u4ea7\u4e1a\u200b\u9002\u914d\u200b\u3002PP-OCRv3\u200b\u7cfb\u7edf\u200b\u6846\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff08\u200b\u7c89\u8272\u200b\u6846\u4e2d\u200b\u4e3a\u200bPP-OCRv3\u200b\u65b0\u589e\u200b\u7b56\u7565\u200b\uff09\uff1a</p> <p></p> <p>\u200b\u4ece\u200b\u7b97\u6cd5\u200b\u6539\u8fdb\u200b\u601d\u8def\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5171\u200b9\u200b\u4e2a\u200b\u65b9\u9762\u200b\u7684\u200b\u6539\u8fdb\u200b\uff1a</p> <ul> <li> <p>\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>LK-PAN\uff1a\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200b\uff1b</li> <li>DML\uff1a\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\uff1b</li> <li>RSE-FPN\uff1a\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200b\uff1b</li> </ul> </li> <li> <p>\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b\uff1b</li> <li>GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1b</li> <li>UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\uff1b</li> <li>UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002</li> </ul> </li> </ul> <p>\u200b\u4ece\u200b\u6548\u679c\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u591a\u79cd\u200b\u573a\u666f\u200b\u7cbe\u5ea6\u200b\u5747\u200b\u6709\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff1a</p> <ul> <li>\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200bPP-OCRv2\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b\u8d85\u200b5%\uff1b</li> <li>\u200b\u82f1\u6587\u200b\u6570\u5b57\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv2\u200b\u82f1\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b11%\uff1b</li> <li>\u200b\u591a\u200b\u8bed\u8a00\u200b\u573a\u666f\u200b\uff0c\u200b\u4f18\u5316\u200b80+\u200b\u8bed\u79cd\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\uff0c\u200b\u5e73\u5747\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b\u8d85\u200b5%\u3002</li> </ul>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#2","title":"2. \u200b\u68c0\u6d4b\u200b\u4f18\u5316","text":"<p>PP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u662f\u200b\u5bf9\u200bPP-OCRv2\u200b\u4e2d\u200b\u7684\u200bCML\uff08Collaborative Mutual Learning) \u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cCML\u200b\u7684\u200b\u6838\u5fc3\u601d\u60f3\u200b\u7ed3\u5408\u200b\u4e86\u200b\u2460\u200b\u4f20\u7edf\u200b\u7684\u200bTeacher\u200b\u6307\u5bfc\u200bStudent\u200b\u7684\u200b\u6807\u51c6\u200b\u84b8\u998f\u200b\u4e0e\u200b \u2461Students\u200b\u7f51\u7edc\u200b\u4e4b\u95f4\u200b\u7684\u200bDML\u200b\u4e92\u200b\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200bStudents\u200b\u7f51\u7edc\u200b\u4e92\u200b\u5b66\u4e60\u200b\u7684\u200b\u540c\u65f6\u200b\uff0cTeacher\u200b\u7f51\u7edc\u200b\u4e88\u4ee5\u200b\u6307\u5bfc\u200b\u3002PP-OCRv3\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u6548\u679c\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u5728\u200b\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200bLK-PAN\u200b\u548c\u200b\u5f15\u5165\u200b\u4e86\u200bDML\uff08Deep Mutual Learning\uff09\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff1b\u200b\u5728\u200b\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200bRSE-FPN\u3002</p> <p></p> <p>\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b hmean \u200b\u901f\u5ea6\u200b\uff08cpu + mkldnn) baseline teacher PP-OCR server 49.0M 83.20% 171ms teacher1 DB-R50-LK-PAN 124.0M 85.00% 396ms teacher2 DB-R50-LK-PAN-DML 124.0M 86.00% 396ms baseline student PP-OCRv2 3.0M 83.20% 117ms student0 DB-MV3-RSE-FPN 3.6M 84.50% 124ms student1 DB-MV3-CML\uff08teacher2\uff09 3.0M 84.30% 117ms student2 DB-MV3-RSE-FPN-CML\uff08teacher2\uff09 3.60M 85.40% 124ms <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#1lk-panpan","title":"\uff081\uff09LK-PAN\uff1a\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784","text":"<p>LK-PAN (Large Kernel PAN) \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u66f4\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200bPAN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u6838\u5fc3\u200b\u662f\u200b\u5c06\u200bPAN\u200b\u7ed3\u6784\u200b\u7684\u200bpath augmentation\u200b\u4e2d\u200b\u5377\u79ef\u200b\u6838\u4ece\u200b<code>3*3</code>\u200b\u6539\u4e3a\u200b<code>9*9</code>\u3002\u200b\u901a\u8fc7\u200b\u589e\u5927\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u63d0\u5347\u200b\u7279\u5f81\u200b\u56fe\u200b\u6bcf\u4e2a\u200b\u4f4d\u7f6e\u200b\u8986\u76d6\u200b\u7684\u200b\u611f\u53d7\u200b\u91ce\u200b\uff0c\u200b\u66f4\u200b\u5bb9\u6613\u200b\u68c0\u6d4b\u200b\u5927\u5b57\u4f53\u200b\u7684\u200b\u6587\u5b57\u200b\u4ee5\u53ca\u200b\u6781\u7aef\u200b\u957f\u5bbd\u200b\u6bd4\u200b\u7684\u200b\u6587\u5b57\u200b\u3002\u200b\u4f7f\u7528\u200bLK-PAN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u4ece\u200b83.2%\u200b\u63d0\u5347\u200b\u5230\u200b85.0%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#2dml","title":"\uff082\uff09DML\uff1a\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>DML \uff08Deep Mutual Learning\uff09\u200b\u4e92\u200b\u5b66\u4e60\u200b\u84b8\u998f\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4e24\u4e2a\u200b\u7ed3\u6784\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u4e92\u76f8\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6709\u6548\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u91c7\u7528\u200bDML\u200b\u7b56\u7565\u200b\uff0chmean\u200b\u4ece\u200b85%\u200b\u63d0\u5347\u200b\u5230\u200b86%\u3002\u200b\u5c06\u200bPP-OCRv2\u200b\u4e2d\u200bCML\u200b\u7684\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u66f4\u65b0\u200b\u4e3a\u200b\u4e0a\u8ff0\u200b\u66f4\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u4ece\u200b83.2%\u200b\u63d0\u5347\u200b\u5230\u200b84.3%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#3rse-fpnfpn","title":"\uff083\uff09RSE-FPN\uff1a\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784","text":"<p>RSE-FPN\uff08Residual Squeeze-and-Excitation FPN\uff09\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5f15\u5165\u200b\u6b8b\u5dee\u200b\u7ed3\u6784\u200b\u548c\u200b\u901a\u9053\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5c06\u200bFPN\u200b\u4e2d\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u66f4\u6362\u200b\u4e3a\u200b\u901a\u9053\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u6784\u200b\u7684\u200bRSEConv\u200b\u5c42\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u8868\u5f81\u200b\u80fd\u529b\u200b\u3002\u200b\u8003\u8651\u200b\u5230\u200bPP-OCRv2\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4e2d\u200bFPN\u200b\u901a\u9053\u200b\u6570\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c\u200b\u4ec5\u4e3a\u200b96\uff0c\u200b\u5982\u679c\u200b\u76f4\u63a5\u200b\u7528\u200bSEblock\u200b\u4ee3\u66ff\u200bFPN\u200b\u4e2d\u200b\u5377\u79ef\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u67d0\u4e9b\u200b\u901a\u9053\u200b\u7684\u200b\u7279\u5f81\u200b\u88ab\u200b\u6291\u5236\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4f1a\u200b\u4e0b\u964d\u200b\u3002RSEConv\u200b\u5f15\u5165\u200b\u6b8b\u5dee\u200b\u7ed3\u6784\u200b\u4f1a\u200b\u7f13\u89e3\u200b\u4e0a\u8ff0\u200b\u95ee\u9898\u200b\uff0c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200bPP-OCRv2\u200b\u4e2d\u200bCML\u200b\u7684\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200b\u66f4\u65b0\u200b\u4e3a\u200bRSE-FPN\uff0c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u4ece\u200b84.3%\u200b\u63d0\u5347\u200b\u5230\u200b85.4%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#3","title":"3. \u200b\u8bc6\u522b\u200b\u4f18\u5316","text":"<p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u76f4\u63a5\u200b\u5c06\u200bPP-OCRv2\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u66ff\u6362\u6210\u200bSVTR_Tiny\uff0c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u4ece\u200b74.8%\u200b\u63d0\u5347\u200b\u5230\u200b80.1%\uff08+5.3%\uff09\uff0c\u200b\u4f46\u662f\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u6162\u200b\u4e86\u200b\u5c06\u8fd1\u200b11\u200b\u500d\u200b\uff0cCPU\u200b\u4e0a\u200b\u9884\u6d4b\u200b\u4e00\u6761\u200b\u6587\u672c\u200b\u884c\u200b\uff0c\u200b\u5c06\u8fd1\u200b100ms\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPP-OCRv3\u200b\u91c7\u7528\u200b\u5982\u4e0b\u200b6\u200b\u4e2a\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u52a0\u901f\u200b\u3002</p> <p></p> <p>\u200b\u57fa\u4e8e\u200b\u4e0a\u8ff0\u200b\u7b56\u7565\u200b\uff0cPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200bPP-OCRv2\uff0c\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b4.6%\u3002 \u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\uff08CPU + MKLDNN) 01 PP-OCRv2 8.0M 74.80% 8.54ms 02 SVTR_Tiny 21.0M 80.10% 97.00ms 03 SVTR_LCNet(h32) 12.0M 71.90% 6.60ms 04 SVTR_LCNet(h48) 12.0M 73.98% 7.60ms 05 + GTC 12.0M 75.80% 7.60ms 06 + TextConAug 12.0M 76.30% 7.60ms 07 + TextRotNet 12.0M 76.90% 7.60ms 08 + UDML 12.0M 78.40% 7.60ms 09 + UIM 12.0M 79.40% 7.60ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c\u200b\u5b9e\u9a8c\u200b01-03\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,32,320)\uff0c04-08\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,48,320)\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u56fe\u50cf\u200b\u4e3a\u200b\u53d8\u957f\u200b\u8f93\u5165\u200b\uff0c\u200b\u901f\u5ea6\u200b\u4f1a\u200b\u6709\u6240\u200b\u53d8\u5316\u200b\u3002\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#1svtr_lcnet","title":"\uff081\uff09SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc","text":"<p>SVTR_LCNet\u200b\u662f\u200b\u9488\u5bf9\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5c06\u200b\u57fa\u4e8e\u200bTransformer\u200b\u7684\u200bSVTR\u200b\u7f51\u7edc\u200b\u548c\u200b\u8f7b\u91cf\u7ea7\u200bCNN\u200b\u7f51\u7edc\u200bPP-LCNet \u200b\u878d\u5408\u200b\u7684\u200b\u4e00\u79cd\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7f51\u7edc\u200b\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u4f18\u4e8e\u200bPP-OCRv2\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b20%\uff0c\u200b\u4f46\u662f\u200b\u7531\u4e8e\u200b\u6ca1\u6709\u200b\u91c7\u7528\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff0c\u200b\u8be5\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u7565\u5dee\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u7247\u200b\u89c4\u8303\u5316\u200b\u9ad8\u5ea6\u200b\u4ece\u200b32\u200b\u63d0\u5347\u200b\u5230\u200b48\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u7a0d\u5fae\u200b\u53d8\u6162\u200b\uff0c\u200b\u4f46\u662f\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff0c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u8fbe\u5230\u200b73.98%\uff08+2.08%\uff09\uff0c\u200b\u63a5\u8fd1\u200bPP-OCRv2\u200b\u91c7\u7528\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u3002</p> <p>SVTR_Tiny \u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u7531\u4e8e\u200b MKLDNN \u200b\u52a0\u901f\u200b\u5e93\u200b\u652f\u6301\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u9650\u200b\uff0cSVTR \u200b\u5728\u200b CPU+MKLDNN \u200b\u4e0a\u200b\u76f8\u6bd4\u200b PP-OCRv2 \u200b\u6162\u200b\u4e86\u200b10\u200b\u500d\u200b\u3002PP-OCRv3 \u200b\u671f\u671b\u200b\u5728\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e0d\u200b\u5e26\u6765\u200b\u989d\u5916\u200b\u7684\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\u3002\u200b\u901a\u8fc7\u200b\u5206\u6790\u200b\u53d1\u73b0\u200b\uff0cSVTR_Tiny \u200b\u7ed3\u6784\u200b\u7684\u200b\u4e3b\u8981\u200b\u8017\u65f6\u200b\u6a21\u5757\u200b\u4e3a\u200b Mixing Block\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5bf9\u200b SVTR_Tiny \u200b\u7684\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u4f18\u5316\u200b\uff08\u200b\u8be6\u7ec6\u200b\u901f\u5ea6\u200b\u6570\u636e\u200b\u8bf7\u200b\u53c2\u8003\u200b\u4e0b\u65b9\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u8868\u683c\u200b\uff09:</p> <ol> <li> <p>\u200b\u5c06\u200b SVTR \u200b\u7f51\u7edc\u200b\u524d\u534a\u90e8\u200b\u5206\u200b\u66ff\u6362\u200b\u4e3a\u200b PP-LCNet \u200b\u7684\u200b\u524d\u200b\u4e09\u4e2a\u200bstage\uff0c\u200b\u4fdd\u7559\u200b4\u200b\u4e2a\u200b Global Mixing Block \uff0c\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b76%\uff0c\u200b\u52a0\u901f\u200b69%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> <li> <p>\u200b\u5c06\u200b4\u200b\u4e2a\u200b Global Mixing Block \u200b\u51cf\u5c0f\u200b\u5230\u200b2\u200b\u4e2a\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b72.9%\uff0c\u200b\u52a0\u901f\u200b69%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> <li> <p>\u200b\u5b9e\u9a8c\u200b\u53d1\u73b0\u200b Global Mixing Block \u200b\u7684\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u4e0e\u200b\u8f93\u5165\u200b\u5176\u200b\u7279\u5f81\u200b\u7684\u200bshape\u200b\u6709\u5173\u200b\uff0c\u200b\u56e0\u6b64\u200b\u540e\u79fb\u200b Global Mixing Block \u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5230\u200b\u6c60\u5316\u5c42\u200b\u4e4b\u540e\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4e0b\u964d\u200b\u4e3a\u200b71.9%\uff0c\u200b\u901f\u5ea6\u200b\u8d85\u8d8a\u200b\u57fa\u4e8e\u200bCNN\u200b\u7ed3\u6784\u200b\u7684\u200bPP-OCRv2-baseline 22%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> </ol> <p>\u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u901f\u5ea6\u200b\uff08CPU + MKLDNN) 01 PP-OCRv2-baseline 8.0M 69.30% 8.54ms 02 SVTR_Tiny 21.0M 80.10% 97.00ms 03 SVTR_LCNet(G4) 9.2M 76.00% 30.00ms 04 SVTR_LCNet(G2) 13.0M 72.98% 9.37ms 05 SVTR_LCNet(h32) 12.0M 71.90% 6.60ms 06 SVTR_LCNet(h48) 12.0M 73.98% 7.60ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c01-05\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,32,320)\uff1b PP-OCRv2-baseline \u200b\u4ee3\u8868\u200b\u6ca1\u6709\u200b\u501f\u52a9\u200b\u84b8\u998f\u200b\u65b9\u6cd5\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200b\u6a21\u578b\u200b</p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#2gtcattentionctc","title":"\uff082\uff09GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>GTC\uff08Guided Training of CTC\uff09\uff0c\u200b\u5229\u7528\u200bAttention\u200b\u6a21\u5757\u200bCTC\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u878d\u5408\u200b\u591a\u79cd\u200b\u6587\u672c\u200b\u7279\u5f81\u200b\u7684\u200b\u8868\u8fbe\u200b\uff0c\u200b\u662f\u200b\u4e00\u79cd\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u7b56\u7565\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5b8c\u5168\u200b\u53bb\u9664\u200b Attention \u200b\u6a21\u5757\u200b\uff0c\u200b\u5728\u200b\u63a8\u7406\u200b\u9636\u6bb5\u200b\u4e0d\u200b\u589e\u52a0\u200b\u4efb\u4f55\u200b\u8017\u65f6\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b75.8%\uff08+1.82%\uff09\u3002\u200b\u8bad\u7ec3\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#3textconaug","title":"\uff083\uff09TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565","text":"<p>TextConAug\u200b\u662f\u200b\u4e00\u79cd\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff0c\u200b\u4e3b\u8981\u200b\u601d\u60f3\u200b\u6765\u6e90\u4e8e\u200b\u8bba\u6587\u200bConCLR\uff0c\u200b\u4f5c\u8005\u200b\u63d0\u51fa\u200bConAug\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e2a\u200bbatch\u200b\u5185\u200b\u5bf9\u200b2\u200b\u5f20\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u8054\u7ed3\u200b\uff0c\u200b\u7ec4\u6210\u200b\u65b0\u200b\u7684\u200b\u56fe\u50cf\u200b\u5e76\u200b\u8fdb\u884c\u200b\u81ea\u200b\u76d1\u7763\u200b\u5bf9\u6bd4\u200b\u5b66\u4e60\u200b\u3002PP-OCRv3\u200b\u5c06\u200b\u6b64\u200b\u65b9\u6cd5\u200b\u5e94\u7528\u200b\u5230\u200b\u6709\u200b\u76d1\u7763\u200b\u7684\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u8bbe\u8ba1\u200b\u4e86\u200bTextConAug\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e30\u5bcc\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u63d0\u5347\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u591a\u6837\u6027\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b76.3%\uff08+0.5%\uff09\u3002TextConAug\u200b\u793a\u610f\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#4textrotnet","title":"\uff084\uff09TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>TextRotNet\u200b\u662f\u200b\u4f7f\u7528\u200b\u5927\u91cf\u200b\u65e0\u200b\u6807\u6ce8\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\u6570\u636e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u81ea\u200b\u76d1\u7763\u200b\u65b9\u5f0f\u200b\u8bad\u7ec3\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u53c2\u8003\u200b\u4e8e\u200b\u8bba\u6587\u200bSTR-Fewer-Labels\u3002\u200b\u8be5\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u521d\u59cb\u5316\u200bSVTR_LCNet\u200b\u7684\u200b\u521d\u59cb\u200b\u6743\u91cd\u200b\uff0c\u200b\u4ece\u800c\u200b\u5e2e\u52a9\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6536\u655b\u200b\u5230\u200b\u66f4\u4f73\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b76.9%\uff08+0.6%\uff09\u3002TextRotNet\u200b\u8bad\u7ec3\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#5udml","title":"\uff085\uff09UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>UDML\uff08Unified-Deep Mutual Learning\uff09\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u200b\u662f\u200bPP-OCRv2\u200b\u4e2d\u200b\u5c31\u200b\u91c7\u7528\u200b\u7684\u200b\u5bf9\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u975e\u5e38\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u7684\u200b\u7b56\u7565\u200b\u3002\u200b\u5728\u200bPP-OCRv3\u200b\u4e2d\u200b\uff0c\u200b\u9488\u5bf9\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200bSVTR_LCNet\u200b\u548c\u200bAttention\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5bf9\u200b\u4ed6\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200bPP-LCNet\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u3001SVTR\u200b\u6a21\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u548c\u200bAttention\u200b\u6a21\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u540c\u65f6\u200b\u8fdb\u884c\u200b\u76d1\u7763\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b78.4%\uff08+1.5%\uff09\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#6uim","title":"\uff086\uff09UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848","text":"<p>UIM\uff08Unlabeled Images Mining\uff09\u200b\u662f\u200b\u4e00\u79cd\u200b\u975e\u5e38\u7b80\u5355\u200b\u7684\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002\u200b\u6838\u5fc3\u601d\u60f3\u200b\u662f\u200b\u5229\u7528\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u5927\u200b\u6a21\u578b\u200b\u5bf9\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u83b7\u53d6\u200b\u4f2a\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5e76\u4e14\u200b\u9009\u62e9\u200b\u9884\u6d4b\u200b\u7f6e\u4fe1\u5ea6\u200b\u9ad8\u200b\u7684\u200b\u6837\u672c\u200b\u4f5c\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u5c0f\u200b\u6a21\u578b\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b79.4%\uff08+1%\uff09\u3002\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u5168\u91cf\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u9ad8\u7cbe\u5ea6\u200bSVTR-Tiny\u200b\u6a21\u578b\u200b\uff08acc=82.5%\uff09\u200b\u8fdb\u884c\u200b\u6570\u636e\u6316\u6398\u200b\uff0c\u200b\u70b9\u51fb\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\u548c\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv3_introduction.html#4","title":"4. \u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc4\u4f30","text":"<p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u4f18\u5316\u200b\uff0c\u200b\u6700\u7ec8\u200bPP-OCRv3\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u7aef\u5230\u200b\u7aef\u200bHmean\u200b\u6307\u6807\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv2\u200b\u63d0\u5347\u200b5%\uff0c\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u3002\u200b\u5177\u4f53\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Hmean Model Size (M) Time Cost (CPU, ms) Time Cost (T4 GPU, ms) PP-OCR mobile 50.30% 8.1 356.00 116.00 PP-OCR server 57.00% 155.1 1056.00 200.00 PP-OCRv2 57.60% 11.6 330.00 111.00 PP-OCRv3 62.90% 15.6 331.00 86.64 <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1aCPU\u200b\u578b\u53f7\u200b\u4e3a\u200bIntel Gold 6148\uff0cCPU\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p> <p>\u200b\u9664\u4e86\u200b\u66f4\u65b0\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\uff0c\u200b\u672c\u6b21\u200b\u5347\u7ea7\u200b\u4e5f\u200b\u540c\u6b65\u200b\u4f18\u5316\u200b\u4e86\u200b\u82f1\u6587\u200b\u6570\u5b57\u6a21\u578b\u200b\uff0c\u200b\u7aef\u5230\u200b\u7aef\u200b\u6548\u679c\u200b\u63d0\u5347\u200b11%\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Recall Precision Hmean PP-OCR_en 38.99% 45.91% 42.17% PP-OCRv3_en 50.95% 55.53% 53.14% <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e5f\u200b\u5bf9\u200b\u5df2\u200b\u652f\u6301\u200b\u7684\u200b80\u200b\u4f59\u79cd\u200b\u8bed\u8a00\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5728\u200b\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u7684\u200b\u56db\u79cd\u200b\u8bed\u7cfb\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u5e73\u5747\u200b\u63d0\u5347\u200b5%\u200b\u4ee5\u4e0a\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model \u200b\u62c9\u4e01\u8bed\u7cfb\u200b \u200b\u963f\u62c9\u4f2f\u8bed\u200b\u7cfb\u200b \u200b\u65e5\u8bed\u200b \u200b\u97e9\u8bed\u200b PP-OCR_mul 69.60% 40.50% 38.50% 55.40% PP-OCRv3_mul 75.20% 45.37% 45.80% 60.10%"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html","title":"PP-OCRv4","text":""},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>PP-OCRv4\u200b\u5728\u200bPP-OCRv3\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv3\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6570\u636e\u200b\u3001\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u7b49\u200b\u591a\u4e2a\u200b\u6a21\u5757\u200b\u7684\u200b\u4f18\u5316\u200b\u3002 PP-OCRv4\u200b\u7cfb\u7edf\u200b\u6846\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u4ece\u200b\u7b97\u6cd5\u200b\u6539\u8fdb\u200b\u601d\u8def\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5171\u200b10\u200b\u4e2a\u200b\u65b9\u9762\u200b\u7684\u200b\u6539\u8fdb\u200b\uff1a</p> <ul> <li> <p>\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b</li> <li>PFHead\uff1a\u200b\u5e76\u884c\u200bhead\u200b\u5206\u652f\u200b\u878d\u5408\u200b\u7ed3\u6784\u200b</li> <li>DSR: \u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u52a8\u6001\u200b\u589e\u52a0\u200bshrink ratio</li> <li>CML\uff1a\u200b\u6dfb\u52a0\u200bStudent\u200b\u548c\u200bTeacher\u200b\u7f51\u7edc\u200b\u8f93\u51fa\u200b\u7684\u200bKL div loss</li> </ul> </li> <li> <p>\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>SVTR_LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b</li> <li>Lite-Neck\uff1a\u200b\u7cbe\u7b80\u200b\u7684\u200bNeck\u200b\u7ed3\u6784\u200b</li> <li>GTC-NRTR\uff1a\u200b\u7a33\u5b9a\u200b\u7684\u200bAttention\u200b\u6307\u5bfc\u200b\u5206\u652f\u200b</li> <li>Multi-Scale\uff1a\u200b\u591a\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b</li> <li>DF: \u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b</li> <li>DKD \uff1aDKD\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b</li> </ul> </li> </ul> <p>\u200b\u4ece\u200b\u6548\u679c\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u591a\u79cd\u200b\u573a\u666f\u200b\u7cbe\u5ea6\u200b\u5747\u200b\u6709\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff1a</p> <ul> <li>\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200bPP-OCRv3\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b\u8d85\u200b4%\uff1b</li> <li>\u200b\u82f1\u6587\u200b\u6570\u5b57\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv3\u200b\u82f1\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b6%\uff1b</li> <li>\u200b\u591a\u200b\u8bed\u8a00\u200b\u573a\u666f\u200b\uff0c\u200b\u4f18\u5316\u200b80\u200b\u4e2a\u200b\u8bed\u79cd\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\uff0c\u200b\u5e73\u5747\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b\u8d85\u200b8%\u3002</li> </ul>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#2","title":"2. \u200b\u68c0\u6d4b\u200b\u4f18\u5316","text":"<p>PP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5728\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u7ed3\u6784\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u4e09\u4e2a\u200b\u65b9\u9762\u200b\u505a\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u9996\u5148\u200b\uff0cPP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPP-LCNetV3\u200b\u66ff\u6362\u200bMobileNetv3\uff0c\u200b\u5e76\u200b\u63d0\u51fa\u200b\u5e76\u884c\u200b\u5206\u652f\u200b\u878d\u5408\u200b\u7684\u200bPFhead\u200b\u7ed3\u6784\u200b\uff1b\u200b\u5176\u6b21\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u52a8\u6001\u200b\u8c03\u6574\u200bshrink ratio\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff1b\u200b\u6700\u540e\u200b\uff0cPP-OCRv4\u200b\u5bf9\u200bCML\u200b\u7684\u200b\u84b8\u998f\u200bloss\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b hmean \u200b\u901f\u5ea6\u200b\uff08cpu + mkldnn) baseline PP-OCRv3 3.4M 78.84% 69ms baseline student PP-OCRv3 student 3.4M 76.22% 69ms 01 +PFHead 3.6M 76.97% 96ms 02 +Dynamic Shrink Ratio 3.6M 78.24% 96ms 03 +PP-LCNetv3 4.8M 79.08% 94ms 03 +CML 4.8M 79.87% 67ms <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u4f7f\u7528\u200bopenvino\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#1pfheadhead","title":"\uff081\uff09PFhead\uff1a\u200b\u591a\u200b\u5206\u652f\u200b\u878d\u5408\u200bHead\u200b\u7ed3\u6784","text":"<p>PFhead\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPFHead\u200b\u5728\u200b\u7ecf\u8fc7\u200b\u7b2c\u4e00\u4e2a\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u540e\u200b\uff0c\u200b\u5206\u522b\u200b\u8fdb\u884c\u200b\u4e0a\u200b\u91c7\u6837\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\uff0c\u200b\u4e0a\u200b\u91c7\u6837\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u8fc7\u200b3x3\u200b\u5377\u79ef\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\uff0c\u200b\u7136\u540e\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u7684\u200b\u5206\u652f\u200b\u7684\u200b\u7ed3\u679c\u200b\u7ea7\u8054\u200b\u5e76\u200b\u7ecf\u8fc7\u200b1x1\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u6700\u540e\u200b1x1\u200b\u5377\u79ef\u200b\u7684\u200b\u7ed3\u679c\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u7684\u200b\u7ed3\u679c\u200b\u76f8\u52a0\u200b\u5f97\u5230\u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u6982\u7387\u200b\u56fe\u200b\u3002PP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPFhead\uff0chmean\u200b\u4ece\u200b76.22%\u200b\u589e\u52a0\u200b\u5230\u200b76.97%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#2dsr","title":"\uff082\uff09DSR: \u200b\u6536\u7f29\u200b\u6bd4\u4f8b\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u7b56\u7565","text":"<p>\u200b\u52a8\u6001\u200bshrink ratio(dynamic shrink ratio): \u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0cshrink ratio\u200b\u7531\u200b\u56fa\u5b9a\u503c\u200b\u8c03\u6574\u200b\u4e3a\u200b\u52a8\u6001\u53d8\u5316\u200b\uff0c\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200bepoch\u200b\u7684\u200b\u589e\u52a0\u200b\uff0cshrink ratio\u200b\u4ece\u200b0.4\u200b\u7ebf\u6027\u200b\u589e\u52a0\u200b\u5230\u200b0.6\u3002\u200b\u8be5\u200b\u7b56\u7565\u200b\u5728\u200bPP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4e0a\u200b\uff0chmean\u200b\u4ece\u200b76.97%\u200b\u63d0\u5347\u200b\u5230\u200b78.24%\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#3-pp-lcnetv3","title":"(3) PP-LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc","text":"<p>PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u662f\u200bPP-LCNet\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u7684\u200b\u5ef6\u7eed\u200b\uff0c\u200b\u8986\u76d6\u200b\u4e86\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\uff0c\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u4e0d\u540c\u200b\u4e0b\u6e38\u200b\u4efb\u52a1\u200b\u7684\u200b\u9700\u8981\u200b\u3002PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4ece\u200b\u591a\u4e2a\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u53ef\u200b\u5b66\u4e60\u200b\u4eff\u5c04\u53d8\u6362\u200b\u6a21\u5757\u200b\uff0c\u200b\u5bf9\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7b56\u7565\u200b\u3001\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\uff0c\u200b\u540c\u65f6\u200b\u8c03\u6574\u200b\u4e86\u200b\u7f51\u7edc\u200b\u6df1\u5ea6\u200b\u4e0e\u200b\u5bbd\u5ea6\u200b\u3002\u200b\u6700\u7ec8\u200b\uff0cPP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5728\u200b\u6027\u80fd\u200b\u4e0e\u200b\u6548\u7387\u200b\u4e4b\u95f4\u200b\u8fbe\u5230\u6700\u4f73\u200b\u7684\u200b\u5e73\u8861\u200b\uff0c\u200b\u5728\u200b\u4e0d\u540c\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\u5185\u200b\u53d6\u5f97\u200b\u6781\u81f4\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002\u200b\u4f7f\u7528\u200bPP-LCNetV3\u200b\u66ff\u6362\u200bMobileNetv3 backbone\uff0cPP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200bhmean\u200b\u4ece\u200b78.24%\u200b\u63d0\u5347\u200b\u5230\u200b79.08%\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#4cml-kd","title":"\uff084\uff09CML: \u200b\u878d\u5408\u200bKD\u200b\u7684\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>PP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5bf9\u200bPP-OCRv3\u200b\u4e2d\u200b\u7684\u200bCML\uff08Collaborative Mutual Learning) \u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200bStudent Model\u200b\u548c\u200bTeacher Model\u200b\u7684\u200bdistill Loss\u200b\u65f6\u200b\uff0c\u200b\u989d\u5916\u200b\u6dfb\u52a0\u200bKL div loss\uff0c\u200b\u8ba9\u200b\u4e24\u8005\u200b\u8f93\u51fa\u200b\u7684\u200bresponse maps\u200b\u5206\u5e03\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u7531\u6b64\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200bStudent\u200b\u7f51\u7edc\u200b\u7684\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u68c0\u6d4b\u200bHmean\u200b\u4ece\u200b79.08%\u200b\u589e\u52a0\u200b\u5230\u200b79.56%\uff0c\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u4ece\u200b61.31%\u200b\u589e\u52a0\u200b\u5230\u200b61.87%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#3","title":"3. \u200b\u8bc6\u522b\u200b\u4f18\u5316","text":"<p>PP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200bPP-OCRv3\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u5206\u522b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6570\u636e\u200b\u3001\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u4f18\u5316\u200b\u3002</p> <p></p> <p>\u200b\u7ecf\u8fc7\u200b\u5982\u56fe\u6240\u793a\u200b\u7684\u200b\u7b56\u7565\u200b\u4f18\u5316\u200b\uff0cPP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200bPP-OCRv3\uff0c\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b4%\u3002 \u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\uff08CPU openvino) 01 PP-OCRv3 12M 71.50% 8.54ms 02 +DF 12M 72.70% 8.54ms 03 + LiteNeck + GTC 9.6M 73.21% 9.09ms 04 + PP-LCNetV3 11M 74.18% 9.8ms 05 + multi-scale 11M 74.20% 9.8ms 06 + TextConAug 11M 74.72% 9.8ms 08 + UDML 11M 75.45% 9.8ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,48,320)\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u56fe\u50cf\u200b\u4e3a\u200b\u53d8\u957f\u200b\u8f93\u5165\u200b\uff0c\u200b\u901f\u5ea6\u200b\u4f1a\u200b\u6709\u6240\u200b\u53d8\u5316\u200b\u3002\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u4f7f\u7528\u200bOpenvino\u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#1df","title":"\uff081\uff09DF\uff1a\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848","text":"<p>DF(Data Filter) \u200b\u662f\u200b\u4e00\u79cd\u200b\u7b80\u5355\u200b\u6709\u6548\u200b\u7684\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002\u200b\u6838\u5fc3\u601d\u60f3\u200b\u662f\u200b\u5229\u7528\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u7f6e\u4fe1\u5ea6\u200b\u548c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5bf9\u200b\u5168\u91cf\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u7b5b\u9009\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\uff1a\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u5feb\u901f\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u4f4e\u200b\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8be5\u200b\u4f4e\u200b\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5343\u4e07\u7ea7\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u53bb\u9664\u200b\u7f6e\u4fe1\u5ea6\u200b\u5927\u4e8e\u200b0.95\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u8be5\u200b\u90e8\u5206\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u5bf9\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u65e0\u6548\u200b\u7684\u200b\u5197\u4f59\u200b\u6837\u672c\u200b\u3002\u200b\u5176\u6b21\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u4f5c\u4e3a\u200b\u9ad8\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\uff0c\u200b\u5bf9\u200b\u5269\u4f59\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u53bb\u9664\u200b\u7f6e\u4fe1\u5ea6\u200b\u5c0f\u4e8e\u200b0.15\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u8be5\u200b\u90e8\u5206\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u96be\u4ee5\u200b\u8bc6\u522b\u200b\u6216\u200b\u8d28\u91cf\u200b\u5f88\u5dee\u200b\u7684\u200b\u6837\u672c\u200b\u3002 \u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u5343\u4e07\u200b\u7ea7\u522b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u88ab\u200b\u7cbe\u7b80\u200b\u81f3\u200b\u767e\u4e07\u200b\u7ea7\u200b\uff0c\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u4ece\u200b2\u200b\u5468\u200b\u51cf\u5c11\u200b\u5230\u200b5\u200b\u5929\u200b\uff0c\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b\uff0c\u200b\u540c\u65f6\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b72.7%(+1.2%)\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#2pp-lcnetv3","title":"\uff082\uff09PP-LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u4f18\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc","text":"<p>PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u662f\u200bPP-LCNet\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u7684\u200b\u5ef6\u7eed\u200b\uff0c\u200b\u8986\u76d6\u200b\u4e86\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\uff0c\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u4e0d\u540c\u200b\u4e0b\u6e38\u200b\u4efb\u52a1\u200b\u7684\u200b\u9700\u8981\u200b\u3002PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4ece\u200b\u591a\u4e2a\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u53ef\u200b\u5b66\u4e60\u200b\u4eff\u5c04\u53d8\u6362\u200b\u6a21\u5757\u200b\uff0c\u200b\u5bf9\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7b56\u7565\u200b\u3001\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\uff0c\u200b\u540c\u65f6\u200b\u8c03\u6574\u200b\u4e86\u200b\u7f51\u7edc\u200b\u6df1\u5ea6\u200b\u4e0e\u200b\u5bbd\u5ea6\u200b\u3002\u200b\u6700\u7ec8\u200b\uff0cPP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5728\u200b\u6027\u80fd\u200b\u4e0e\u200b\u6548\u7387\u200b\u4e4b\u95f4\u200b\u8fbe\u5230\u6700\u4f73\u200b\u7684\u200b\u5e73\u8861\u200b\uff0c\u200b\u5728\u200b\u4e0d\u540c\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\u5185\u200b\u53d6\u5f97\u200b\u6781\u81f4\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#3lite-neckneck","title":"\uff083\uff09Lite-Neck\uff1a\u200b\u7cbe\u7b80\u200b\u53c2\u6570\u200b\u7684\u200bNeck\u200b\u7ed3\u6784","text":"<p>Lite-Neck\u200b\u6574\u4f53\u200b\u7ed3\u6784\u200b\u6cbf\u7528\u200bPP-OCRv3\u200b\u7248\u672c\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5728\u200b\u53c2\u6570\u200b\u4e0a\u200b\u7a0d\u4f5c\u200b\u7cbe\u7b80\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u7684\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u53ef\u200b\u4ece\u200b12M\u200b\u964d\u4f4e\u200b\u5230\u200b8.5M\uff0c\u200b\u800c\u200b\u7cbe\u5ea6\u200b\u4e0d\u53d8\u200b\uff1b\u200b\u5728\u200bCTCHead\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200bNeck\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4ece\u200b64\u200b\u63d0\u5347\u200b\u5230\u200b120\uff0c\u200b\u6b64\u65f6\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u4ece\u200b8.5M\u200b\u63d0\u5347\u200b\u5230\u200b9.6M\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#4gtc-nrtrattentionctc","title":"\uff084\uff09GTC-NRTR\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>GTC\uff08Guided Training of CTC\uff09\uff0c\u200b\u662f\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u6700\u200b\u6709\u6548\u200b\u7684\u200b\u7b56\u7565\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u878d\u5408\u200b\u591a\u79cd\u200b\u6587\u672c\u200b\u7279\u5f81\u200b\u7684\u200b\u8868\u8fbe\u200b\uff0c\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u5728\u200bPP-OCRv4\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u7a33\u5b9a\u200b\u7684\u200bTransformer\u200b\u6a21\u578b\u200bNRTR\u200b\u4f5c\u4e3a\u200b\u6307\u5bfc\u200b\u5206\u652f\u200b\uff0c\u200b\u76f8\u6bd4\u200bV3\u200b\u7248\u672c\u200b\u4e2d\u200b\u7684\u200bSAR\u200b\u57fa\u4e8e\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0cNRTR\u200b\u57fa\u4e8e\u200bTransformer\u200b\u5b9e\u73b0\u200b\u89e3\u7801\u200b\u8fc7\u7a0b\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u66f4\u5f3a\u200b\uff0c\u200b\u80fd\u200b\u6709\u6548\u200b\u6307\u5bfc\u200bCTC\u200b\u5206\u652f\u200b\u5b66\u4e60\u200b\uff0c\u200b\u89e3\u51b3\u200b\u7b80\u5355\u200b\u573a\u666f\u200b\u4e0b\u200b\u5feb\u901f\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u4f7f\u7528\u200bLite-Neck\u200b\u548c\u200bGTC-NRTR\u200b\u4e24\u4e2a\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b73.21%(+0.5%)\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#5multi-scale","title":"\uff085\uff09Multi-Scale\uff1a\u200b\u591a\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>\u200b\u52a8\u6001\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u968f\u673a\u200bresize\u200b\u8f93\u5165\u200b\u56fe\u7247\u200b\u7684\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u4ee5\u200b\u589e\u5f3a\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u4e32\u8054\u200b\u4f7f\u7528\u200b\u65f6\u200b\u7684\u200b\u9c81\u68d2\u6027\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u6bcf\u4e2a\u200biter\u200b\u4ece\u200b\uff0832\uff0c48\uff0c64\uff09\u200b\u4e09\u79cd\u200b\u9ad8\u5ea6\u200b\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e00\u79cd\u200b\u9ad8\u5ea6\u200b\u8fdb\u884c\u200bresize\u3002\u200b\u5b9e\u9a8c\u200b\u8bc1\u660e\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u5728\u200b\u8bc6\u522b\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u51c6\u786e\u7387\u200b\u6ca1\u6709\u200b\u63d0\u5347\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u4e32\u8054\u200b\u8bc4\u4f30\u200b\u65f6\u200b\uff0c\u200b\u6307\u6807\u200b\u63d0\u5347\u200b0.5%\u3002</p> <p></p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#6dkd","title":"\uff086\uff09DKD\uff1a\u200b\u84b8\u998f\u200b\u7b56\u7565","text":"<p>\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u84b8\u998f\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff0cNRTRhead\u200b\u84b8\u998f\u200b\u548c\u200bCTCHead\u200b\u84b8\u998f\u200b;</p> <p>\u200b\u5bf9\u4e8e\u200bNRTR head\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200bDKD loss\u200b\u84b8\u998f\u200b\uff0c\u200b\u62c9\u8fd1\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u548c\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u7684\u200bNRTR head logits\u3002\u200b\u6700\u7ec8\u200bNRTR head\u200b\u7684\u200bloss\u200b\u662f\u200b\u5b66\u751f\u200b\u4e0e\u200b\u6559\u5e08\u200b\u95f4\u200b\u7684\u200bDKD loss\u200b\u548c\u200b\u4e0e\u200bground truth\u200b\u7684\u200bcross entropy loss\u200b\u7684\u200b\u52a0\u6743\u200b\u548c\u200b\uff0c\u200b\u7528\u4e8e\u200b\u76d1\u7763\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bbackbone\u200b\u8bad\u7ec3\u200b\u3002\u200b\u901a\u8fc7\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u52a0\u5165\u200bDKD loss\u200b\u540e\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u4e0e\u200bground truth\u200b\u7684\u200bcross entropy loss\u200b\u65f6\u200b\u53bb\u9664\u200blabel smoothing\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u4e0d\u5e26\u200blabel smoothing\u200b\u7684\u200bcross entropy loss\u3002</p> <p>\u200b\u5bf9\u4e8e\u200bCTCHead\uff0c\u200b\u7531\u4e8e\u200bCTC\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u5b58\u5728\u200bBlank\u200b\u4f4d\u200b\uff0c\u200b\u5373\u4f7f\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e00\u6837\u200b\uff0c\u200b\u4e8c\u8005\u200b\u7684\u200b\u8f93\u51fa\u200b\u7684\u200blogits\u200b\u5206\u5e03\u200b\u4e5f\u200b\u4f1a\u200b\u5b58\u5728\u200b\u5dee\u5f02\u200b\uff0c\u200b\u5f71\u54cd\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u5411\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u77e5\u8bc6\u200b\u4f20\u9012\u200b\u3002PP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200bCTC\u200b\u8f93\u51fa\u200blogits\u200b\u6cbf\u7740\u200b\u6587\u672c\u200b\u957f\u5ea6\u200b\u7ef4\u5ea6\u200b\u8ba1\u7b97\u200b\u5747\u503c\u200b\uff0c\u200b\u5c06\u200b\u591a\u200b\u5b57\u7b26\u8bc6\u522b\u200b\u95ee\u9898\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u591a\u200b\u5b57\u7b26\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u7528\u4e8e\u200b\u76d1\u7763\u200bCTC Head\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\u878d\u5408\u200bNRTRhead DKD\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff0c\u200b\u6307\u6807\u200b\u4ece\u200b74.72%\u200b\u63d0\u5347\u200b\u5230\u200b75.45%\u3002</p>"},{"location":"en/ppocr/blog/PP-OCRv4_introduction.html#4","title":"4. \u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc4\u4f30","text":"<p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u4f18\u5316\u200b\uff0c\u200b\u6700\u7ec8\u200bPP-OCRv4\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u7aef\u5230\u200b\u7aef\u200bHmean\u200b\u6307\u6807\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv3\u200b\u63d0\u5347\u200b4.5%\uff0c\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u3002\u200b\u5177\u4f53\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Hmean Model Size (M) Time Cost (CPU, ms) PP-OCRv3 57.99% 15.6 78 PP-OCRv4 62.24% 15.8 76 <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1aCPU\u200b\u578b\u53f7\u200b\u4e3a\u200bIntel Gold 6148\uff0cCPU\u200b\u9884\u6d4b\u200b\u65f6\u200b\u4f7f\u7528\u200bopenvino\u3002</p> <p>\u200b\u9664\u4e86\u200b\u66f4\u65b0\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\uff0c\u200b\u672c\u6b21\u200b\u5347\u7ea7\u200b\u4e5f\u200b\u4f18\u5316\u200b\u4e86\u200b\u82f1\u6587\u200b\u6570\u5b57\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u81ea\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u4e0a\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b6%\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model ACC PP-OCR_en 54.38% PP-OCRv3_en 64.04% PP-OCRv4_en 70.1% <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u5bf9\u200b\u5df2\u200b\u652f\u6301\u200b\u7684\u200b80\u200b\u4f59\u79cd\u200b\u8bed\u8a00\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5728\u200b\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u7684\u200b\u56db\u79cd\u200b\u8bed\u7cfb\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u5e73\u5747\u200b\u63d0\u5347\u200b8%\u200b\u4ee5\u4e0a\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model \u200b\u62c9\u4e01\u8bed\u7cfb\u200b \u200b\u963f\u62c9\u4f2f\u8bed\u200b\u7cfb\u200b \u200b\u65e5\u8bed\u200b \u200b\u97e9\u8bed\u200b PP-OCR_mul 69.60% 40.50% 38.50% 55.40% PP-OCRv3_mul 71.57% 72.90% 45.85% 77.23% PP-OCRv4_mul 80.00% 75.48% 56.50% 83.25%"},{"location":"en/ppocr/blog/clone.html","title":"Project Clone","text":""},{"location":"en/ppocr/blog/clone.html#1-clone-paddleocr","title":"1. Clone PaddleOCR","text":"<pre><code># Recommend\ngit clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If you cannot pull successfully due to network problems, you can switch to the mirror hosted on Gitee:\n\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: The mirror on Gitee may not keep in synchronization with the latest project on GitHub. There might be a delay of 3-5 days. Please try GitHub at first.\n</code></pre>"},{"location":"en/ppocr/blog/clone.html#2-install-third-party-libraries","title":"2. Install third-party libraries","text":"<pre><code>cd PaddleOCR\npip3 install -r requirements.txt\n</code></pre> <p>If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows.</p> <p>Please try to download Shapely whl file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely.</p> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/ppocr/blog/config.html","title":"Configuration","text":""},{"location":"en/ppocr/blog/config.html#1-optional-parameter-list","title":"1. Optional Parameter List","text":"<p>The following list can be viewed through <code>--help</code></p> FLAG Supported script Use Defaults Note -c ALL Specify configuration file to use None Please refer to the parameter introduction for configuration file usage -o ALL set configuration options None Configuration using -o has higher priority than the configuration file selected with -c. E.g: -o Global.use_gpu=false"},{"location":"en/ppocr/blog/config.html#2-introduction-to-global-parameters-of-configuration-file","title":"2. Introduction to Global Parameters of Configuration File","text":"<p>Take rec_chinese_lite_train_v2.0.yml as an example</p>"},{"location":"en/ppocr/blog/config.html#global","title":"Global","text":"Parameter Use Defaults Note use_gpu Set using GPU or not true \\ epoch_num Maximum training epoch number 500 \\ log_smooth_window Log queue length, the median value in the queue each time will be printed 20 \\ print_batch_step Set print log interval 10 \\ save_model_dir Set model save path output/{\u200b\u7b97\u6cd5\u200b\u540d\u79f0\u200b} \\ save_epoch_step Set model save interval 3 \\ eval_batch_step Set the model evaluation interval 2000 or [1000, 2000] running evaluation every 2000 iters or evaluation is run every 2000 iterations after the 1000th iteration cal_metric_during_train Set whether to evaluate the metric during the training process. At this time, the metric of the model under the current batch is evaluated true \\ load_static_weights Set whether the pre-training model is saved in static graph mode (currently only required by the detection algorithm) true \\ pretrained_model Set the path of the pre-trained model ./pretrain_models/CRNN/best_accuracy \\ checkpoints set model parameter path None Used to load parameters after interruption to continue training use_visualdl Set whether to enable visualdl for visual log display False Tutorial use_wandb Set whether to enable W&amp;B for visual log display False Documentation infer_img Set inference image path or folder path ./infer_img | character_dict_path Set dictionary path ./ppocr/utils/ppocr_keys_v1.txt If the character_dict_path is None, model can only recognize number and lower letters max_text_length Set the maximum length of text 25 \\ use_space_char Set whether to recognize spaces True | label_list Set the angle supported by the direction classifier ['0','180'] Only valid in angle classifier model save_res_path Set the save address of the test model results ./output/det_db/predicts_db.txt Only valid in the text detection model"},{"location":"en/ppocr/blog/config.html#optimizer-ppocroptimizer","title":"Optimizer (ppocr/optimizer)","text":"Parameter Use Defaults Note name Optimizer class name Adam Currently supports<code>Momentum</code>,<code>Adam</code>,<code>RMSProp</code>, see ppocr/optimizer/optimizer.py beta1 Set the exponential decay rate for the 1st moment estimates 0.9 \\ beta2 Set the exponential decay rate for the 2nd moment estimates 0.999 \\ clip_norm The maximum norm value - \\ lr Set the learning rate decay method - \\ name Learning rate decay class name Cosine Currently supports<code>Linear</code>,<code>Cosine</code>,<code>Step</code>,<code>Piecewise</code>, seeppocr/optimizer/learning_rate.py learning_rate Set the base learning rate 0.001 \\ regularizer Set network regularization method - \\ name Regularizer class name L2 Currently support<code>L1</code>,<code>L2</code>, seeppocr/optimizer/regularizer.py factor Regularizer coefficient 0.00001 \\"},{"location":"en/ppocr/blog/config.html#architecture-ppocrmodeling","title":"Architecture (ppocr/modeling)","text":"<p>In PaddleOCR, the network is divided into four stages: Transform, Backbone, Neck and Head</p> Parameter Use Defaults Note model_type Network Type rec Currently support<code>rec</code>,<code>det</code>,<code>cls</code> algorithm Model name CRNN See algorithm_overview for the support list Transform Set the transformation method - Currently only recognition algorithms are supported, see ppocr/modeling/transform for details name Transformation class name TPS Currently supports <code>TPS</code> num_fiducial Number of TPS control points 20 Ten on the top and bottom loc_lr Localization network learning rate 0.1 \\ model_name Localization network size small Currently support<code>small</code>,<code>large</code> Backbone Set the network backbone class name - see ppocr/modeling/backbones name backbone class name ResNet Currently support<code>MobileNetV3</code>,<code>ResNet</code> layers resnet layers 34 Currently support18,34,50,101,152,200 model_name MobileNetV3 network size small Currently support<code>small</code>,<code>large</code> Neck Set network neck - seeppocr/modeling/necks name neck class name SequenceEncoder Currently support<code>SequenceEncoder</code>,<code>DBFPN</code> encoder_type SequenceEncoder encoder type rnn Currently support<code>reshape</code>,<code>fc</code>,<code>rnn</code> hidden_size rnn number of internal units 48 \\ out_channels Number of DBFPN output channels 256 \\ Head Set the network head - seeppocr/modeling/heads name head class name CTCHead Currently support<code>CTCHead</code>,<code>DBHead</code>,<code>ClsHead</code> fc_decay CTCHead regularization coefficient 0.0004 \\ k DBHead binarization coefficient 50 \\ class_dim ClsHead output category number 2 \\"},{"location":"en/ppocr/blog/config.html#loss-ppocrlosses","title":"Loss (ppocr/losses)","text":"Parameter Use Defaults Note name loss class name CTCLoss Currently support<code>CTCLoss</code>,<code>DBLoss</code>,<code>ClsLoss</code> balance_loss Whether to balance the number of positive and negative samples in DBLossloss (using OHEM) True \\ ohem_ratio The negative and positive sample ratio of OHEM in DBLossloss 3 \\ main_loss_type The loss used by shrink_map in DBLossloss DiceLoss Currently support<code>DiceLoss</code>,<code>BCELoss</code> alpha The coefficient of shrink_map_loss in DBLossloss 5 \\ beta The coefficient of threshold_map_loss in DBLossloss 10 \\"},{"location":"en/ppocr/blog/config.html#postprocess-ppocrpostprocess","title":"PostProcess (ppocr/postprocess)","text":"Parameter Use Defaults Note name Post-processing class name CTCLabelDecode Currently support<code>CTCLoss</code>,<code>AttnLabelDecode</code>,<code>DBPostProcess</code>,<code>ClsPostProcess</code> thresh The threshold for binarization of the segmentation map in DBPostProcess 0.3 \\ box_thresh The threshold for filtering output boxes in DBPostProcess. Boxes below this threshold will not be output 0.7 \\ max_candidates The maximum number of text boxes output in DBPostProcess 1000 unclip_ratio The unclip ratio of the text box in DBPostProcess 2.0 \\"},{"location":"en/ppocr/blog/config.html#metric-ppocrmetrics","title":"Metric (ppocr/metrics)","text":"Parameter Use Defaults Note name Metric method name CTCLabelDecode Currently support<code>DetMetric</code>,<code>RecMetric</code>,<code>ClsMetric</code> main_indicator Main indicators, used to select the best model acc For the detection method is hmean, the recognition and classification method is acc"},{"location":"en/ppocr/blog/config.html#dataset-ppocrdata","title":"Dataset  (ppocr/data)","text":"Parameter Use Defaults Note dataset Return one sample per iteration - - name dataset class name SimpleDataSet Currently support<code>SimpleDataSet</code>,<code>LMDBDataSet</code> data_dir Image folder path ./train_data \\ label_file_list Groundtruth file path [\"./train_data/train_list.txt\"] This parameter is not required when dataset is LMDBDataSet ratio_list Ratio of data set [1.0] If there are two train_lists in label_file_list and ratio_list is [0.4,0.6], 40% will be sampled from train_list1, and 60% will be sampled from train_list2 to combine the entire dataset transforms List of methods to transform images and labels [DecodeImage,CTCLabelEncode,RecResizeImg,KeepKeys] seeppocr/data/imaug loader dataloader related - shuffle Does each epoch disrupt the order of the data set True \\ batch_size_per_card Single card batch size during training 256 \\ drop_last Whether to discard the last incomplete mini-batch because the number of samples in the data set cannot be divisible by batch_size True \\ num_workers The number of sub-processes used to load data, if it is 0, the sub-process is not started, and the data is loaded in the main process 8 \\"},{"location":"en/ppocr/blog/config.html#weights-biases-wb","title":"Weights &amp; Biases (W&amp;B)","text":"Parameter Use Defaults Note project Project to which the run is to be logged uncategorized \\ name Alias/Name of the run Randomly generated by wandb \\ id ID of the run Randomly generated by wandb \\ entity User or team to which the run is being logged The logged in user \\ save_dir local directory in which all the models and other data is saved wandb \\ config model configuration None \\"},{"location":"en/ppocr/blog/config.html#3-multilingual-config-file-generation","title":"3. Multilingual Config File Generation","text":"<p>PaddleOCR currently supports recognition for 80 languages (besides Chinese). A multi-language configuration file template is provided under the path <code>configs/rec/multi_languages</code>: rec_multi_language_lite_train.yml\u3002</p> <p>There are two ways to create the required configuration file:</p> <ol> <li>Automatically generated by script</li> </ol> <p>Script generate_multi_language_configs.py can help you generate configuration files for multi-language models.</p> <ul> <li> <p>Take Italian as an example, if your data is prepared in the following format:</p> <pre><code>|-train_data\n    |- it_train.txt # train_set label\n    |- it_val.txt # val_set label\n    |- data\n        |- word_001.jpg\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre> <p>You can use the default parameters to generate a configuration file:</p> <pre><code># The code needs to be run in the specified directory\ncd PaddleOCR/configs/rec/multi_language/\n# Set the configuration file of the language to be generated through the -l or --language parameter.\n# This command will write the default parameters into the configuration file\npython3 generate_multi_language_configs.py -l it\n</code></pre> </li> <li> <p>If your data is placed in another location, or you want to use your own dictionary, you can generate the configuration file by specifying the relevant parameters:</p> <pre><code># -l or --language field is required\n# --train to modify the training set\n# --val to modify the validation set\n# --data_dir to modify the data set directory\n# --dict to modify the dict path\n# -o to modify the corresponding default parameters\ncd PaddleOCR/configs/rec/multi_language/\npython3 generate_multi_language_configs.py -l it \\  # language\n--train {path/of/train_label.txt} \\ # path of train_label\n--val {path/of/val_label.txt} \\     # path of val_label\n--data_dir {train_data/path} \\      # root directory of training data\n--dict {path/of/dict} \\             # path of dict\n-o Global.use_gpu=False             # whether to use gpu\n...\n</code></pre> </li> </ul> <p>Italian is made up of Latin letters, so after executing the command, you will get the rec_latin_lite_train.yml.</p> <ol> <li>Manually modify the configuration file</li> </ol> <p>You can also manually modify the following fields in the template:</p> <pre><code> Global:\n   use_gpu: True\n   epoch_num: 500\n   ...\n   character_dict_path:  {path/of/dict} # path of dict\n\nTrain:\n   dataset:\n     name: SimpleDataSet\n     data_dir: train_data/ # root directory of training data\n     label_file_list: [\"./train_data/train_list.txt\"] # train label path\n   ...\n\nEval:\n   dataset:\n     name: SimpleDataSet\n     data_dir: train_data/ # root directory of val data\n     label_file_list: [\"./train_data/val_list.txt\"] # val label path\n   ...\n</code></pre> <p>Currently, the multi-language algorithms supported by PaddleOCR are:</p> Configuration file Algorithm name backbone trans seq pred language rec_chinese_cht_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc chinese traditional rec_en_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc English(Case sensitive) rec_french_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc French rec_ger_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc German rec_japan_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Japanese rec_korean_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Korean rec_latin_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Latin rec_arabic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc arabic rec_cyrillic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc cyrillic rec_devanagari_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc devanagari <p>For more supported languages, please refer to : Multi-language model</p> <p>The multi-language model training method is the same as the Chinese model. The training data set is 100w synthetic data. A small amount of fonts and test data can be downloaded using the following two methods.</p> <ul> <li>Baidu Netdisk,Extraction code:frgi.</li> <li>Google drive</li> </ul>"},{"location":"en/ppocr/blog/customize.html","title":"HOW TO MAKE YOUR OWN LIGHTWEIGHT OCR MODEL?","text":"<p>The process of making a customized ultra-lightweight OCR models can be divided into three steps: training text detection model, training text recognition model, and concatenate the predictions from previous steps.</p>"},{"location":"en/ppocr/blog/customize.html#step1-train-text-detection-model","title":"STEP1: TRAIN TEXT DETECTION MODEL","text":"<p>PaddleOCR provides two text detection algorithms: EAST and DB. Both support MobileNetV3 and ResNet50_vd backbone networks, select the corresponding configuration file as needed and start training. For example, to train with MobileNetV3 as the backbone network for DB detection model :</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml 2&gt;&amp;1 | tee det_db.log\n</code></pre> <p>For more details about data preparation and training tutorials, refer to the documentation Text detection model training/evaluation/prediction</p>"},{"location":"en/ppocr/blog/customize.html#step2-train-text-recognition-model","title":"STEP2: TRAIN TEXT RECOGNITION MODEL","text":"<p>PaddleOCR provides four text recognition algorithms: CRNN, Rosetta, STAR-Net, and RARE. They all support two backbone networks: MobileNetV3 and ResNet34_vd, select the corresponding configuration files as needed to start training. For example, to train a CRNN recognition model that uses MobileNetV3 as the backbone network:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_chinese_lite_train.yml 2&gt;&amp;1 | tee rec_ch_lite.log\n</code></pre> <p>For more details about data preparation and training tutorials, refer to the documentation Text recognition model training/evaluation/prediction</p>"},{"location":"en/ppocr/blog/customize.html#step3-concatenate-predictions","title":"STEP3: CONCATENATE PREDICTIONS","text":"<p>PaddleOCR provides a concatenation tool for detection and recognition models, which can connect any trained detection model and any recognition model into a two-stage text recognition system. The input image goes through four main stages: text detection, text rectification, text recognition, and score filtering to output the text position and recognition results, and at the same time, you can choose to visualize the results.</p> <p>When performing prediction, you need to specify the path of a single image or a image folder through the parameter <code>image_dir</code>, the parameter <code>det_model_dir</code> specifies the path of detection model, and the parameter <code>rec_model_dir</code> specifies the path of recognition model. The visualized results are saved to the <code>./inference_results</code> folder by default.</p> <pre><code>python3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/11.jpg\" --det_model_dir=\"./inference/det/\"  --rec_model_dir=\"./inference/rec/\"\n</code></pre> <p>For more details about text detection and recognition concatenation, please refer to the document Inference</p>"},{"location":"en/ppocr/blog/distributed_training.html","title":"Distributed training","text":""},{"location":"en/ppocr/blog/distributed_training.html#introduction","title":"Introduction","text":"<p>The high performance of distributed training is one of the core advantages of PaddlePaddle. In the classification task, distributed training can achieve almost linear speedup ratio. Generally, OCR training task need massive training data. Such as recognition, PP-OCR v2.0 model is trained based on 1800W dataset, which is very time-consuming if using single machine. Therefore, the distributed training is used in PaddleOCR to speedup the training task. For more information about distributed training, please refer to distributed training quick start tutorial.</p>"},{"location":"en/ppocr/blog/distributed_training.html#quick-start","title":"Quick Start","text":""},{"location":"en/ppocr/blog/distributed_training.html#training-with-single-machine","title":"Training with single machine","text":"<p>Take recognition as an example. After the data is prepared locally, start the training task with the interface of <code>paddle.distributed.launch</code>. The start command as follows:</p> <pre><code>python3 -m paddle.distributed.launch \\\n    --log_dir=./log/ \\\n    --gpus \"0,1,2,3,4,5,6,7\" \\\n    tools/train.py \\\n    -c configs/rec/rec_mv3_none_bilstm_ctc.yml\n</code></pre>"},{"location":"en/ppocr/blog/distributed_training.html#training-with-multi-machine","title":"Training with multi machine","text":"<p>Compared with single machine, training with multi machine only needs to add the parameter <code>--ips</code> to start command, which represents the IP list of machines used for distributed training, and the IP of different machines are separated by commas. The start command as follows:</p> <pre><code>ip_list=\"192.168.0.1,192.168.0.2\"\npython3 -m paddle.distributed.launch \\\n    --log_dir=./log/ \\\n    --ips=\"${ip_list}\" \\\n    --gpus=\"0,1,2,3,4,5,6,7\" \\\n    tools/train.py \\\n    -c configs/rec/rec_mv3_none_bilstm_ctc.yml\n</code></pre> <p>Notice:</p> <ul> <li>The IP addresses of different machines need to be separated by commas, which can be queried through <code>ifconfig</code> or <code>ipconfig</code>.</li> <li>Different machines need to be set to be secret free and can <code>ping</code> success with others directly, otherwise communication cannot establish between them.</li> <li>The code, data and start command between different machines must be completely consistent and then all machines need to run start command. The first machine in the <code>ip_list</code> is set to <code>trainer0</code>, and so on.</li> </ul>"},{"location":"en/ppocr/blog/distributed_training.html#performance-comparison","title":"Performance comparison","text":"<p>We conducted model training on 2x8 P40 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 3x8 GPU training time / Accuracy Acceleration ratio CRNN rec_chinese_lite_train_v2.0.yml 260k Chinese dataset 2.50d/66.70% 1.67d/67.00% 1.5 <p>We conducted model training on 3x8 V100 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 3x8 GPU training time / Accuracy Acceleration ratio SLANet SLANet.yml PubTabNet 49.80h/76.20% 19.75h/74.77% 2.52 <p>Note: when training with 3x8 GPUs, the single card batch size is unchanged compared with the 1x8 GPUs' training process, and the learning rate is multiplied by 2 (if it is multiplied by 3 by default, the accuracy is only 73.42%).</p> <p>We conducted model training on 4x8 V100 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 4x8 GPU training time / Accuracy Acceleration ratio SVTR ch_PP-OCRv3_rec_distillation.yml PP-OCRv3_rec data 10d/- 2.84d/74.00% 3.5"},{"location":"en/ppocr/blog/enhanced_ctc_loss.html","title":"Enhanced CTC Loss","text":"<p>In OCR recognition, CRNN is a text recognition algorithm widely applied in the industry. In the training phase, it uses CTCLoss to calculate the network loss. In the inference phase, it uses CTCDecode to obtain the decoding result. Although the CRNN algorithm has been proven to achieve reliable recognition results in actual business, users have endless requirements for recognition accuracy. So how to improve the accuracy of text recognition? Taking CTCLoss as the starting point, this paper explores the improved fusion scheme of CTCLoss from three different perspectives: Hard Example Mining, Multi-task Learning, and Metric Learning. Based on the exploration, we propose EnhancedCTCLoss, which includes the following 3 components: Focal-CTC Loss, A-CTC Loss, C-CTC Loss.</p>"},{"location":"en/ppocr/blog/enhanced_ctc_loss.html#1-focal-ctc-loss","title":"1. Focal-CTC Loss","text":"<p>Focal Loss was proposed by the paper, \"Focal Loss for Dense Object Detection\". When the loss was first proposed, it was mainly to solve the problem of a serious imbalance in the ratio of positive and negative samples in one-stage target detection. This loss function reduces the weight of a large number of simple negative samples in training and also can be understood as a kind of difficult sample mining. The form of the loss function is as follows:</p> \\[ \\begin{equation} L_{fl}=\\left\\{ \\begin{array}{cl} -\\alpha(1 - y^{'})^{\\gamma}logy^{'} ,&amp;  y = 1 \\\\ -(1 - \\alpha)y^{'\\gamma}log(1 - y^{'}), &amp;  y = 0 \\\\ \\end{array} \\right. \\end{equation} \\] <p>Among them, y' is the output of the activation function, and the value is between 0-1. It adds a modulation factor (1-y\u2019)^\u03b3 and a balance factor \u03b1 on the basis of the original cross-entropy loss. When \u03b1 = 1, y = 1, the comparison between the loss function and the cross-entropy loss is shown in the following figure:</p> <p></p> <p>As can be seen from the above figure, when \u03b3\u00a0&gt; 0, the adjustment coefficient (1-y\u2019)^\u03b3 gives smaller weight to the easy-to-classify sample loss, making the network pay more attention to the difficult and misclassified samples. The adjustment factor \u03b3 is used to adjust the rate at which the weight of simple samples decreases. When \u03b3 = 0, it is the cross-entropy loss function. When \u03b3 increases, the influence of the adjustment factor will also increase. Experiments revealed that 2 is the optimal value of \u03b3. The balance factor \u03b1 is used to balance the uneven proportions of the positive and negative samples. In the text, \u03b1 is taken as 0.25.</p> <p>For the classic CTC algorithm, suppose a certain feature sequence (f<sub>1</sub>, f<sub>2</sub>, ......f<sub>t</sub>), after CTC decoding, the probability that the result is equal to label is y', then the probability that the CTC decoding result is not equal to label is (1-y'); it is not difficult to find that the CTCLoss value and y' have the following relationship:</p> \\[ L_{CTC} = -log(y^{'}) \\] <p>Combining the idea of Focal Loss, assigning larger weights to difficult samples and smaller weights to simple samples can make the network focus more on the mining of difficult samples and further improve the accuracy of recognition. Therefore, we propose Focal-CTC Loss. Its definition is as follows:</p> \\[ L_{Focal\\_CTC} = \\alpha * (1 - y^{'})^{\\gamma} * L_{CTC} \\] <p>In the experiment, the value of \u03b3 is 2, \u03b1\u00a0= 1, see this for specific implementation: rec_ctc_loss.py</p>"},{"location":"en/ppocr/blog/enhanced_ctc_loss.html#2-a-ctc-loss","title":"2. A-CTC Loss","text":"<p>A-CTC Loss is short for CTC Loss + ACE Loss. Among them, ACE Loss was proposed by the paper, \u201cAggregation Cross-Entropy for Sequence Recognition\u201d. Compared with CTCLoss, ACE Loss has the following two advantages: + ACE Loss can solve the recognition problem of 2-D text, while CTCLoss can only process 1-D text + ACE Loss is better than CTC loss in time complexity and space complexity</p> <p>The advantages and disadvantages of the OCR recognition algorithm summarized by the predecessors are shown in the following figure:</p> <p></p> <p>Although ACELoss does handle 2D predictions, as shown in the figure above, and has advantages in memory usage and inference speed, in practice, we found that using ACELoss alone, the recognition effect is not as good as CTCLoss. Consequently, we tried to combine CTCLoss and ACELoss, and CTCLoss is the mainstay while ACELoss acts as an auxiliary supervision loss. This attempt has achieved better results. On our internal experimental data set, compared to using CTCLoss alone, the recognition accuracy can be improved by about 1%. A_CTC Loss is defined as follows:</p> \\[ L_{A-CTC} = L_{CTC} + \\lambda * L_{ACE} \\] <p>In the experiment, \u03bb = 0.1. See the ACE loss implementation code: ace_loss.py</p>"},{"location":"en/ppocr/blog/enhanced_ctc_loss.html#3-c-ctc-loss","title":"3. C-CTC Loss","text":"<p>C-CTC Loss is short for CTC Loss + Center Loss. Among them, Center Loss was proposed by the paper, \u201cA Discriminative Feature Learning Approach for Deep Face Recognition\u201c. It was first used in face recognition tasks to increase the distance between classes and reduce the distance within classes. It is an earlier and also widely used algorithm.</p> <p>In the task of Chinese OCR recognition, through the analysis of bad cases, we found that a major difficulty in Chinese recognition is that there are many similar characters, which are easy to misunderstand. From this, we thought about whether we can learn from the idea of n to increase the class spacing of similar characters, to improve recognition accuracy. However, Metric Learning is mainly used in the field of image recognition, and the label of the training data is a fixed value; for OCR recognition, it is a sequence recognition task essentially, and there is no explicit alignment between features and labels. Therefore, how to combine the two is still a direction worth exploring.</p> <p>By trying Arcmargin, Cosmargin and other methods, we finally found that Centerloss can help further improve the accuracy of recognition. C_CTC Loss is defined as follows:</p> \\[ L_{C-CTC} = L_{CTC} + \\lambda * L_{center} \\] <p>In the experiment, we set \u03bb=0.25. See the center_loss implementation code: center_loss.py</p> <p>It is worth mentioning that in C-CTC Loss, choosing to initialize the Center randomly does not bring significant improvement. Our Center initialization method is as follows: + Based on the original CTCLoss, a network N is obtained by training + Select the training set, identify the completely correct part, and form the set G + Send each sample in G to the network, perform forward calculation, and extract the correspondence between the input of the last FC layer (ie feature) and the result of argmax calculation (ie index) + Aggregate features with the same index, calculate the average, and get the initial center of each character.</p> <p>Taking the configuration file <code>configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml</code> as an example, the center extraction command is as follows:</p> <pre><code>python tools/export_center.py -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml -o Global.pretrained_model=\"./output/rec_mobile_pp-OCRv2/best_accuracy\"\n</code></pre> <p>After running, <code>train_center.pkl</code> will be generated in the main directory of PaddleOCR.</p>"},{"location":"en/ppocr/blog/enhanced_ctc_loss.html#4-experiment","title":"4. Experiment","text":"<p>For the above three solutions, we conducted training and evaluation based on Baidu's internal data set. The experimental conditions are shown in the following table:</p> algorithm Focal_CTC A_CTC C-CTC gain +0.3% +0.7% +1.7% <p>Based on the above experimental conclusions, we adopted the C-CTC strategy in PP-OCRv2. It is worth mentioning that, because PP-OCRv2 deals with the recognition task of 6625 Chinese characters, the character set is relatively large and there are many similar characters, so the C-CTC solution brings a significant improvement on this task. But if you switch to other OCR recognition tasks, the conclusion may be different. You can try Focal-CTC, A-CTC, C-CTC, and the combined solution EnhancedCTC. We believe it will bring different degrees of improvement.</p> <p>The unified combined plan is shown in the following file: rec_enhanced_ctc_loss.py</p>"},{"location":"en/ppocr/blog/inference_args.html","title":"PaddleOCR Model Inference Parameter Explanation","text":"<p>When using PaddleOCR for model inference, you can customize the modification parameters to modify the model, data, preprocessing, postprocessing, etc. (parameter file: utility.py)\uff0cThe detailed parameter explanation is as follows:</p> <ul> <li>Global parameters</li> </ul> parameters type default implication image_dir str None, must be specified explicitly Image or folder path page_num int 0 Valid when the input type is pdf file, specify to predict the previous page_num pages, all pages are predicted by default vis_font_path str \"./doc/fonts/simfang.ttf\" font path for visualization drop_score float 0.5 Results with a recognition score less than this value will be discarded and will not be returned as results use_pdserving bool False Whether to use Paddle Serving for prediction warmup bool False Whether to enable warmup, this method can be used when statistical prediction time draw_img_save_dir str \"./inference_results\" The saving folder of the system's tandem prediction OCR results save_crop_res bool False Whether to save the recognized text image for OCR crop_res_save_dir str \"./output\" Save the text image path recognized by OCR use_mp bool False Whether to enable multi-process prediction total_process_num int 6 The number of processes, which takes effect when <code>use_mp</code> is <code>True</code> process_id int 0 The id number of the current process, no need to modify it yourself benchmark bool False Whether to enable benchmark, and make statistics on prediction speed, memory usage, etc. save_log_path str \"./log_output/\" Folder where log results are saved when <code>benchmark</code> is enabled show_log bool True Whether to show the log information in the inference use_onnx bool False Whether to enable onnx prediction <ul> <li>Prediction engine related parameters</li> </ul> parameters type default implication use_gpu bool True Whether to use GPU for prediction ir_optim bool True Whether to analyze and optimize the calculation graph. The prediction process can be accelerated when <code>ir_optim</code> is enabled use_tensorrt bool False Whether to enable tensorrt min_subgraph_size int 15 The minimum subgraph size in tensorrt. When the size of the subgraph is greater than this value, it will try to use the trt engine to calculate the subgraph. precision str fp32 The precision of prediction, supports <code>fp32</code>, <code>fp16</code>, <code>int8</code> enable_mkldnn bool True Whether to enable mkldnn cpu_threads int 10 When mkldnn is enabled, the number of threads predicted by the cpu <ul> <li>Text detection model related parameters</li> </ul> parameters type default implication det_algorithm str \"DB\" Text detection algorithm name, currently supports <code>DB</code>, <code>EAST</code>, <code>SAST</code>, <code>PSE</code>, <code>DB++</code>, <code>FCE</code> det_model_dir str xx Detection inference model paths det_limit_side_len int 960 image side length limit det_limit_type str \"max\" The side length limit type, currently supports <code>min</code>and <code>max</code>. <code>min</code> means to ensure that the shortest side of the image is not less than <code>det_limit_side_len</code>, <code>max</code> means to ensure that the longest side of the image is not greater than <code>det_limit_side_len</code> <p>The relevant parameters of the DB algorithm are as follows</p> parameters type default implication det_db_thresh float 0.3 In the probability map output by DB, only pixels with a score greater than this threshold will be considered as text pixels det_db_box_thresh float 0.6 Within the detection box, when the average score of all pixels is greater than the threshold, the result will be considered as a text area det_db_unclip_ratio float 1.5 The expansion factor of the <code>Vatti clipping</code> algorithm, which is used to expand the text area max_batch_size int 10 max batch size use_dilation bool False Whether to inflate the segmentation results to obtain better detection results det_db_score_mode str \"fast\" DB detection result score calculation method, supports <code>fast</code> and <code>slow</code>, <code>fast</code> calculates the average score according to all pixels within the bounding rectangle of the polygon, <code>slow</code> calculates the average score according to all pixels within the original polygon, The calculation speed is relatively slower, but more accurate. <p>The relevant parameters of the EAST algorithm are as follows</p> parameters type default implication det_east_score_thresh float 0.8 Threshold for score map in EAST postprocess det_east_cover_thresh float 0.1 Average score threshold for text boxes in EAST postprocess det_east_nms_thresh float 0.2 Threshold of nms in EAST postprocess <p>The relevant parameters of the SAST algorithm are as follows</p> parameters type default implication det_sast_score_thresh float 0.5 Score thresholds in SAST postprocess det_sast_nms_thresh float 0.5 Thresholding of nms in SAST postprocess det_box_type str 'quad' Whether polygon detection, curved text scene (such as Total-Text) is set to 'poly' <p>The relevant parameters of the PSE algorithm are as follows</p> parameters type default implication det_pse_thresh float 0.0 Threshold for binarizing the output image det_pse_box_thresh float 0.85 Threshold for filtering boxes, below this threshold is discarded det_pse_min_area float 16 The minimum area of the box, below this threshold is discarded det_box_type str \"quad\" The type of the returned box, quad: four point coordinates, poly: all point coordinates of the curved text det_pse_scale int 1 The ratio of the input image relative to the post-processed image, such as an image of <code>640*640</code>, the network output is <code>160*160</code>, and when the scale is 2, the shape of the post-processed image is <code>320*320</code>. Increasing this value can speed up the post-processing speed, but it will bring about a decrease in accuracy <ul> <li>Text recognition model related parameters</li> </ul> parameters type default implication rec_algorithm str \"CRNN\" Text recognition algorithm name, currently supports <code>CRNN</code>, <code>SRN</code>, <code>RARE</code>, <code>NETR</code>, <code>SAR</code>, <code>ViTSTR</code>, <code>ABINet</code>, <code>VisionLAN</code>, <code>SPIN</code>, <code>RobustScanner</code>, <code>SVTR</code>, <code>SVTR_LCNet</code> rec_model_dir str None, it is required if using the recognition model recognition inference model paths rec_image_shape str \"3,48,320\" ] Image size at the time of recognition rec_batch_num int 6 batch size max_text_length int 25 The maximum length of the recognition result, valid in <code>SRN</code> rec_char_dict_path str \"./ppocr/utils/ppocr_keys_v1.txt\" character dictionary file use_space_char bool True Whether to include spaces, if <code>True</code>, the <code>space</code> character will be added at the end of the character dictionary <ul> <li>End-to-end text detection and recognition model related parameters</li> </ul> parameters type default implication e2e_algorithm str \"PGNet\" End-to-end algorithm name, currently supports <code>PGNet</code> e2e_model_dir str None, it is required if using the end-to-end model end-to-end model inference model path e2e_limit_side_len int 768 End-to-end input image side length limit e2e_limit_type str \"max\" End-to-end side length limit type, currently supports <code>min</code> and <code>max</code>. <code>min</code> means to ensure that the shortest side of the image is not less than <code>e2e_limit_side_len</code>, <code>max</code> means to ensure that the longest side of the image is not greater than <code>e2e_limit_side_len</code> e2e_pgnet_score_thresh float 0.5 End-to-end score threshold, results below this threshold are discarded e2e_char_dict_path str \"./ppocr/utils/ic15_dict.txt\" Recognition dictionary file path e2e_pgnet_valid_set str \"totaltext\" The name of the validation set, currently supports <code>totaltext</code>, <code>partvgg</code>, the post-processing methods corresponding to different data sets are different, and it can be consistent with the training process e2e_pgnet_mode str \"fast\" PGNet's detection result score calculation method, supports <code>fast</code> and <code>slow</code>, <code>fast</code> calculates the average score according to all pixels within the bounding rectangle of the polygon, <code>slow</code> calculates the average score according to all pixels within the original polygon, The calculation speed is relatively slower, but more accurate. <ul> <li>Angle classifier model related parameters</li> </ul> parameters type default implication use_angle_cls bool False whether to use an angle classifier cls_model_dir str None, if you need to use, you must specify the path explicitly angle classifier inference model path cls_image_shape str \"3,48,192\" prediction shape label_list list ['0', '180'] The angle value corresponding to the class id cls_batch_num int 6 batch size cls_thresh float 0.9 Prediction threshold, when the model prediction result is 180 degrees, and the score is greater than the threshold, the final prediction result is considered to be 180 degrees and needs to be flipped <ul> <li>OCR image preprocessing parameters</li> </ul> parameters type default implication invert bool False whether to invert image before processing binarize bool False whether to threshold binarize image before processing alphacolor tuple \"255,255,255\" Replacement color for the alpha channel, if the latter is present; R,G,B integers"},{"location":"en/ppocr/blog/multi_languages.html","title":"Multi-language model","text":"<p>Recent Update</p> <ul> <li>2022.5.8 update the <code>PP-OCRv3</code> version of the multi-language detection and recognition model, and the average recognition accuracy has increased by more than 5%.</li> <li>2021.4.9 supports the detection and recognition of 80 languages</li> <li>2021.4.9 supports lightweight high-precision English model detection and recognition</li> </ul> <p>PaddleOCR aims to create a rich, leading, and practical OCR tool library, which not only provides Chinese and English models in general scenarios, but also provides models specifically trained in English scenarios. And multilingual models covering 80 languages.</p> <p>Among them, the English model supports the detection and recognition of uppercase and lowercase letters and common punctuation, and the recognition of space characters is optimized:</p> <p></p> <p>The multilingual models cover Latin, Arabic, Traditional Chinese, Korean, Japanese, etc.:</p> <p></p> <p></p> <p></p> <p></p> <p>This document will briefly introduce how to use the multilingual model.</p>"},{"location":"en/ppocr/blog/multi_languages.html#1-installation","title":"1 Installation","text":""},{"location":"en/ppocr/blog/multi_languages.html#11-paddle-installation","title":"1.1 Paddle installation","text":"<pre><code># cpu\npip install paddlepaddle\n\n# gpu\npip install paddlepaddle-gpu\n</code></pre>"},{"location":"en/ppocr/blog/multi_languages.html#12-paddleocr-package-installation","title":"1.2 PaddleOCR package installation","text":"<pre><code>pip install paddleocr\n</code></pre> <p>Build and install locally</p> <pre><code>python3 -m build\npip3 install dist/paddleocr-x.x.x-py3-none-any.whl # x.x.x is the version number of paddleocr\n</code></pre>"},{"location":"en/ppocr/blog/multi_languages.html#2-quick-use","title":"2 Quick use","text":""},{"location":"en/ppocr/blog/multi_languages.html#21-command-line-operation","title":"2.1 Command line operation","text":"<p>View help information</p> <pre><code>paddleocr -h\n</code></pre> <ul> <li>Whole image prediction (detection + recognition)</li> </ul> <p>PaddleOCR currently supports 80 languages, which can be specified by the --lang parameter. The supported languages are listed in the table.</p> <pre><code>paddleocr --image_dir doc/imgs_en/254.jpg --lang=en\n</code></pre> <p></p> <p></p> <p>The result is a list. Each item contains a text box, text and recognition confidence</p> <pre><code>[('PHO CAPITAL', 0.95723116), [[66.0, 50.0], [327.0, 44.0], [327.0, 76.0], [67.0, 82.0]]]\n[('107 State Street', 0.96311164), [[72.0, 90.0], [451.0, 84.0], [452.0, 116.0], [73.0, 121.0]]]\n[('Montpelier Vermont', 0.97389287), [[69.0, 132.0], [501.0, 126.0], [501.0, 158.0], [70.0, 164.0]]]\n[('8022256183', 0.99810505), [[71.0, 175.0], [363.0, 170.0], [364.0, 202.0], [72.0, 207.0]]]\n[('REG 07-24-201706:59 PM', 0.93537045), [[73.0, 299.0], [653.0, 281.0], [654.0, 318.0], [74.0, 336.0]]]\n[('045555', 0.99346405), [[509.0, 331.0], [651.0, 325.0], [652.0, 356.0], [511.0, 362.0]]]\n[('CT1', 0.9988654), [[535.0, 367.0], [654.0, 367.0], [654.0, 406.0], [535.0, 406.0]]]\n......\n</code></pre> <ul> <li>Recognition</li> </ul> <pre><code>paddleocr --image_dir doc/imgs_words_en/word_308.png --det false --lang=en\n</code></pre> <p></p> <p>The result is a 2-tuple, which contains the recognition result and recognition confidence</p> <pre><code>(0.99879867, 'LITTLE')\n</code></pre> <ul> <li>Detection</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs/11.jpg --rec false\n</code></pre> <p>The result is a list. Each item represents the coordinates of a text box.</p> <pre><code>[[26.0, 457.0], [137.0, 457.0], [137.0, 477.0], [26.0, 477.0]]\n[[25.0, 425.0], [372.0, 425.0], [372.0, 448.0], [25.0, 448.0]]\n[[128.0, 397.0], [273.0, 397.0], [273.0, 414.0], [128.0, 414.0]]\n......\n</code></pre>"},{"location":"en/ppocr/blog/multi_languages.html#22-run-with-python-script","title":"2.2 Run with Python script","text":"<p>PPOCR is able to run with Python scripts for easy integration with your own code:</p> <ul> <li>Whole image prediction (detection + recognition)</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Also switch the language by modifying the lang parameter\nocr = PaddleOCR(lang=\"korean\") # The model file will be downloaded automatically when executed for the first time\nimg_path ='doc/imgs/korean_1.jpg'\nresult = ocr.ocr(img_path)\n# Recognition and detection can be performed separately through parameter control\n# result = ocr.ocr(img_path, det=False)  Only perform recognition\n# result = ocr.ocr(img_path, rec=False)  Only perform detection\n# Print detection frame and recognition result\nfor line in result:\n    print(line)\n\n# Visualization\nfrom PIL import Image\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/korean.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Visualization of results:</p> <p></p> <p>PPOCR also supports direction classification. For more detailed usage, please refer to: whl package instructions.</p>"},{"location":"en/ppocr/blog/multi_languages.html#3-custom-training","title":"3 Custom training","text":"<p>PPOCR supports using your own data for custom training or fine-tune, where the recognition model can refer to French configuration file Modify the training data path, dictionary and other parameters.</p> <p>For specific data preparation and training process, please refer to: Text Detection, Text Recognition, more functions such as predictive deployment, For functions such as data annotation, you can read the complete Document Tutorial.</p>"},{"location":"en/ppocr/blog/multi_languages.html#4-inference-and-deployment","title":"4 Inference and Deployment","text":"<p>In addition to installing the whl package for quick forecasting, PPOCR also provides a variety of forecasting deployment methods. If necessary, you can read related documents:</p> <ul> <li>Python Inference</li> <li>C++ Inference</li> <li>Serving</li> <li>Mobile</li> <li>Benchmark</li> </ul>"},{"location":"en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations","title":"5 Support languages and abbreviations","text":"Language Abbreviation Language Abbreviation Chinese &amp; English ch Arabic ar English en Hindi hi French fr Uyghur ug German german Persian fa Japan japan Urdu ur Korean korean Serbian(latin) rs_latin Chinese Traditional chinese_cht Occitan oc Italian it Marathi mr Spanish es Nepali ne Portuguese pt Serbian(cyrillic) rs_cyrillic Russia ru Bulgarian bg Ukranian uk Estonian et Belarusian be Irish ga Telugu te Croatian hr Saudi Arabia sa Hungarian hu Tamil ta Indonesian id Afrikaans af Icelandic is Azerbaijani az Kurdish ku Bosnian bs Lithuanian lt Czech cs Latvian lv Welsh cy Maori mi Danish da Malay ms Maltese mt Adyghe ady Dutch nl Kabardian kbd Norwegian no Avar ava Polish pl Dargwa dar Romanian ro Ingush inh Slovak sk Lak lbe Slovenian sl Lezghian lez Albanian sq Tabassaran tab Swedish sv Bihari bh Swahili sw Maithili mai Tagalog tl Angika ang Turkish tr Bhojpuri bho Uzbek uz Magahi mah Vietnamese vi Nagpur sck Mongolian mn Newari new Abaza abq Goan Konkani gom"},{"location":"en/ppocr/blog/ocr_book.html","title":"E-book: Dive Into OCR","text":"<p>\"Dive Into OCR\" is a textbook that combines OCR theory and practice, written by the PaddleOCR community. The main features are as follows:</p> <ul> <li>OCR full-stack technology covering text detection, recognition and document analysis</li> <li>Closely integrate theory and practice, cross the code implementation gap, and supporting instructional videos</li> <li>Jupyter Notebook textbook, flexibly modifying code for instant results</li> </ul>"},{"location":"en/ppocr/blog/ocr_book.html#structure","title":"Structure","text":"<ul> <li> <p>The first part is the preliminary knowledge of the book, including the knowledge index and resource links needed in the process of positioning and using the book content of the book</p> </li> <li> <p>The second part is chapters 4-8 of the book, which introduce the concepts, applications, and industry practices related to the detection and identification capabilities of the OCR engine. In the \"Introduction to OCR Technology\", the application scenarios and challenges of OCR, the basic concepts of technology, and the pain points in industrial applications are comprehensively explained. Then, in the two chapters of \"Text Detection\" and \"Text Recognition\", the two basic tasks of OCR are introduced. In each chapter, an algorithm is accompanied by a detailed explanation of the code and practical exercises. Chapters 6 and 7 are a detailed introduction to the PP-OCR series model, PP-OCR is a set of OCR systems for industrial applications, on the basis of the basic detection and identification model, after a series of optimization strategies to achieve the general field of industrial SOTA model, while opening up a variety of predictive deployment solutions, enabling enterprises to quickly land OCR applications.</p> </li> <li> <p>The third part is chapter 9-12 of the book, which introduces applications other than the two-stage OCR engine, including data synthesis, preprocessing algorithm, and end-to-end model, focusing on OCR's layout analysis, table recognition, visual document question and answer capabilities in the document scene, and also through the combination of algorithm and code, so that readers can deeply understand and apply.</p> </li> </ul>"},{"location":"en/ppocr/blog/ocr_book.html#address","title":"Address","text":"<ul> <li>E-book: Dive Into OCR (PDF)</li> <li>Notebook (.ipynb)</li> <li>Videos (Chinese only)</li> </ul>"},{"location":"en/ppocr/blog/slice.html","title":"Slice Operator","text":"<p>If you have a very large image/document that you would like to run PaddleOCR (detection and recognition) on, you can use the slice operation as follows:</p> <p><code>ocr_inst  =  PaddleOCR(**ocr_settings)</code> <code>results  =  ocr_inst.ocr(img, det=True,rec=True, slice=slice, cls=False,bin=False,inv=False,alpha_color=False)</code></p> <p>where <code>slice  = {'horizontal_stride': h_stride, 'vertical_stride':v_stride, 'merge_x_thres':x_thres, 'merge_y_thres': y_thres}</code></p> <p>Here, <code>h_stride</code>, <code>v_stride</code>, <code>x_thres</code>, and <code>y_thres</code> are user-configurable values and need to be set manually. The way the <code>slice</code> operator works is that it runs a sliding window across the large input image, creating slices of it and runs the OCR algorithms on it.</p> <p>The fragmented slice-level results are then merged together to output image-level detection and recognition results. The horizontal and vertical strides cannot be lower than a certain limit (as too low values would create so many slices it would be very computationally expensive to get results for each of them). However, as an example the recommended values for an image with dimensions 6616x14886 would be as follows.</p> <p><code>slice = {'horizontal_stride': 300, 'vertical_stride':500, 'merge_x_thres':50, 'merge_y_thres': 35}</code></p> <p>All slice-level detections with bounding boxes as close as <code>merge_x_thres</code> and <code>merge_y_thres</code> will be merged together.</p>"},{"location":"en/ppocr/blog/whl.html","title":"Paddleocr Package","text":""},{"location":"en/ppocr/blog/whl.html#1-get-started-quickly","title":"1 Get started quickly","text":""},{"location":"en/ppocr/blog/whl.html#11-install-package","title":"1.1 install package","text":"<p>install by pypi</p> <pre><code>pip install \"paddleocr&gt;=2.0.1\" # Recommend to use version 2.0.1+\n</code></pre> <p>build own whl package and install</p> <pre><code>python3 -m build\npip3 install dist/paddleocr-x.x.x-py3-none-any.whl # x.x.x is the version of paddleocr\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#2-use","title":"2 Use","text":""},{"location":"en/ppocr/blog/whl.html#21-use-by-code","title":"2.1 Use by code","text":"<p>The paddleocr whl package will automatically download the ppocr lightweight model as the default model, which can be customized and replaced according to the section 3 Custom Model.</p> <ul> <li>detection angle classification and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[442.0, 173.0], [1169.0, 173.0], [1169.0, 225.0], [442.0, 225.0]], ['ACKNOWLEDGEMENTS', 0.99283075]]\n[[[393.0, 340.0], [1207.0, 342.0], [1207.0, 389.0], [393.0, 387.0]], ['We would like to thank all the designers and', 0.9357758]]\n[[[399.0, 398.0], [1204.0, 398.0], [1204.0, 433.0], [399.0, 433.0]], ['contributors whohave been involved in the', 0.9592447]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>detection and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\nocr = PaddleOCR(lang='en') # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[442.0, 173.0], [1169.0, 173.0], [1169.0, 225.0], [442.0, 225.0]], ['ACKNOWLEDGEMENTS', 0.99283075]]\n[[[393.0, 340.0], [1207.0, 342.0], [1207.0, 389.0], [393.0, 387.0]], ['We would like to thank all the designers and', 0.9357758]]\n[[[399.0, 398.0], [1204.0, 398.0], [1204.0, 433.0], [399.0, 433.0]], ['contributors whohave been involved in the', 0.9592447]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>classification and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains recognition text and confidence</p> <pre><code>['PAIN', 0.990372]\n</code></pre> <ul> <li>only detection</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\nocr = PaddleOCR() # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path,rec=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_ocr(image, result, txts=None, scores=None, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[756.0, 812.0], [805.0, 812.0], [805.0, 830.0], [756.0, 830.0]]\n[[820.0, 803.0], [1085.0, 801.0], [1085.0, 836.0], [820.0, 838.0]]\n[[393.0, 801.0], [715.0, 805.0], [715.0, 839.0], [393.0, 836.0]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>only recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(lang='en') # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, cls=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains recognition text and confidence</p> <pre><code>['PAIN', 0.990372]\n</code></pre> <ul> <li>only classification</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(use_angle_cls=True) # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, rec=False, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains classification result and confidence</p> <pre><code>['0', 0.99999964]\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#22-use-by-command-line","title":"2.2 Use by command line","text":"<p>show help information</p> <pre><code>paddleocr -h\n</code></pre> <ul> <li>detection classification and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --use_angle_cls true --lang en\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <p>pdf file is also supported, you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>detection and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --lang en\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <ul> <li>classification and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --use_angle_cls true --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <ul> <li>only detection</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --rec false\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[397.0, 802.0], [1092.0, 802.0], [1092.0, 841.0], [397.0, 841.0]]\n[[397.0, 750.0], [1211.0, 750.0], [1211.0, 789.0], [397.0, 789.0]]\n[[397.0, 702.0], [1209.0, 698.0], [1209.0, 734.0], [397.0, 738.0]]\n......\n</code></pre> <ul> <li>only recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <ul> <li>only classification</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --use_angle_cls true --det false --rec false\n</code></pre> <p>Output will be a list, each item contains classification result and confidence</p> <pre><code>['0', 0.99999964]\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#3-use-custom-model","title":"3 Use custom model","text":"<p>When the built-in model cannot meet the needs, you need to use your own trained model. First, refer to export doc to convert your det and rec model to inference model, and then use it as follows</p>"},{"location":"en/ppocr/blog/whl.html#31-use-by-code","title":"3.1 Use by code","text":"<pre><code>from paddleocr import PaddleOCR,draw_ocr\n# The path of detection and recognition model must contain model and params files\nocr = PaddleOCR(det_model_dir='{your_det_model_dir}', rec_model_dir='{your_rec_model_dir}', rec_char_dict_path='{your_rec_char_dict_path}', cls_model_dir='{your_cls_model_dir}', use_angle_cls=True)\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#32-use-by-command-line","title":"3.2 Use by command line","text":"<pre><code>paddleocr --image_dir PaddleOCR/doc/imgs/11.jpg --det_model_dir {your_det_model_dir} --rec_model_dir {your_rec_model_dir} --rec_char_dict_path {your_rec_char_dict_path} --cls_model_dir {your_cls_model_dir} --use_angle_cls true\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#4-use-web-images-or-numpy-array-as-input","title":"4 Use web images or numpy array as input","text":""},{"location":"en/ppocr/blog/whl.html#41-web-image","title":"4.1 Web image","text":"<ul> <li>Use by code</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\") # need to run only once to download and load model into memory\nimg_path = 'http://n.sinaimg.cn/ent/transform/w630h933/20171222/o111-fypvuqf1838418.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# show result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <ul> <li>Use by command line</li> </ul> <pre><code>paddleocr --image_dir http://n.sinaimg.cn/ent/transform/w630h933/20171222/o111-fypvuqf1838418.jpg --use_angle_cls=true\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#42-numpy-array","title":"4.2 Numpy array","text":"<p>Support numpy array as input only when used by code</p> <pre><code>import cv2\nfrom paddleocr import PaddleOCR, draw_ocr, download_with_progressbar\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\") # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs/11.jpg'\nimg = cv2.imread(img_path)\n# img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY), If your own training model supports grayscale images, you can uncomment this line\nresult = ocr.ocr(img, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# show result\nfrom PIL import Image\nresult = result[0]\ndownload_with_progressbar(img_path, 'tmp.jpg')\nimage = Image.open('tmp.jpg').convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#5-pdf-file","title":"5 PDF file","text":"<ul> <li>Use by command line</li> </ul> <p>you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>Use by code</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\"\uff0c page_num=2)  # need to run only once to download and load model into memory\nimg_path = './xxx.pdf'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nimport fitz\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimgs = []\nwith fitz.open(img_path) as pdf:\n    for pg in range(0, pdf.pageCount):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.getPixmap(matrix=mat, alpha=False)\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.getPixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\nfor idx in range(len(result)):\n    res = result[idx]\n    image = imgs[idx]\n    boxes = [line[0] for line in res]\n    txts = [line[1][0] for line in res]\n    scores = [line[1][1] for line in res]\n    im_show = draw_ocr(image, boxes, txts, scores, font_path='doc/fonts/simfang.ttf')\n    im_show = Image.fromarray(im_show)\n    im_show.save('result_page_{}.jpg'.format(idx))\n</code></pre>"},{"location":"en/ppocr/blog/whl.html#6-parameter-description","title":"6 Parameter Description","text":"Parameter Description Default value use_gpu use GPU or not TRUE gpu_mem GPU memory size used for initialization 8000M image_dir The images path or folder path for predicting when used by the command line page_num Valid when the input type is pdf file, specify to predict the previous page_num pages, all pages are predicted by default 0 det_algorithm Type of detection algorithm selected DB det_model_dir the text detection inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/det</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None det_max_side_len The maximum size of the long side of the image. When the long side exceeds this value, the long side will be resized to this size, and the short side will be scaled proportionally 960 det_db_thresh Binarization threshold value of DB output map 0.3 det_db_box_thresh The threshold value of the DB output box. Boxes score lower than this value will be discarded 0.5 det_db_unclip_ratio The expanded ratio of DB output box 2 det_db_score_mode The parameter that control how the score of the detection frame is calculated. There are 'fast' and 'slow' options. If the text to be detected is curved, it is recommended to use 'slow' 'fast' det_east_score_thresh Binarization threshold value of EAST output map 0.8 det_east_cover_thresh The threshold value of the EAST output box. Boxes score lower than this value will be discarded 0.1 det_east_nms_thresh The NMS threshold value of EAST model output box 0.2 rec_algorithm Type of recognition algorithm selected CRNN rec_model_dir the text recognition inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/rec</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None rec_image_shape image shape of recognition algorithm \"3,32,320\" rec_batch_num When performing recognition, the batchsize of forward images 30 max_text_length The maximum text length that the recognition algorithm can recognize 25 rec_char_dict_path the alphabet path which needs to be modified to your own path when <code>rec_model_Name</code> use mode 2 ./ppocr/utils/ppocr_keys_v1.txt use_space_char Whether to recognize spaces TRUE drop_score Filter the output by score (from the recognition model), and those below this score will not be returned 0.5 use_angle_cls Whether to load classification model FALSE cls_model_dir the classification inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/cls</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None cls_image_shape image shape of classification algorithm \"3,48,192\" label_list label list of classification algorithm ['0','180'] cls_batch_num When performing classification, the batchsize of forward images 30 enable_mkldnn Whether to enable mkldnn FALSE use_zero_copy_run Whether to forward by zero_copy_run FALSE lang The support language, now only Chinese(ch)\u3001English(en)\u3001French(french)\u3001German(german)\u3001Korean(korean)\u3001Japanese(japan) are supported ch det Enable detction when <code>ppocr.ocr</code> func exec TRUE rec Enable recognition when <code>ppocr.ocr</code> func exec TRUE cls Enable classification when <code>ppocr.ocr</code> func exec((Use use_angle_cls in command line mode to control whether to start classification in the forward direction) FALSE show_log Whether to print log FALSE type Perform ocr or table structuring, the value is selected in ['ocr','structure'] ocr ocr_version OCR Model version number, the current model support list is as follows: PP-OCRv3 supports Chinese and English detection, recognition, multilingual recognition, direction classifier models, PP-OCRv2 support Chinese detection and recognition model, PP-OCR support Chinese detection, recognition and direction classifier, multilingual recognition model PP-OCRv3"},{"location":"en/ppocr/infer_deploy/index.html","title":"PP-OCR Deployment","text":""},{"location":"en/ppocr/infer_deploy/index.html#paddle-deployment-introduction","title":"Paddle Deployment Introduction","text":"<p>Paddle provides a variety of deployment schemes to meet the deployment requirements of different scenarios. Please choose according to the actual situation:</p> <p></p> <p>PP-OCR has supported muti deployment schemes. Click the link to get the specific tutorial.</p> <ul> <li>Python Inference</li> <li>C++ Inference</li> <li>Serving (Python/C++)</li> <li>Paddle-Lite (ARM CPU/OpenCL ARM GPU)</li> <li>Paddle.js</li> <li>Jetson Inference</li> <li>Paddle2ONNX</li> </ul> <p>If you need the deployment tutorial of academic algorithm models other than PP-OCR, please directly enter the main page of corresponding algorithms, entrance\u3002</p>"},{"location":"en/ppocr/infer_deploy/Jetson_infer.html","title":"Jetson Deployment for PaddleOCR","text":"<p>This section introduces the deployment of PaddleOCR on Jetson NX, TX2, nano, AGX and other series of hardware.</p>"},{"location":"en/ppocr/infer_deploy/Jetson_infer.html#1-prepare-environment","title":"1. Prepare Environment","text":"<p>You need to prepare a Jetson development hardware. If you need TensorRT, you need to prepare the TensorRT environment. It is recommended to use TensorRT version 7.1.3;</p>"},{"location":"en/ppocr/infer_deploy/Jetson_infer.html#1-install-paddlepaddle-in-jetson","title":"1. Install PaddlePaddle in Jetson","text":"<p>The PaddlePaddle download link Please select the appropriate installation package for your Jetpack version, cuda version, and trt version. Here, we download paddlepaddle_gpu-2.3.0rc0-cp36-cp36m-linux_aarch64.whl.</p> <p>Install PaddlePaddle\uff1a</p> <pre><code>pip3 install -U paddlepaddle_gpu-2.3.0rc0-cp36-cp36m-linux_aarch64.whl\n</code></pre>"},{"location":"en/ppocr/infer_deploy/Jetson_infer.html#2-download-paddleocr-code-and-install-dependencies","title":"2. Download PaddleOCR code and install dependencies","text":"<p>Clone the PaddleOCR code:</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR\n</code></pre> <p>and install dependencies\uff1a</p> <pre><code>cd PaddleOCR\npip3 install -r requirements.txt\n</code></pre> <ul> <li>Note: Jetson hardware CPU is poor, dependency installation is slow, please wait patiently</li> </ul>"},{"location":"en/ppocr/infer_deploy/Jetson_infer.html#2-perform-prediction","title":"2. Perform prediction","text":"<p>Obtain the PPOCR model from the document model library. The following takes the PP-OCRv3 model as an example to introduce the use of the PPOCR model on Jetson:</p> <p>Download and unzip the PP-OCRv3 models.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\ntar xf ch_PP-OCRv3_det_infer.tar\ntar xf ch_PP-OCRv3_rec_infer.tar\n</code></pre> <p>The text detection inference:</p> <pre><code>cd PaddleOCR\npython3 tools/infer/predict_det.py --det_model_dir=./inference/ch_PP-OCRv2_det_infer/  --image_dir=./doc/imgs/french_0.jpg  --use_gpu=True\n</code></pre> <p>After executing the command, the predicted information will be printed out in the terminal, and the visualization results will be saved in the <code>./inference_results/</code> directory.</p> <p></p> <p>The text recognition inference:</p> <pre><code>python3 tools/infer/predict_det.py --rec_model_dir=./inference/ch_PP-OCRv2_rec_infer/  --image_dir=./doc/imgs_words/en/word_2.png  --use_gpu=True --rec_image_shape=\"3,48,320\"\n</code></pre> <p>After executing the command, the predicted information will be printed on the terminal, and the output is as follows:</p> <pre><code>[2022/04/28 15:41:45] root INFO: Predicts of ./doc/imgs_words/en/word_2.png:('yourself', 0.98084533)\n</code></pre> <p>The text  detection and text recognition inference:</p> <pre><code>python3 tools/infer/predict_system.py --det_model_dir=./inference/ch_PP-OCRv2_det_infer/ --rec_model_dir=./inference/ch_PP-OCRv2_rec_infer/ --image_dir=./doc/imgs/00057937.jpg --use_gpu=True --rec_image_shape=\"3,48,320\"\n</code></pre> <p>After executing the command, the predicted information will be printed out in the terminal, and the visualization results will be saved in the <code>./inference_results/</code> directory.</p> <p></p> <p>To enable TRT prediction, you only need to set <code>--use_tensorrt=True</code> on the basis of the above command:</p> <pre><code>python3 tools/infer/predict_system.py --det_model_dir=./inference/ch_PP-OCRv2_det_infer/ --rec_model_dir=./inference/ch_PP-OCRv2_rec_infer/ --image_dir=./doc/imgs/  --rec_image_shape=\"3,48,320\" --use_gpu=True --use_tensorrt=True\n</code></pre> <p>For more ppocr model predictions, please refer todocument</p>"},{"location":"en/ppocr/infer_deploy/android_demo.html","title":"Android Demo","text":""},{"location":"en/ppocr/infer_deploy/android_demo.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>\u200b\u6b64\u200b\u4e3a\u200bPaddleOCR\u200b\u7684\u200bAndroid Demo\uff0c\u200b\u76ee\u524d\u200b\u652f\u6301\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u6587\u672c\u200b\u65b9\u5411\u200b\u5206\u7c7b\u5668\u200b\u548c\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u4f7f\u7528\u200b\u3002\u200b\u4f7f\u7528\u200b PaddleLite v2.10 \u200b\u8fdb\u884c\u200b\u5f00\u53d1\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/android_demo.html#2","title":"2. \u200b\u8fd1\u671f\u200b\u66f4\u65b0","text":"<ul> <li>2022.02.27<ul> <li>\u200b\u9884\u6d4b\u200b\u5e93\u200b\u66f4\u65b0\u200b\u5230\u200bPaddleLite v2.10</li> <li>\u200b\u652f\u6301\u200b6\u200b\u79cd\u200b\u8fd0\u884c\u200b\u6a21\u5f0f\u200b\uff1a<ul> <li>\u200b\u68c0\u6d4b\u200b+\u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u68c0\u6d4b\u200b+\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u68c0\u6d4b\u200b</li> <li>\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u5206\u7c7b\u200b</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/ppocr/infer_deploy/android_demo.html#3","title":"3. \u200b\u5feb\u901f\u200b\u4f7f\u7528","text":""},{"location":"en/ppocr/infer_deploy/android_demo.html#31","title":"3.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<ol> <li>\u200b\u5728\u200b\u672c\u5730\u200b\u73af\u5883\u200b\u5b89\u88c5\u200b\u597d\u200b Android Studio \u200b\u5de5\u5177\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u5b89\u88c5\u200b\u65b9\u6cd5\u200b\u8bf7\u200b\u89c1\u200bAndroid Stuido \u200b\u5b98\u7f51\u200b\u3002</li> <li>\u200b\u51c6\u5907\u200b\u4e00\u90e8\u200b Android \u200b\u624b\u673a\u200b\uff0c\u200b\u5e76\u200b\u5f00\u542f\u200b USB \u200b\u8c03\u8bd5\u6a21\u5f0f\u200b\u3002\u200b\u5f00\u542f\u200b\u65b9\u6cd5\u200b: <code>\u200b\u624b\u673a\u200b\u8bbe\u7f6e\u200b -&gt; \u200b\u67e5\u627e\u200b\u5f00\u53d1\u8005\u200b\u9009\u9879\u200b -&gt; \u200b\u6253\u5f00\u200b\u5f00\u53d1\u8005\u200b\u9009\u9879\u200b\u548c\u200b USB \u200b\u8c03\u8bd5\u6a21\u5f0f\u200b</code></li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b Android Studio \u200b\u5c1a\u672a\u200b\u914d\u7f6e\u200b NDK \uff0c\u200b\u8bf7\u200b\u6839\u636e\u200b Android Studio \u200b\u7528\u6237\u200b\u6307\u5357\u200b\u4e2d\u200b\u7684\u200b\u5b89\u88c5\u200b\u53ca\u200b\u914d\u7f6e\u200b NDK \u200b\u548c\u200b CMake\u200b\u5185\u5bb9\u200b\uff0c\u200b\u9884\u5148\u200b\u914d\u7f6e\u200b\u597d\u200b NDK \u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u6700\u65b0\u200b\u7684\u200b NDK \u200b\u7248\u672c\u200b\uff0c\u200b\u6216\u8005\u200b\u4f7f\u7528\u200b Paddle Lite \u200b\u9884\u6d4b\u200b\u5e93\u200b\u7248\u672c\u200b\u4e00\u6837\u200b\u7684\u200b NDK</p>"},{"location":"en/ppocr/infer_deploy/android_demo.html#32","title":"3.2 \u200b\u5bfc\u5165\u200b\u9879\u76ee","text":"<p>\u200b\u70b9\u51fb\u200b File-&gt;New-&gt;Import Project...\uff0c \u200b\u7136\u540e\u200b\u8ddf\u7740\u200bAndroid Studio\u200b\u7684\u200b\u5f15\u5bfc\u200b\u5bfc\u5165\u200b \u200b\u5bfc\u5165\u200b\u5b8c\u6210\u200b\u540e\u200b\u5448\u73b0\u200b\u5982\u4e0b\u200b\u754c\u9762\u200b </p>"},{"location":"en/ppocr/infer_deploy/android_demo.html#33-demo","title":"3.3 \u200b\u8fd0\u884c\u200bdemo","text":"<p>\u200b\u5c06\u200b\u624b\u673a\u200b\u8fde\u63a5\u200b\u4e0a\u200b\u7535\u8111\u200b\u540e\u200b\uff0c\u200b\u70b9\u51fb\u200bAndroid Studio\u200b\u5de5\u5177\u680f\u200b\u4e2d\u200b\u7684\u200b\u8fd0\u884c\u200b\u6309\u94ae\u200b\u5373\u53ef\u200b\u8fd0\u884c\u200bdemo\u3002\u200b\u5728\u200b\u6b64\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u624b\u673a\u200b\u4f1a\u5f39\u200b\u51fa\u200b\"\u200b\u5141\u8bb8\u200b\u4ece\u200b USB \u200b\u5b89\u88c5\u200b\u8f6f\u4ef6\u200b\u6743\u9650\u200b\"\u200b\u7684\u200b\u5f39\u7a97\u200b\uff0c\u200b\u70b9\u51fb\u200b\u5141\u8bb8\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u8f6f\u4ef6\u200b\u5b89\u8f6c\u200b\u5230\u200b\u624b\u673a\u200b\u4e0a\u540e\u200b\u4f1a\u200b\u5728\u200b\u624b\u673a\u200b\u4e3b\u5c4f\u200b\u6700\u540e\u200b\u4e00\u9875\u200b\u770b\u5230\u200b\u5982\u4e0b\u200bapp</p> <p></p> <p>\u200b\u70b9\u51fb\u200bapp\u200b\u56fe\u6807\u200b\u5373\u53ef\u200b\u542f\u52a8\u200bapp\uff0c\u200b\u542f\u52a8\u200b\u540e\u200bapp\u200b\u4e3b\u9875\u200b\u5982\u4e0b\u200b</p> <p></p> <p>app\u200b\u4e3b\u9875\u200b\u4e2d\u6709\u200b\u56db\u4e2a\u200b\u6309\u94ae\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u4e0b\u200b\u62c9\u200b\u5217\u8868\u200b\u548c\u200b\u4e00\u4e2a\u200b\u83dc\u5355\u200b\u6309\u94ae\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u7684\u200b\u529f\u80fd\u200b\u5206\u522b\u200b\u4e3a\u200b</p> <ul> <li>\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\uff1a\u200b\u6309\u7167\u200b\u5df2\u200b\u9009\u62e9\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8fd0\u884c\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6a21\u578b\u200b\u7ec4\u5408\u200b</li> <li>\u200b\u62cd\u7167\u200b\u8bc6\u522b\u200b\uff1a\u200b\u5524\u8d77\u200b\u624b\u673a\u200b\u76f8\u673a\u200b\u62cd\u7167\u200b\u5e76\u200b\u83b7\u53d6\u200b\u62cd\u7167\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u62cd\u7167\u200b\u5b8c\u6210\u200b\u540e\u200b\u9700\u8981\u200b\u70b9\u51fb\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u9009\u53d6\u200b\u56fe\u7247\u200b\uff1a\u200b\u5524\u8d77\u200b\u624b\u673a\u200b\u76f8\u518c\u200b\u62cd\u7167\u200b\u9009\u62e9\u200b\u56fe\u50cf\u200b\uff0c\u200b\u9009\u62e9\u200b\u5b8c\u6210\u200b\u540e\u200b\u9700\u8981\u200b\u70b9\u51fb\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b</li> <li>\u200b\u6e05\u7a7a\u200b\u7ed8\u56fe\u200b\uff1a\u200b\u6e05\u7a7a\u200b\u5f53\u524d\u200b\u663e\u793a\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u7ed8\u5236\u200b\u7684\u200b\u6587\u672c\u6846\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u8fdb\u884c\u200b\u4e0b\u200b\u4e00\u6b21\u200b\u8bc6\u522b\u200b(\u200b\u6bcf\u6b21\u200b\u8bc6\u522b\u200b\u4f7f\u7528\u200b\u7684\u200b\u56fe\u50cf\u200b\u90fd\u200b\u662f\u200b\u5f53\u524d\u200b\u663e\u793a\u200b\u7684\u200b\u56fe\u50cf\u200b)</li> <li>\u200b\u4e0b\u200b\u62c9\u200b\u5217\u8868\u200b\uff1a\u200b\u8fdb\u884c\u200b\u8fd0\u884c\u200b\u6a21\u5f0f\u200b\u7684\u200b\u9009\u62e9\u200b\uff0c\u200b\u76ee\u524d\u200b\u5305\u542b\u200b6\u200b\u79cd\u200b\u8fd0\u884c\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u6a21\u5f0f\u200b\u4e3a\u200b\u68c0\u6d4b\u200b+\u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u200b\u8be6\u7ec6\u200b\u8bf4\u660e\u200b\u89c1\u200b\u4e0b\u200b\u4e00\u8282\u200b\u3002</li> <li>\u200b\u83dc\u5355\u200b\u6309\u94ae\u200b\uff1a\u200b\u70b9\u51fb\u200b\u540e\u200b\u4f1a\u200b\u8fdb\u5165\u200b\u83dc\u5355\u200b\u754c\u9762\u200b\uff0c\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u548c\u200b\u5185\u7f6e\u200b\u56fe\u50cf\u200b\u6709\u5173\u200b\u8bbe\u7f6e\u200b</li> </ul> <p>\u200b\u70b9\u51fb\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u4f1a\u200b\u6309\u7167\u200b\u6240\u200b\u9009\u62e9\u200b\u7684\u200b\u6a21\u5f0f\u200b\u8fd0\u884c\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u68c0\u6d4b\u200b+\u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u6a21\u5f0f\u200b\u4e0b\u200b\u8fd0\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6a21\u578b\u200b\u548c\u200b\u8fd0\u884c\u200b\u72b6\u6001\u200b\u663e\u793a\u200b\u533a\u200b<code>STATUS</code>\u200b\u5b57\u200b\u6bb5\u200b\u663e\u793a\u200b\u4e86\u200b\u5f53\u524d\u200b\u6a21\u578b\u200b\u7684\u200b\u8fd0\u884c\u200b\u72b6\u6001\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u663e\u793a\u200b\u4e3a\u200b<code>run model successed</code>\u200b\u8868\u660e\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u6210\u529f\u200b\u3002</p> <p>\u200b\u6a21\u578b\u200b\u7684\u200b\u8fd0\u884c\u200b\u7ed3\u679c\u663e\u793a\u200b\u5728\u200b\u8fd0\u884c\u200b\u7ed3\u679c\u663e\u793a\u200b\u533a\u200b\uff0c\u200b\u663e\u793a\u200b\u683c\u5f0f\u200b\u4e3a\u200b</p> <pre><code>\u200b\u5e8f\u53f7\u200b\uff1aDet\uff1a(x1,y1)(x2,y2)(x3,y3)(x4,y4) Rec: \u200b\u8bc6\u522b\u200b\u6587\u672c\u200b,\u200b\u8bc6\u522b\u200b\u7f6e\u4fe1\u5ea6\u200b Cls\uff1a\u200b\u5206\u7c7b\u200b\u7c7b\u522b\u200b,\u200b\u5206\u7c7b\u200b\u5206\u200b\u65f6\u200b\n</code></pre>"},{"location":"en/ppocr/infer_deploy/android_demo.html#34","title":"3.4 \u200b\u8fd0\u884c\u200b\u6a21\u5f0f","text":"<p>PaddleOCR demo\u200b\u5171\u200b\u63d0\u4f9b\u200b\u4e86\u200b6\u200b\u79cd\u200b\u8fd0\u884c\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b</p> <p></p> <p>\u200b\u6bcf\u79cd\u200b\u6a21\u5f0f\u200b\u7684\u200b\u8fd0\u884c\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b</p> \u200b\u68c0\u6d4b\u200b+\u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u200b \u200b\u68c0\u6d4b\u200b+\u200b\u8bc6\u522b\u200b \u200b\u5206\u7c7b\u200b+\u200b\u8bc6\u522b\u200b \u200b\u68c0\u6d4b\u200b \u200b\u8bc6\u522b\u200b \u200b\u5206\u7c7b"},{"location":"en/ppocr/infer_deploy/android_demo.html#35","title":"3.5 \u200b\u8bbe\u7f6e","text":"<p>\u200b\u8bbe\u7f6e\u200b\u754c\u9762\u200b\u5982\u4e0b\u200b</p> <p></p> <p>\u200b\u5728\u200b\u8bbe\u7f6e\u200b\u754c\u9762\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u5982\u4e0b\u200b\u51e0\u9879\u200b\u8bbe\u5b9a\u200b\uff1a</p> <ol> <li>\u200b\u666e\u901a\u200b\u8bbe\u7f6e\u200b</li> <li>Enable custom settings: \u200b\u9009\u4e2d\u200b\u72b6\u6001\u200b\u4e0b\u200b\u624d\u80fd\u200b\u66f4\u6539\u200b\u8bbe\u7f6e\u200b</li> <li>Model Path: \u200b\u6240\u200b\u8fd0\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\u5730\u5740\u200b\uff0c\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u503c\u200b\u5c31\u200b\u597d\u200b</li> <li>Label Path: \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u5b57\u5178\u200b</li> <li>Image Path: \u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\u7684\u200b\u5185\u7f6e\u200b\u56fe\u50cf\u200b\u540d\u200b</li> <li>\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u6001\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u6b64\u9879\u200b\u8bbe\u7f6e\u200b\u66f4\u6539\u200b\u540e\u200b\u8fd4\u56de\u200b\u4e3b\u200b\u754c\u9762\u200b\u65f6\u200b\uff0c\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b</li> <li>CPU Thread Num: \u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u4f7f\u7528\u200b\u7684\u200bCPU\u200b\u6838\u5fc3\u200b\u6570\u91cf\u200b</li> <li>CPU Power Mode: \u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5927\u5c0f\u200b\u6838\u200b\u8bbe\u5b9a\u200b</li> <li>\u200b\u8f93\u5165\u200b\u8bbe\u7f6e\u200b</li> <li>det long size: DB\u200b\u6a21\u578b\u200b\u9884\u5904\u7406\u200b\u65f6\u200b\u56fe\u50cf\u200b\u7684\u200b\u957f\u8fb9\u200b\u957f\u5ea6\u200b\uff0c\u200b\u8d85\u8fc7\u200b\u6b64\u200b\u957f\u5ea6\u200bresize\u200b\u5230\u200b\u8be5\u503c\u200b\uff0c\u200b\u77ed\u8fb9\u200b\u8fdb\u884c\u200b\u7b49\u200b\u6bd4\u4f8b\u200b\u7f29\u653e\u200b\uff0c\u200b\u5c0f\u4e8e\u200b\u6b64\u200b\u957f\u5ea6\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002</li> <li>\u200b\u8f93\u51fa\u200b\u8bbe\u7f6e\u200b</li> <li>Score Threshold: DB\u200b\u6a21\u578b\u200b\u540e\u5904\u7406\u200bbox\u200b\u7684\u200b\u9608\u503c\u200b\uff0c\u200b\u4f4e\u4e8e\u200b\u6b64\u200b\u9608\u503c\u200b\u7684\u200bbox\u200b\u8fdb\u884c\u200b\u8fc7\u6ee4\u200b\uff0c\u200b\u4e0d\u200b\u663e\u793a\u200b\u3002</li> </ol>"},{"location":"en/ppocr/infer_deploy/android_demo.html#4","title":"4 \u200b\u66f4\u200b\u591a\u200b\u652f\u6301","text":"<ol> <li>\u200b\u5b9e\u65f6\u200b\u8bc6\u522b\u200b\uff0c\u200b\u66f4\u65b0\u200b\u9884\u6d4b\u200b\u5e93\u200b\u53ef\u200b\u53c2\u8003\u200b https://github.com/PaddlePaddle/Paddle-Lite-Demo/tree/develop/ocr/android/app/cxx/ppocr_demo</li> <li>\u200b\u66f4\u200b\u591a\u200bPaddle-Lite\u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u53ef\u200b\u524d\u5f80\u200bPaddle-Lite \uff0c\u200b\u83b7\u5f97\u200b\u66f4\u200b\u591a\u200b\u5f00\u53d1\u200b\u652f\u6301\u200b</li> </ol>"},{"location":"en/ppocr/infer_deploy/benchmark.html","title":"Benchmark","text":"<p>This document gives the performance of the series models for Chinese and English recognition.</p>"},{"location":"en/ppocr/infer_deploy/benchmark.html#test-data","title":"Test Data","text":"<p>We collected 300 images for different real application scenarios to evaluate the overall OCR system, including contract samples, license plates, nameplates, train tickets, test sheets, forms, certificates, street view images, business cards, digital meter, etc. The following figure shows some images of the test set.</p> <p></p>"},{"location":"en/ppocr/infer_deploy/benchmark.html#measurement","title":"Measurement","text":"<p>Explanation:</p> <ul> <li> <p>The long size of the input for the text detector is 960.</p> </li> <li> <p>The evaluation time-consuming stage is the complete stage from image input to result output, including image pre-processing and post-processing.</p> </li> <li> <p><code>Intel Xeon 6148</code> is the server-side CPU model. Intel MKL-DNN is used in the test to accelerate the CPU prediction speed.</p> </li> <li> <p><code>Snapdragon 855</code> is a mobile processing platform model.</p> </li> </ul> <p>Compares the model size and F-score:</p> Model Name Model Size  of the  Whole System\\(M\\) Model Size of the Text  Detector\\(M\\) Model Size  of the Direction  Classifier\\(M\\) Model Sizeof the Text  Recognizer \\(M\\) F-score PP-OCRv2 11.6 3.0 0.9 8.6 0.5224 PP-OCR mobile 8.1 2.6 0.9 4.6 0.503 PP-OCR server 155.1 47.2 0.9 107 0.570 <p>Compares the time-consuming on CPU and T4 GPU (ms):</p> Model Name CPU T4 GPU PP-OCRv2 330 111 PP-OCR mobile 356 116 PP-OCR server 1056 200"},{"location":"en/ppocr/infer_deploy/cpp_infer.html","title":"Server-side C++ Inference","text":"<p>This chapter introduces the C++ deployment steps of the PaddleOCR model. C++ is better than Python in terms of performance. Therefore, in CPU and GPU deployment scenarios, C++ deployment is mostly used. This section will introduce how to configure the C++ environment and deploy PaddleOCR in Linux (CPU\\GPU) environment. For Windows deployment please refer to Windows compilation guidelines.</p>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#1-prepare-the-environment","title":"1. Prepare the Environment","text":""},{"location":"en/ppocr/infer_deploy/cpp_infer.html#11-environment","title":"1.1 Environment","text":"<ul> <li>Linux, docker is recommended.</li> <li>Windows.</li> </ul>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#12-compile-opencv","title":"1.2 Compile OpenCV","text":"<ul> <li>First of all, you need to download the source code compiled package in the Linux environment from the OpenCV official website. Taking OpenCV 3.4.7 as an example, the download command is as follows.</li> </ul> <pre><code>cd deploy/cpp_infer\nwget https://paddleocr.bj.bcebos.com/libs/opencv/opencv-3.4.7.tar.gz\ntar -xf opencv-3.4.7.tar.gz\n</code></pre> <p>Finally, you will see the folder of <code>opencv-3.4.7/</code> in the current directory.</p> <ul> <li>Compile OpenCV, the OpenCV source path (<code>root_path</code>) and installation path (<code>install_path</code>) should be set by yourself. Enter the OpenCV source code path and compile it in the following way.</li> </ul> <pre><code>root_path=your_opencv_root_path\ninstall_path=${root_path}/opencv3\n\nrm -rf build\nmkdir build\ncd build\n\ncmake .. \\\n    -DCMAKE_INSTALL_PREFIX=${install_path} \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DWITH_IPP=OFF \\\n    -DBUILD_IPP_IW=OFF \\\n    -DWITH_LAPACK=OFF \\\n    -DWITH_EIGEN=OFF \\\n    -DCMAKE_INSTALL_LIBDIR=lib64 \\\n    -DWITH_ZLIB=ON \\\n    -DBUILD_ZLIB=ON \\\n    -DWITH_JPEG=ON \\\n    -DBUILD_JPEG=ON \\\n    -DWITH_PNG=ON \\\n    -DBUILD_PNG=ON \\\n    -DWITH_TIFF=ON \\\n    -DBUILD_TIFF=ON\n\nmake -j\nmake install\n</code></pre> <p>In the above commands, <code>root_path</code> is the downloaded OpenCV source code path, and <code>install_path</code> is the installation path of OpenCV. After <code>make install</code> is completed, the OpenCV header file and library file will be generated in this folder for later OCR source code compilation.</p> <p>The final file structure under the OpenCV installation path is as follows.</p> <pre><code>opencv3/\n|-- bin\n|-- include\n|-- lib\n|-- lib64\n|-- share\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#13-compile-or-download-or-the-paddle-inference-library","title":"1.3 Compile or Download or the Paddle Inference Library","text":"<ul> <li>There are 2 ways to obtain the Paddle inference library, described in detail below.</li> </ul>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#131-direct-download-and-installation","title":"1.3.1 Direct download and installation","text":"<p>Paddle inference library official website. You can review and select the appropriate version of the inference library on the official website.</p> <ul> <li>After downloading, use the following command to extract files.</li> </ul> <pre><code>tar -xf paddle_inference.tgz\n</code></pre> <p>Finally you will see the folder of <code>paddle_inference/</code> in the current path.</p>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#132-compile-the-inference-source-code","title":"1.3.2 Compile the inference source code","text":"<ul> <li> <p>If you want to get the latest Paddle inference library features, you can download the latest code from Paddle GitHub repository and compile the inference library from the source code. It is recommended to download the inference library with paddle version greater than or equal to 2.0.1.</p> </li> <li> <p>You can refer to Paddle inference library to get the Paddle source code from GitHub, and then compile To generate the latest inference library. The method of using git to access the code is as follows.</p> </li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/Paddle.git\ngit checkout develop\n</code></pre> <ul> <li>Enter the Paddle directory and run the following commands to compile the paddle inference library.</li> </ul> <pre><code>rm -rf build\nmkdir build\ncd build\n\ncmake  .. \\\n    -DWITH_CONTRIB=OFF \\\n    -DWITH_MKL=ON \\\n    -DWITH_MKLDNN=ON  \\\n    -DWITH_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DWITH_INFERENCE_API_TEST=OFF \\\n    -DON_INFER=ON \\\n    -DWITH_PYTHON=ON\nmake -j\nmake inference_lib_dist\n</code></pre> <p>For more compilation parameter options, please refer to the document.</p> <ul> <li>After the compilation process, you can see the following files in the folder of <code>build/paddle_inference_install_dir/</code>.</li> </ul> <pre><code>build/paddle_inference_install_dir/\n|-- CMakeCache.txt\n|-- paddle\n|-- third_party\n|-- version.txt\n</code></pre> <p><code>paddle</code> is the Paddle library required for C++ prediction later, and <code>version.txt</code> contains the version information of the current inference library.</p>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#2-compile-and-run-the-demo","title":"2. Compile and Run the Demo","text":""},{"location":"en/ppocr/infer_deploy/cpp_infer.html#21-export-the-inference-model","title":"2.1 Export the inference model","text":"<ul> <li>You can refer to Model inference and export the inference model. After the model is exported, assuming it is placed in the <code>inference</code> directory, the directory structure is as follows.</li> </ul> <pre><code>inference/\n|-- det_db\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- rec_rcnn\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- cls\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- table\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- layout\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#22-compile-paddleocr-c-inference-demo","title":"2.2 Compile PaddleOCR C++ inference demo","text":"<ul> <li>The compilation commands are as follows. The addresses of Paddle C++ inference library, opencv and other Dependencies need to be replaced with the actual addresses on your own machines.</li> </ul> <pre><code>sh tools/build.sh\n</code></pre> <p>Specifically, you should modify the paths in <code>tools/build.sh</code>. The related content is as follows.</p> <pre><code>OPENCV_DIR=your_opencv_dir\nLIB_DIR=your_paddle_inference_dir\nCUDA_LIB_DIR=your_cuda_lib_dir\nCUDNN_LIB_DIR=your_cudnn_lib_dir\n</code></pre> <p><code>OPENCV_DIR</code> is the OpenCV installation path; <code>LIB_DIR</code> is the download (<code>paddle_inference</code> folder) or the generated Paddle inference library path (<code>build/paddle_inference_install_dir</code> folder); <code>CUDA_LIB_DIR</code> is the CUDA library file path, in docker; it is <code>/usr/local/cuda/lib64</code>; <code>CUDNN_LIB_DIR</code> is the cuDNN library file path, in docker it is <code>/usr/lib/x86_64-linux-gnu/</code>.</p> <ul> <li>After the compilation is completed, an executable file named <code>ppocr</code> will be generated in the <code>build</code> folder.</li> </ul>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#23-run-the-demo","title":"2.3 Run the demo","text":"<p>Execute the built executable file:</p> <pre><code>./build/ppocr [--param1] [--param2] [...]\n</code></pre> <p>Note:ppocr uses the <code>PP-OCRv3</code> model by default, and the input shape used by the recognition model is <code>3, 48, 320</code>, if you want to use the old version model, you should add the parameter <code>--rec_img_h=32</code>.</p> <p>Specifically,</p>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#1-detclsrec","title":"1. det+cls+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=true \\\n    --det=true \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#2-detrec","title":"2. det+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=false \\\n    --det=true \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#3-det","title":"3. det","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --det=true \\\n    --rec=false\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#4-clsrec","title":"4. cls+rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#5-rec","title":"5. rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=false \\\n    --det=false \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#6-cls","title":"6. cls","text":"<pre><code>./build/ppocr --cls_model_dir=inference/cls \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=false \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#7-layouttable","title":"7. layout+table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --layout_model_dir=inference/layout \\\n    --type=structure \\\n    --table=true \\\n    --layout=true\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#8-layout","title":"8. layout","text":"<pre><code>./build/ppocr --layout_model_dir=inference/layout \\\n    --image_dir=../../ppstructure/docs/table/1.png \\\n    --type=structure \\\n    --table=false \\\n    --layout=true \\\n    --det=false \\\n    --rec=false\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#9-table","title":"9. table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --type=structure \\\n    --table=true\n</code></pre> <p>More parameters are as follows,</p> <p>Common parameters</p> parameter data type default meaning use_gpu bool false Whether to use GPU gpu_id int 0 GPU id when use_gpu is true gpu_mem int 4000 GPU memory requested cpu_math_library_num_threads int 10 Number of threads when using CPU inference. When machine cores is enough, the large the value, the faster the inference speed enable_mkldnn bool true Whether to use mkdlnn library output str ./output Path where visualization results are saved <p>forward</p> parameter data type default meaning det bool true Whether to perform text detection in the forward direction rec bool true Whether to perform text recognition in the forward direction cls bool false Whether to perform text direction classification in the forward direction <p>Detection related parameters</p> parameter data type default meaning det_model_dir string - Address of detection inference model max_side_len int 960 Limit the maximum image height and width to 960 det_db_thresh float 0.3 Used to filter the binarized image of DB prediction, setting 0.-0.3 has no obvious effect on the result det_db_box_thresh float 0.5 DB post-processing filter box threshold, if there is a missing box detected, it can be reduced as appropriate det_db_unclip_ratio float 1.6 Indicates the compactness of the text box, the smaller the value, the closer the text box to the text det_db_score_mode string slow slow: use polygon box to calculate bbox score, fast: use rectangle box to calculate. Use rectangular box to calculate faster, and polygonal box more accurate for curved text area. visualize bool true Whether to visualize the results\uff0cwhen it is set as true, the prediction results will be saved in the folder specified by the <code>output</code> field on an image with the same name as the input image. <p>Classifier related parameters</p> parameter data type default meaning use_angle_cls bool false Whether to use the direction classifier cls_model_dir string - Address of direction classifier inference model cls_thresh float 0.9 Score threshold of the  direction classifier cls_batch_num int 1 batch size of classifier <p>Recognition related parameters</p> parameter data type default meaning rec_model_dir string - Address of recognition inference model rec_char_dict_path string ../../ppocr/utils/ppocr_keys_v1.txt dictionary file rec_batch_num int 6 batch size of recognition rec_img_h int 48 image height of recognition rec_img_w int 320 image width of recognition <p>Layout related parameters</p> parameter data type default meaning layout_model_dir string - Address of layout inference model layout_dict_path string ../../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt dictionary file layout_score_threshold float 0.5 Threshold of score. layout_nms_threshold float 0.5 Threshold of nms. <p>Table recognition related parameters</p> parameter data type default meaning table_model_dir string - Address of table recognition inference model table_char_dict_path string ../../ppocr/utils/dict/table_structure_dict.txt dictionary file table_max_len int 488 The size of the long side of the input image of the table recognition model, the final input image size of the network is\uff08table_max_len\uff0ctable_max_len\uff09 merge_no_span_structure bool true Whether to merge  and  to &lt;/td <p>Multi-language inference is also supported in PaddleOCR, you can refer to recognition tutorial for more supported languages and models in PaddleOCR. Specifically, if you want to infer using multi-language models, you just need to modify values of <code>rec_char_dict_path</code> and <code>rec_model_dir</code>.</p> <p>The detection results will be shown on the screen, which is as follows.</p> <pre><code>predict img: ../../doc/imgs/12.jpg\n../../doc/imgs/12.jpg\n0       det boxes: [[74,553],[427,542],[428,571],[75,582]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b252935\u200b\u53f7\u200b rec score: 0.947724\n1       det boxes: [[23,507],[513,488],[515,529],[24,548]] rec text: \u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b rec score: 0.993728\n2       det boxes: [[187,456],[399,448],[400,480],[188,488]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b15\u200b\u53f7\u200b rec score: 0.964994\n3       det boxes: [[42,413],[483,391],[484,428],[43,450]] rec text: \u200b\u4e0a\u6d77\u200b\u65af\u683c\u5a01\u200b\u94c2\u200b\u5c14\u200b\u5927\u9152\u5e97\u200b rec score: 0.980086\nThe detection visualized image saved in ./output//12.jpg\n</code></pre> <ul> <li>layout+table</li> </ul> <pre><code>predict img: ../../ppstructure/docs/table/1.png\n0       type: text, region: [12,729,410,848], score: 0.781044, res: count of ocr result is : 7\n********** print ocr result **********\n0       det boxes: [[4,1],[79,1],[79,12],[4,12]] rec text: CTW1500. rec score: 0.769472\n...\n6       det boxes: [[4,99],[391,99],[391,112],[4,112]] rec text: sate-of-the-artmethods[12.34.36l.ourapproachachieves rec score: 0.90414\n********** end print ocr result **********\n1       type: text, region: [69,342,342,359], score: 0.703666, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[269,2],[269,13],[8,13]] rec text: Table6.Experimentalresults on CTW-1500 rec score: 0.890454\n********** end print ocr result **********\n2       type: text, region: [70,316,706,332], score: 0.659738, res: count of ocr result is : 2\n********** print ocr result **********\n0       det boxes: [[373,2],[630,2],[630,11],[373,11]] rec text: oroposals.andthegreencontoursarefinal rec score: 0.919729\n1       det boxes: [[8,3],[357,3],[357,11],[8,11]] rec text: Visualexperimentalresultshebluecontoursareboundar rec score: 0.915963\n********** end print ocr result **********\n3       type: text, region: [489,342,789,359], score: 0.630538, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[294,2],[294,14],[8,14]] rec text: Table7.Experimentalresults onMSRA-TD500 rec score: 0.942251\n********** end print ocr result **********\n4       type: text, region: [444,751,841,848], score: 0.607345, res: count of ocr result is : 5\n********** print ocr result **********\n0       det boxes: [[19,3],[389,3],[389,17],[19,17]] rec text: Inthispaper,weproposeanovel adaptivebound rec score: 0.941031\n1       det boxes: [[4,22],[390,22],[390,36],[4,36]] rec text: aryproposalnetworkforarbitraryshapetextdetection rec score: 0.960172\n2       det boxes: [[4,42],[392,42],[392,56],[4,56]] rec text: whichadoptanboundaryproposalmodeltogeneratecoarse rec score: 0.934647\n3       det boxes: [[4,61],[389,61],[389,75],[4,75]] rec text: ooundaryproposals,andthenadoptanadaptiveboundary rec score: 0.946296\n4       det boxes: [[5,80],[387,80],[387,93],[5,93]] rec text: leformationmodelcombinedwithGCNandRNNtoper rec score: 0.952401\n********** end print ocr result **********\n5       type: title, region: [444,705,564,724], score: 0.785429, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[6,2],[113,2],[113,14],[6,14]] rec text: 5.Conclusion rec score: 0.856903\n********** end print ocr result **********\n6       type: table, region: [14,360,402,711], score: 0.963643, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;Ext&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;85.3&lt;/td&gt;&lt;td&gt;67.9&lt;/td&gt;&lt;td&gt;75.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CSE [17]&lt;/td&gt;&lt;td&gt;MiLT&lt;/td&gt;&lt;td&gt;76.1&lt;/td&gt;&lt;td&gt;78.7&lt;/td&gt;&lt;td&gt;77.4&lt;/td&gt;&lt;td&gt;0.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LOMO[40]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;76.5&lt;/td&gt;&lt;td&gt;85.7&lt;/td&gt;&lt;td&gt;80.8&lt;/td&gt;&lt;td&gt;4.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;Sy-&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SegLink++ [28]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;81.4&lt;/td&gt;&lt;td&gt;6.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.0&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;81.5&lt;/td&gt;&lt;td&gt;4.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PSENet-1s [33]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;79.7&lt;/td&gt;&lt;td&gt;84.8&lt;/td&gt;&lt;td&gt;82.2&lt;/td&gt;&lt;td&gt;3.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB [12]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;86.9&lt;/td&gt;&lt;td&gt;83.4&lt;/td&gt;&lt;td&gt;22.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.1&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;83.5&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextDragon [5]&lt;/td&gt;&lt;td&gt;MLT+&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;84.5&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.2&lt;/td&gt;&lt;td&gt;86.4&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;39.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ContourNet [36]&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;83.9&lt;/td&gt;&lt;td&gt;4.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.02&lt;/td&gt;&lt;td&gt;85.93&lt;/td&gt;&lt;td&gt;84.45&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextPerception[23]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.9&lt;/td&gt;&lt;td&gt;87.5&lt;/td&gt;&lt;td&gt;84.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt; Syn&lt;/td&gt;&lt;td&gt;80.57&lt;/td&gt;&lt;td&gt;87.66&lt;/td&gt;&lt;td&gt;83.97&lt;/td&gt;&lt;td&gt;12.08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;81.45&lt;/td&gt;&lt;td&gt;87.81&lt;/td&gt;&lt;td&gt;84.51&lt;/td&gt;&lt;td&gt;12.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.60&lt;/td&gt;&lt;td&gt;86.45&lt;/td&gt;&lt;td&gt;85.00&lt;/td&gt;&lt;td&gt;12.21&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//6_1.png\n7       type: table, region: [462,359,820,657], score: 0.953917, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;SegLink [26]&lt;/td&gt;&lt;td&gt;70.0&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;77.0&lt;/td&gt;&lt;td&gt;8.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PixelLink [4]&lt;/td&gt;&lt;td&gt;73.2&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;77.8&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;73.9&lt;/td&gt;&lt;td&gt;83.2&lt;/td&gt;&lt;td&gt;78.3&lt;/td&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;75.9&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;5.2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;76.7&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FTSN[3]&lt;/td&gt;&lt;td&gt;77.1&lt;/td&gt;&lt;td&gt;87.6&lt;/td&gt;&lt;td&gt;82.0&lt;/td&gt;&lt;td&gt;:&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LSE[30]&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;84.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;78.2&lt;/td&gt;&lt;td&gt;88.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;8.6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MCN [16]&lt;/td&gt;&lt;td&gt;79&lt;/td&gt;&lt;td&gt;88&lt;/td&gt;&lt;td&gt;83&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;82.1&lt;/td&gt;&lt;td&gt;85.2&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;83.8&lt;/td&gt;&lt;td&gt;84.4&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;30.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB[12]&lt;/td&gt;&lt;td&gt;79.2&lt;/td&gt;&lt;td&gt;91.5&lt;/td&gt;&lt;td&gt;84.9&lt;/td&gt;&lt;td&gt;32.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;82.30&lt;/td&gt;&lt;td&gt;88.05&lt;/td&gt;&lt;td&gt;85.08&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (SynText)&lt;/td&gt;&lt;td&gt;80.68&lt;/td&gt;&lt;td&gt;85.40&lt;/td&gt;&lt;td&gt;82.97&lt;/td&gt;&lt;td&gt;12.68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (MLT-17)&lt;/td&gt;&lt;td&gt;84.54&lt;/td&gt;&lt;td&gt;86.62&lt;/td&gt;&lt;td&gt;85.57&lt;/td&gt;&lt;td&gt;12.31&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//7_1.png\n8       type: figure, region: [14,3,836,310], score: 0.969443, res: count of ocr result is : 26\n********** print ocr result **********\n0       det boxes: [[506,14],[539,15],[539,22],[506,21]] rec text: E rec score: 0.318073\n...\n25      det boxes: [[680,290],[759,288],[759,303],[680,305]] rec text: (d) CTW1500 rec score: 0.95911\n********** end print ocr result **********\n</code></pre>"},{"location":"en/ppocr/infer_deploy/cpp_infer.html#3-faq","title":"3. FAQ","text":"<ol> <li>Encountered the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.</code>, change the github address in <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to the https://gitee.com/Double_V/AutoLog address.</li> </ol>"},{"location":"en/ppocr/infer_deploy/lite.html","title":"Mobile deployment based on Paddle-Lite","text":"<p>This tutorial will introduce how to use Paddle-Lite to deploy PaddleOCR ultra-lightweight Chinese and English detection models on mobile phones.</p> <p>Paddle-Lite is a lightweight inference engine for PaddlePaddle. It provides efficient inference capabilities for mobile phones and IoT, and extensively integrates cross-platform hardware to provide lightweight deployment solutions for end-side deployment issues.</p>"},{"location":"en/ppocr/infer_deploy/lite.html#1-preparation","title":"1. Preparation","text":""},{"location":"en/ppocr/infer_deploy/lite.html#preparation-environment","title":"Preparation environment","text":"<ul> <li>Computer (for Compiling Paddle Lite)</li> <li>Mobile phone (arm7 or arm8)</li> </ul>"},{"location":"en/ppocr/infer_deploy/lite.html#11-prepare-the-cross-compilation-environment","title":"1.1 Prepare the cross-compilation environment","text":"<p>The cross-compilation environment is used to compile C++ demos of Paddle Lite and PaddleOCR. Supports multiple development environments.</p> <p>For the compilation process of different development environments, please refer to the corresponding documents.</p> <ol> <li>Docker</li> <li>Linux</li> <li>MAC OS</li> </ol>"},{"location":"en/ppocr/infer_deploy/lite.html#12-prepare-paddle-lite-library","title":"1.2 Prepare Paddle-Lite library","text":"<p>There are two ways to obtain the Paddle-Lite library:</p> <ol> <li> <p>[Recommended] Download directly, the download link of the Paddle-Lite library is as follows:</p> Platform Paddle-Lite library download link Android arm7 / arm8 IOS arm7 / arm8 <p>Note: 1. The above Paddle-Lite library is compiled from the Paddle-Lite 2.10 branch. For more information about Paddle-Lite 2.10, please refer to link.</p> <p>Note: It is recommended to use paddlelite&gt;=2.10 version of the prediction library, other prediction library versions download link</p> </li> <li> <p>Compile Paddle-Lite to get the prediction library. The compilation method of Paddle-Lite is as follows:</p> </li> </ol> <pre><code>git clone https://github.com/PaddlePaddle/Paddle-Lite.git\ncd Paddle-Lite\n# Switch to Paddle-Lite release/v2.10 stable branch\ngit checkout release/v2.10\n./lite/tools/build_android.sh  --arch=armv8  --with_cv=ON --with_extra=ON\n</code></pre> <p>Note: When compiling Paddle-Lite to obtain the Paddle-Lite library, you need to turn on the two options <code>--with_cv=ON --with_extra=ON</code>, <code>--arch</code> means the <code>arm</code> version, here is designated as armv8,</p> <p>More compilation commands refer to the introduction link \u3002</p> <p>After directly downloading the Paddle-Lite library and decompressing it, you can get the <code>inference_lite_lib.android.armv8/</code> folder, and the Paddle-Lite library obtained by compiling Paddle-Lite is located <code>Paddle-Lite/build.lite.android.armv8.gcc/inference_lite_lib.android.armv8/</code> folder.</p> <p>The structure of the prediction library is as follows:</p> <pre><code>inference_lite_lib.android.armv8/\n|-- cxx                                        C++ prebuild library\n|   |-- include                                C++\n|   |   |-- paddle_api.h\n|   |   |-- paddle_image_preprocess.h\n|   |   |-- paddle_lite_factory_helper.h\n|   |   |-- paddle_place.h\n|   |   |-- paddle_use_kernels.h\n|   |   |-- paddle_use_ops.h\n|   |   `-- paddle_use_passes.h\n|   `-- lib                                           C++ library\n|       |-- libpaddle_api_light_bundled.a             C++ static library\n|       `-- libpaddle_light_api_shared.so             C++ dynamic library\n|-- java                                     Java library\n|   |-- jar\n|   |   `-- PaddlePredictor.jar\n|   |-- so\n|   |   `-- libpaddle_lite_jni.so\n|   `-- src\n|-- demo                                     C++ and Java demo\n|   |-- cxx                                  C++ demo\n|   `-- java                                 Java demo\n</code></pre>"},{"location":"en/ppocr/infer_deploy/lite.html#2-run","title":"2 Run","text":""},{"location":"en/ppocr/infer_deploy/lite.html#21-inference-model-optimization","title":"2.1 Inference Model Optimization","text":"<p>Paddle Lite provides a variety of strategies to automatically optimize the original training model, including quantization, sub-graph fusion, hybrid scheduling, Kernel optimization and so on. In order to make the optimization process more convenient and easy to use, Paddle Lite provide opt tools to automatically complete the optimization steps and output a lightweight, optimal executable model.</p> <p>If you have prepared the model file ending in .nb, you can skip this step.</p> <p>The following table also provides a series of models that can be deployed on mobile phones to recognize Chinese. You can directly download the optimized model.</p> Version Introduction Model size Detection model Text Direction model Recognition model Paddle-Lite branch PP-OCRv3 extra-lightweight chinese OCR optimized model 16.2M download link download link download link v2.10 PP-OCRv3(slim) extra-lightweight chinese OCR optimized model 5.9M download link download link download link v2.10 PP-OCRv2 extra-lightweight chinese OCR optimized model 11M download link download link download link v2.10 PP-OCRv2(slim) extra-lightweight chinese OCR optimized model 4.6M download link download link download link v2.10 <p>If you directly use the model in the above table for deployment, you can skip the following steps and directly read Section 2.2.</p> <p>If the model to be deployed is not in the above table, you need to follow the steps below to obtain the optimized model.</p> <p>Step 1: Refer to document to install paddlelite, which is used to convert paddle inference model to paddlelite required for running nb model</p> <pre><code>pip install paddlelite==2.10 # The paddlelite version should be the same as the prediction library version\n</code></pre> <p>After installation, the following commands can view the help information</p> <pre><code>paddle_lite_opt\n</code></pre> <p>Introduction to paddle_lite_opt parameters:</p> Options Description --model_dir The path of the PaddlePaddle model to be optimized (non-combined form) --model_file The network structure file path of the PaddlePaddle model (combined form) to be optimized --param_file The weight file path of the PaddlePaddle model (combined form) to be optimized --optimize_out_type Output model type, currently supports two types: protobuf and naive_buffer, among which naive_buffer is a more lightweight serialization/deserialization implementation. If you need to perform model prediction on the mobile side, please set this option to naive_buffer. The default is protobuf --optimize_out The output path of the optimized model --valid_targets The executable backend of the model, the default is arm. Currently it supports x86, arm, opencl, npu, xpu, multiple backends can be specified at the same time (separated by spaces), and Model Optimize Tool will automatically select the best method. If you need to support Huawei NPU (DaVinci architecture NPU equipped with Kirin 810/990 Soc), it should be set to npu, arm --record_tailoring_info When using the function of cutting library files according to the model, set this option to true to record the kernel and OP information contained in the optimized model. The default is false <p><code>--model_dir</code> is suitable for the non-combined mode of the model to be optimized, and the inference model of PaddleOCR is the combined mode, that is, the model structure and model parameters are stored in a single file.</p> <p>Step 2: Use paddle_lite_opt to convert the inference model to the mobile model format.</p> <p>The following takes the ultra-lightweight Chinese model of PaddleOCR as an example to introduce the use of the compiled opt file to complete the conversion of the inference model to the Paddle-Lite optimized model</p> <pre><code># \u3010[Recommendation] Download the Chinese and English inference model of PP-OCRv3\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_slim_infer.tar &amp;&amp; tar xf  ch_PP-OCRv3_det_slim_infer.tar\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_slim_infer.tar &amp;&amp; tar xf  ch_PP-OCRv2_rec_slim_quant_infer.tar\nwget  https://paddleocr.bj.bcebos.com/dygraph_v2.0/slim/ch_ppocr_mobile_v2.0_cls_slim_infer.tar &amp;&amp; tar xf  ch_ppocr_mobile_v2.0_cls_slim_infer.tar\n# Convert detection model\npaddle_lite_opt --model_file=./ch_PP-OCRv3_det_slim_infer/inference.pdmodel  --param_file=./ch_PP-OCRv3_det_slim_infer/inference.pdiparams  --optimize_out=./ch_PP-OCRv3_det_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n# Convert recognition model\npaddle_lite_opt --model_file=./ch_PP-OCRv3_rec_slim_infer/inference.pdmodel  --param_file=./ch_PP-OCRv3_rec_slim_infer/inference.pdiparams  --optimize_out=./ch_PP-OCRv3_rec_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n# Convert angle classifier model\npaddle_lite_opt --model_file=./ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdmodel  --param_file=./ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdiparams  --optimize_out=./ch_ppocr_mobile_v2.0_cls_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n</code></pre> <p>After the conversion is successful, there will be more files ending with <code>.nb</code> in the inference model directory, which is the successfully converted model file.</p>"},{"location":"en/ppocr/infer_deploy/lite.html#22-run-optimized-model-on-phone","title":"2.2 Run optimized model on Phone","text":"<p>Some preparatory work is required first.</p> <ol> <li> <p>Prepare an Android phone with arm8. If the compiled prediction library and opt file are armv7, you need an arm7 phone and modify ARM_ABI = arm7 in the Makefile.</p> </li> <li> <p>Make sure the phone is connected to the computer, open the USB debugging option of the phone, and select the file transfer mode.</p> </li> <li> <p>Install the adb tool on the computer.</p> <p>3.1. Install ADB for MAC:</p> <pre><code>brew cask install android-platform-tools\n</code></pre> <p>3.2. Install ADB for Linux</p> <pre><code>sudo apt update\nsudo apt install -y wget adb\n</code></pre> <p>3.3. Install ADB for windows</p> <p>To install on win, you need to go to Google's Android platform to download the adb package for installation:link</p> <p>Verify whether adb is installed successfully</p> <pre><code>adb devices\n</code></pre> <p>If there is device output, it means the installation is successful\u3002</p> <pre><code>List of devices attached\n744be294    device\n</code></pre> </li> <li> <p>Prepare optimized models, prediction library files, test images and dictionary files used.</p> </li> </ol> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR/deploy/lite/\n# run prepare.sh\nsh prepare.sh /{lite prediction library path}/inference_lite_lib.android.armv8\n\n#\ncd /{lite prediction library path}/inference_lite_lib.android.armv8/\ncd demo/cxx/ocr/\n# copy paddle-lite C++ .so file to debug/ directory\ncp ../../../cxx/lib/libpaddle_light_api_shared.so ./debug/\n\ncd inference_lite_lib.android.armv8/demo/cxx/ocr/\ncp ../../../cxx/lib/libpaddle_light_api_shared.so ./debug/\n</code></pre> <p>Prepare the test image, taking PaddleOCR/doc/imgs/11.jpg as an example, copy the image file to the demo/cxx/ocr/debug/ folder. Prepare the model files optimized by the lite opt tool, ch_PP-OCRv3_det_slim_opt.nb , ch_PP-OCRv3_rec_slim_opt.nb , and place them under the demo/cxx/ocr/debug/ folder.</p> <p>The structure of the OCR demo is as follows after the above command is executed:</p> <pre><code>demo/cxx/ocr/\n|-- debug/\n|   |--ch_PP-OCRv3_det_slim_opt.nb           Detection model\n|   |--ch_PP-OCRv3_rec_slim_opt.nb           Recognition model\n|   |--ch_ppocr_mobile_v2.0_cls_slim_opt.nb           Text direction classification model\n|   |--11.jpg                           Image for OCR\n|   |--ppocr_keys_v1.txt                Dictionary file\n|   |--libpaddle_light_api_shared.so    C++ .so file\n|   |--config.txt                       Config file\n|-- config.txt                  Config file\n|-- cls_process.cc              Pre-processing and post-processing files for the angle classifier\n|-- cls_process.h\n|-- crnn_process.cc             Pre-processing and post-processing files for the CRNN model\n|-- crnn_process.h\n|-- db_post_process.cc          Pre-processing and post-processing files for the DB model\n|-- db_post_process.h\n|-- Makefile\n|-- ocr_db_crnn.cc              C++ main code\n</code></pre> <p>Note:</p> <ol> <li><code>ppocr_keys_v1.txt</code> is a Chinese dictionary file. If the nb model is used for English recognition or other language recognition, dictionary file should be replaced with a dictionary of the corresponding language. PaddleOCR provides a variety of dictionaries under ppocr/utils/, including:</li> </ol> <pre><code>dict/french_dict.txt     # french\ndict/german_dict.txt     # german\nic15_dict.txt       # english\ndict/japan_dict.txt      # japan\ndict/korean_dict.txt     # korean\nppocr_keys_v1.txt   # chinese\n</code></pre> <ol> <li> <p><code>config.txt</code> of the detector and classifier, as shown below:</p> <pre><code>max_side_len  960         #  Limit the maximum image height and width to  960\ndet_db_thresh  0.3        # Used to filter the binarized image of DB  prediction, setting 0.-0.3 has no obvious effect on the result\ndet_db_box_thresh  0.5    # DDB post-processing filter box threshold, if  there is a missing box detected, it can be reduced as appropriate\ndet_db_unclip_ratio  1.6  # Indicates the compactness of the text box,  the smaller the value, the closer the text box to the text\nuse_direction_classify  0  # Whether to use the direction classifier, 0  means not to use, 1 means to use\nrec_image_height  48      # The height of the input image of the  recognition model, the PP-OCRv3 model needs to be set to 48, and the  PP-OCRv2 model needs to be set to 32\n</code></pre> </li> <li> <p>Run Model on phone</p> </li> </ol> <p>After the above steps are completed, you can use adb to push the file to the phone to run, the steps are as follows:</p> <pre><code># Execute the compilation and get the executable file ocr_db_crnn\n# The first execution of this command will download dependent libraries such as opencv. After the download is complete, you need to execute it again\nmake -j\n# Move the compiled executable file to the debug folder\nmv ocr_db_crnn ./debug/\n# Push the debug folder to the phone\nadb push debug /data/local/tmp/\nadb shell\ncd /data/local/tmp/debug\nexport LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH\n# The use of ocr_db_crnn is:\n# ./ocr_db_crnn Mode Detection model file Orientation classifier model file Recognition model file  Hardware  Precision  Threads Batchsize  Test image path Dictionary file path\n./ocr_db_crnn system ch_PP-OCRv3_det_slim_opt.nb  ch_PP-OCRv3_rec_slim_opt.nb  ch_ppocr_mobile_v2.0_cls_slim_opt.nb  arm8 INT8 10 1  ./11.jpg  config.txt  ppocr_keys_v1.txt  True\n# precision can be INT8 for quantitative model or FP32 for normal model.\n\n# Only using detection model\n./ocr_db_crnn  det ch_PP-OCRv3_det_slim_opt.nb arm8 INT8 10 1 ./11.jpg  config.txt\n\n# Only using recognition model\n./ocr_db_crnn  rec ch_PP-OCRv3_rec_slim_opt.nb arm8 INT8 10 1 word_1.jpg ppocr_keys_v1.txt config.txt\n</code></pre> <p>If you modify the code, you need to recompile and push to the phone.</p> <p>The outputs are as follows:</p> <p></p>"},{"location":"en/ppocr/infer_deploy/lite.html#faq","title":"FAQ","text":"<p>Q1: What if I want to change the model, do I need to run it again according to the process?</p> <p>A1: If you have performed the above steps, you only need to replace the .nb model file to complete the model replacement.</p> <p>Q2: How to test with another picture?</p> <p>A2: Replace the .jpg test image under ./debug with the image you want to test, and run adb push to push new image to the phone.</p> <p>Q3: How to package it into the mobile APP?</p> <p>A3: This demo aims to provide the core algorithm part that can run OCR on mobile phones. Further, PaddleOCR/deploy/android_demo is an example of encapsulating this demo into a mobile app for reference.</p> <p>Q4: When running the demo, an error is reported <code>Error: This model is not supported, because kernel for 'io_copy' is not supported by Paddle-Lite.</code></p> <p>A4: The problem is that the installed paddlelite version does not match the downloaded prediction library version. Make sure that the paddleliteopt tool matches your prediction library version, and try to switch to the nb model again.</p>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html","title":"Paddle2ONNX model transformation and prediction","text":"<p>This chapter describes how the PaddleOCR model is converted into an ONNX model and predicted based on the ONNXRuntime engine.</p>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#1-environment-preparation","title":"1. Environment preparation","text":"<p>Need to prepare PaddleOCR, Paddle2ONNX model conversion environment, and ONNXRuntime prediction environment</p>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#paddleocr","title":"PaddleOCR","text":"<p>Clone the PaddleOCR repository, use the release/2.6 branch, and install it.</p> <pre><code>git clone  -b release/2.6 https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR &amp;&amp; python3.7 setup.py install\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#paddle2onnx","title":"Paddle2ONNX","text":"<p>Paddle2ONNX supports converting the PaddlePaddle model format to the ONNX model format. The operator currently supports exporting ONNX Opset 9~11 stably, and some Paddle operators support lower ONNX Opset conversion. For more details, please refer to Paddle2ONNX</p> <ul> <li>install Paddle2ONNX</li> </ul> <pre><code>python3.7 -m pip install paddle2onnx\n</code></pre> <ul> <li>install ONNXRuntime</li> </ul> <pre><code># It is recommended to install version 1.9.0, and the version number can be changed according to the environment\npython3.7 -m pip install onnxruntime==1.9.0\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#2-model-conversion","title":"2. Model conversion","text":""},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#paddle-model-download","title":"Paddle model download","text":"<p>There are two ways to obtain the Paddle model: Download the prediction model provided by PaddleOCR in model_list;</p> <p>Take the PP-OCRv3 detection, recognition, and classification model as an example:</p> <pre><code>wget -nc -P ./inference https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv3_rec_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\ncd ./inference &amp;&amp; tar xf ch_ppocr_mobile_v2.0_cls_infer.tar &amp;&amp; cd ..\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#convert-model","title":"Convert model","text":"<p>Convert Paddle inference model to ONNX model format using Paddle2ONNX:</p> <pre><code>paddle2onnx --model_dir ./inference/en_PP-OCRv3_det_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/det_onnx/model.onnx \\\n--opset_version 10 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/en_PP-OCRv3_rec_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/rec_onnx/model.onnx \\\n--opset_version 10 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--model_filename ch_ppocr_mobile_v2.0_cls_infer/inference.pdmodel \\\n--params_filename ch_ppocr_mobile_v2.0_cls_infer/inference.pdiparams \\\n--save_file ./inferencecls_onnx/model.onnx \\\n--opset_version 10 \\\n--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n--enable_onnx_checker True\n</code></pre> <p>After execution, the ONNX model will be saved in <code>./inference/det_onnx/</code>, <code>./inference/rec_onnx/</code>, <code>./inference/cls_onnx/</code> paths respectively</p> <ul> <li>Note: For the OCR model, the conversion process must be in the form of dynamic shape, that is, add the option --input_shape_dict=\"{'x': [-1, 3, -1, -1]}\", otherwise the prediction result may be the same as Predicting directly with Paddle is slightly different.</li> </ul> <p>In addition, the following models do not currently support conversion to ONNX models: NRTR, SAR, RARE, SRN.</p>"},{"location":"en/ppocr/infer_deploy/paddle2onnx.html#3-prediction","title":"3. prediction","text":"<p>Take the English OCR model as an example, use ONNXRuntime to predict and execute the following commands:</p> <pre><code>python3.7 tools/infer/predict_system.py --use_gpu=False --use_onnx=True \\\n--det_model_dir=./inference/det_onnx/model.onnx  \\\n--rec_model_dir=./inference/rec_onnx/model.onnx  \\\n--cls_model_dir=./inference/cls_onnx/model.onnx  \\\n--image_dir=doc/imgs_en/img_12.jpg \\\n--rec_char_dict_path=ppocr/utils/en_dict.txt\n</code></pre> <p>Taking the English OCR model as an example, use Paddle Inference to predict and execute the following commands:</p> <pre><code>python3.7 tools/infer/predict_system.py --use_gpu=False \\\n--cls_model_dir=./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--rec_model_dir=./inference/en_PP-OCRv3_rec_infer \\\n--det_model_dir=./inference/en_PP-OCRv3_det_infer \\\n--image_dir=doc/imgs_en/img_12.jpg \\\n--rec_char_dict_path=ppocr/utils/en_dict.txt\n</code></pre> <p>After executing the command, the predicted identification information will be printed out in the terminal, and the visualization results will be saved under <code>./inference_results/</code>.</p> <p>ONNXRuntime result\uff1a</p> <p></p> <p>Paddle Inference result\uff1a</p> <p></p> <p>Using ONNXRuntime to predict, terminal output:</p> <pre><code>[2022/10/10 12:06:28] ppocr DEBUG: dt_boxes num : 11, elapse : 0.3568880558013916\n[2022/10/10 12:06:31] ppocr DEBUG: rec_res num  : 11, elapse : 2.6445000171661377\n[2022/10/10 12:06:31] ppocr DEBUG: 0  Predict time of doc/imgs_en/img_12.jpg: 3.021s\n[2022/10/10 12:06:31] ppocr DEBUG: ACKNOWLEDGEMENTS, 0.997\n[2022/10/10 12:06:31] ppocr DEBUG: We would like to thank all the designers and, 0.976\n[2022/10/10 12:06:31] ppocr DEBUG: contributors who have been involved in the, 0.979\n[2022/10/10 12:06:31] ppocr DEBUG: production of this book; their contributions, 0.989\n[2022/10/10 12:06:31] ppocr DEBUG: have been indispensable to its creation. We, 0.956\n[2022/10/10 12:06:31] ppocr DEBUG: would also like to express our gratitude to all, 0.991\n[2022/10/10 12:06:31] ppocr DEBUG: the producers for their invaluable opinions, 0.978\n[2022/10/10 12:06:31] ppocr DEBUG: and assistance throughout this project. And to, 0.988\n[2022/10/10 12:06:31] ppocr DEBUG: the many others whose names are not credited, 0.958\n[2022/10/10 12:06:31] ppocr DEBUG: but have made specific input in this book, we, 0.970\n[2022/10/10 12:06:31] ppocr DEBUG: thank you for your continuous support., 0.998\n[2022/10/10 12:06:31] ppocr DEBUG: The visualized image saved in ./inference_results/img_12.jpg\n[2022/10/10 12:06:31] ppocr INFO: The predict total time is 3.2482550144195557\n</code></pre> <p>Using Paddle Inference to predict, terminal output:</p> <pre><code>[2022/10/10 12:06:28] ppocr DEBUG: dt_boxes num : 11, elapse : 0.3568880558013916\n[2022/10/10 12:06:31] ppocr DEBUG: rec_res num  : 11, elapse : 2.6445000171661377\n[2022/10/10 12:06:31] ppocr DEBUG: 0  Predict time of doc/imgs_en/img_12.jpg: 3.021s\n[2022/10/10 12:06:31] ppocr DEBUG: ACKNOWLEDGEMENTS, 0.997\n[2022/10/10 12:06:31] ppocr DEBUG: We would like to thank all the designers and, 0.976\n[2022/10/10 12:06:31] ppocr DEBUG: contributors who have been involved in the, 0.979\n[2022/10/10 12:06:31] ppocr DEBUG: production of this book; their contributions, 0.989\n[2022/10/10 12:06:31] ppocr DEBUG: have been indispensable to its creation. We, 0.956\n[2022/10/10 12:06:31] ppocr DEBUG: would also like to express our gratitude to all, 0.991\n[2022/10/10 12:06:31] ppocr DEBUG: the producers for their invaluable opinions, 0.978\n[2022/10/10 12:06:31] ppocr DEBUG: and assistance throughout this project. And to, 0.988\n[2022/10/10 12:06:31] ppocr DEBUG: the many others whose names are not credited, 0.958\n[2022/10/10 12:06:31] ppocr DEBUG: but have made specific input in this book, we, 0.970\n[2022/10/10 12:06:31] ppocr DEBUG: thank you for your continuous support., 0.998\n[2022/10/10 12:06:31] ppocr DEBUG: The visualized image saved in ./inference_results/img_12.jpg\n[2022/10/10 12:06:31] ppocr INFO: The predict total time is 3.2482550144195557\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html","title":"\u4e91\u4e0a\u200b\u98de\u6868\u200b\u90e8\u7f72\u200b\u5de5\u5177","text":"<p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\uff08PaddleCloud\uff09 \u200b\u662f\u200b\u9762\u5411\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u53ca\u5176\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u7684\u200b\u90e8\u7f72\u200b\u5de5\u5177\u200b\uff0c \u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u5316\u200b\u90e8\u7f72\u200b\u548c\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u90e8\u7f72\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6ee1\u8db3\u200b\u4e0d\u540c\u200b\u573a\u666f\u200b\u4e0e\u200b\u73af\u5883\u200b\u7684\u200b\u90e8\u7f72\u200b\u9700\u6c42\u200b\u3002 \u200b\u672c\u200b\u7ae0\u8282\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bPaddleCloud\u200b\u63d0\u4f9b\u200b\u7684\u200bOCR\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u4ee5\u53ca\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#_2","title":"\u4e91\u4e0a\u200b\u98de\u6868\u200b\u90e8\u7f72\u200b\u5de5\u5177\u200b\u7684\u200b\u4f18\u52bf","text":"<ul> <li>\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u955c\u50cf\u200b\u5927\u793c\u5305\u200b\u3002</li> </ul> <p>PaddleCloud\u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u955c\u50cf\u200b\u5927\u793c\u5305\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u955c\u50cf\u200b\u4e2d\u200b\u5305\u542b\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u6848\u4f8b\u200b\u7684\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b\u5e76\u200b\u80fd\u200b\u6301\u7eed\u200b\u66f4\u65b0\u200b\uff0c\u200b\u652f\u6301\u200b\u5f02\u6784\u200b\u786c\u4ef6\u200b\u73af\u5883\u200b\u548c\u200b\u5e38\u89c1\u200bCUDA\u200b\u7248\u672c\u200b\u3001\u200b\u5f00\u7bb1\u200b\u5373\u7528\u200b\u3002</p> <ul> <li>\u200b\u5177\u6709\u200b\u4e30\u5bcc\u200b\u7684\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u3002</li> </ul> <p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5177\u6709\u200b\u4e30\u5bcc\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u529f\u80fd\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5305\u62ec\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\u7b49\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u7ec4\u4ef6\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5730\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200b\u5de5\u4f5c\u200b\u3002</p> <ul> <li>\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u81ea\u8fd0\u7ef4\u200b\u80fd\u529b\u200b\u3002</li> </ul> <p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200bOperator\u200b\u673a\u5236\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u81ea\u8fd0\u7ef4\u200b\u80fd\u529b\u200b\uff0c\u200b\u5982\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u591a\u79cd\u200b\u67b6\u6784\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u5177\u6709\u200b\u5206\u5e03\u5f0f\u200b\u5bb9\u9519\u200b\u4e0e\u200b\u5f39\u6027\u200b\u8bad\u7ec3\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u81ea\u52a8\u200b\u6269\u7f29\u5bb9\u200b\u4e0e\u200b\u84dd\u7eff\u200b\u53d1\u7248\u200b\u7b49\u200b\u3002</p> <ul> <li>\u200b\u9488\u5bf9\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u7684\u200b\u5b9a\u5236\u200b\u4f18\u5316\u200b\u3002</li> </ul> <p>\u200b\u9664\u4e86\u200b\u90e8\u7f72\u200b\u4fbf\u6377\u200b\u4e0e\u200b\u81ea\u8fd0\u7ef4\u200b\u7684\u200b\u4f18\u52bf\u200b\uff0cPaddleCloud\u200b\u8fd8\u200b\u9488\u5bf9\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6b63\u200b\u5bf9\u6027\u200b\u4f18\u5316\u200b\uff0c\u200b\u5982\u200b\u901a\u8fc7\u200b\u7f13\u5b58\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u6765\u200b\u52a0\u901f\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3001\u200b\u57fa\u4e8e\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u548c\u200b\u8c03\u5ea6\u200b\u5668\u200b\u7684\u200b\u534f\u540c\u200b\u8bbe\u8ba1\u200b\u6765\u200b\u4f18\u5316\u200b\u96c6\u7fa4\u200bGPU\u200b\u5229\u7528\u7387\u200b\u7b49\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#1-pp-ocrv3-docker","title":"1. PP-OCRv3 Docker\u200b\u5316\u200b\u90e8\u7f72","text":"<p>PaddleCloud\u200b\u57fa\u4e8e\u200b Tekton \u200b\u4e3a\u200bOCR\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u955c\u50cf\u200b\u6301\u7eed\u200b\u6784\u5efa\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u5e76\u200b\u652f\u6301\u200bCPU\u3001GPU\u200b\u4ee5\u53ca\u200b\u5e38\u89c1\u200bCUDA\u200b\u7248\u672c\u200b\u7684\u200b\u955c\u50cf\u200b\u3002 \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b PaddleOCR \u200b\u955c\u50cf\u200b\u4ed3\u5e93\u200b \u200b\u6765\u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u7684\u200b\u955c\u50cf\u200b\u5217\u8868\u200b\u3002 \u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5c06\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b\u5b9e\u6218\u200b\u6848\u4f8b\u200b\u653e\u7f6e\u200b\u5230\u200b\u4e86\u200bAI Studio\u200b\u5e73\u53f0\u200b\u4e0a\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b PP-OCRv3\u200b\u8bc6\u522b\u200b\u8bad\u63a8\u200b\u4e00\u4f53\u200b\u9879\u76ee\u200b\u5b9e\u6218\u200b \u200b\u5728\u200b\u5e73\u53f0\u200b\u4e0a\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\u3002</p> <p>\u200b\u9002\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u672c\u5730\u200b\u6d4b\u8bd5\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\u3001\u200b\u5355\u673a\u200b\u90e8\u7f72\u200b\u73af\u5883\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#11-docker","title":"1.1 \u200b\u5b89\u88c5\u200bDocker","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5b89\u88c5\u200b Docker\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b Docker \u200b\u5b98\u65b9\u200b\u6587\u6863\u200b \u200b\u6765\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\u3002 \u200b\u5982\u679c\u200b\u60a8\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u652f\u6301\u200b GPU \u200b\u7248\u672c\u200b\u7684\u200b\u955c\u50cf\u200b\uff0c\u200b\u5219\u200b\u8fd8\u200b\u9700\u200b\u5b89\u88c5\u200b\u597d\u200bNVIDIA\u200b\u76f8\u5173\u9a71\u52a8\u200b\u548c\u200b nvidia-docker \u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5982\u679c\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bWindows\u200b\u7cfb\u7edf\u200b\uff0c\u200b\u9700\u8981\u200b\u5f00\u542f\u200b WSL2\uff08Linux\u200b\u5b50\u7cfb\u7edf\u200b\u529f\u80fd\u200b\uff09\u200b\u529f\u80fd\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#12","title":"1.2 \u200b\u542f\u52a8\u200b\u5bb9\u5668","text":"<p>\u200b\u4f7f\u7528\u200bCPU\u200b\u7248\u672c\u200b\u7684\u200bDocker\u200b\u955c\u50cf\u200b</p> <pre><code># \u200b\u8fd9\u662f\u200b\u52a0\u4e0a\u200b\u53c2\u6570\u200b --shm-size=32g \u200b\u662f\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u5bb9\u5668\u200b\u91cc\u200b\u5185\u5b58\u4e0d\u8db3\u200b\ndocker run --name ppocr -v $PWD:/mnt -p 8888:8888 -it --shm-size=32g paddlecloud/paddleocr:2.5-cpu-efbb0a /bin/bash\n</code></pre> <p>\u200b\u4f7f\u7528\u200bGPU\u200b\u7248\u672c\u200b\u7684\u200bDocker\u200b\u955c\u50cf\u200b</p> <pre><code>docker run --name ppocr --runtime=nvidia -v $PWD:/mnt -p 8888:8888 -it --shm-size=32g paddlecloud/paddleocr:2.5-gpu-cuda10.2-cudnn7-efbb0a /bin/bash\n</code></pre> <p>\u200b\u8fdb\u5165\u200b\u5bb9\u5668\u200b\u5185\u200b\uff0c\u200b\u5219\u200b\u53ef\u200b\u8fdb\u884c\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200b\u5de5\u4f5c\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#13","title":"1.3 \u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u6570\u636e","text":"<p>\u200b\u672c\u200b\u6559\u7a0b\u200b\u4ee5\u200bHierText\u200b\u6570\u636e\u200b\u96c6\u4e3a\u4f8b\u200b\uff0cHierText\u200b\u662f\u200b\u7b2c\u4e00\u4e2a\u200b\u5177\u6709\u200b\u81ea\u7136\u200b\u573a\u666f\u200b\u548c\u200b\u6587\u6863\u200b\u4e2d\u200b\u6587\u672c\u200b\u5206\u5c42\u200b\u6ce8\u91ca\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 \u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b\u4ece\u200b Open Images \u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u9009\u62e9\u200b\u7684\u200b 11639 \u200b\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u9ad8\u8d28\u91cf\u200b\u7684\u200b\u5355\u8bcd\u200b (~1.2M)\u3001\u200b\u884c\u200b\u548c\u200b\u6bb5\u843d\u200b\u7ea7\u522b\u200b\u7684\u200b\u6ce8\u91ca\u200b\u3002 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4f20\u5230\u200b\u767e\u5ea6\u200b\u4e91\u200b\u5bf9\u8c61\u200b\u5b58\u50a8\u200b\uff08BOS\uff09\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u6307\u4ee4\u200b\uff0c\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u4e0b\u8f7d\u200b\u548c\u200b\u89e3\u538b\u200b\u64cd\u4f5c\u200b\uff1a</p> <pre><code># \u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\n$ wget -P /mnt https://paddleflow-public.hkg.bcebos.com/ppocr/hiertext1.tar\n\n# \u200b\u89e3\u538b\u200b\u6570\u636e\u200b\u96c6\u200b\n$ tar xf /mnt/hiertext1.tar -C /mnt &amp;&amp; mv /mnt/hiertext1 /mnt/hiertext\n</code></pre> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u5728\u200b <code>/mnt</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>/mnt/hiertext\n  \u2514\u2500 train/     HierText\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6570\u636e\u200b\n  \u2514\u2500 validation/     HierText\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6570\u636e\u200b\n  \u2514\u2500 label_hiertext_train.txt  HierText\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u884c\u200b\u6807\u6ce8\u200b\n  \u2514\u2500 label_hiertext_val.txt    HierText\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u884c\u200b\u6807\u6ce8\u200b\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#14","title":"1.4 \u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>PP-OCRv3\u200b\u6a21\u578b\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4f4d\u4e8e\u200b<code>/home/PaddleOCR/configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml</code>\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u7684\u200b\u914d\u7f6e\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u914d\u7f6e\u200b\uff1a</li> </ul> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/icdar2015/text_localization/\n    label_file_list:\n      - ./train_data/icdar2015/text_localization/train_icdar2015_label.txt\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u4e3a\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /mnt/\n    label_file_list:\n      - /mnt/hiertext/label_hiertext_train.txt\n</code></pre> <ul> <li>\u200b\u4fee\u6539\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u914d\u7f6e\u200b\uff1a</li> </ul> <pre><code>Eval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/icdar2015/text_localization/\n    label_file_list:\n      - ./train_data/icdar2015/text_localization/test_icdar2015_label.txt\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u4e3a\u200b\uff1a</p> <pre><code>Eval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /mnt/\n    label_file_list:\n      - /mnt/hiertext/label_hiertext_val.txt\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#15","title":"1.5 \u200b\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u7684\u200b\u84b8\u998f\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5982\u4e0b\u200b</p> <pre><code># \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5230\u200b/home/PaddleOCR/pre_train\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\n$ mkdir /home/PaddleOCR/pre_train\n\n$ wget -P /home/PaddleOCR/pre_train https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\n\n$ tar xf /home/PaddleOCR/pre_train/ch_PP-OCRv3_det_distill_train.tar -C /home/PaddleOCR/pre_train/\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>output</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u52a0\u8f7d\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p> <pre><code># \u200b\u8fd9\u91cc\u200b\u4ee5\u200b GPU \u200b\u8bad\u7ec3\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4f7f\u7528\u200b CPU \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u8bdd\u200b\uff0c\u200b\u9700\u8981\u200b\u6307\u5b9a\u200b\u53c2\u6570\u200b Global.use_gpu=false\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.save_model_dir=./output/ Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n</code></pre> <p>\u200b\u5982\u679c\u200b\u8981\u200b\u4f7f\u7528\u200b\u591a\u200bGPU\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code># \u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200boutput\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c--gpus '0,1,2,3'\u200b\u8868\u793a\u200b\u4f7f\u7528\u200b0\uff0c1\uff0c2\uff0c3\u200b\u53f7\u200bGPU\u200b\u8bad\u7ec3\u200b\npython3 -m paddle.distributed.launch --log_dir=./debug/ --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.save_model_dir=./output/ Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#16","title":"1.6 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200boutput\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>best_accuracy.states\nbest_accuracy.pdparams  # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u6700\u4f18\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\nbest_accuracy.pdopt     # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u6700\u4f18\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\nlatest.states\nlatest.pdparams  # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u65b0\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\nlatest.pdopt     # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u65b0\u200b\u6a21\u578b\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff0cbest_accuracy\u200b\u662f\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u8be5\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b</p> <pre><code># \u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\ncd /home/PaddleOCR/\n\npython3 tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#2-pp-ocrv3","title":"2. PP-OCRv3\u200b\u4e91\u7aef\u200b\u90e8\u7f72","text":"<p>PaddleCloud\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200bOperator\u200b\u673a\u5236\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u4e2a\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3001 \u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\uff0c \u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u7ec4\u4ef6\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5730\u200b\u5728\u200b\u4e91\u4e0a\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6a21\u578b\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bPaddleCloud\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u6863\u200b PaddleCloud\u200b\u67b6\u6784\u200b\u6982\u89c8\u200b \u3002</p> <p>\u200b\u9002\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200b\u591a\u673a\u200b\u90e8\u7f72\u200b\u73af\u5883\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#21","title":"2.1 \u200b\u5b89\u88c5\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6","text":""},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#_3","title":"\u73af\u5883\u200b\u8981\u6c42","text":"<ul> <li>Kubernetes v1.16+</li> <li>kubectl</li> <li>Helm</li> </ul> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6ca1\u6709\u200bKubernetes\u200b\u73af\u5883\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bMicroK8S\u200b\u5728\u200b\u672c\u5730\u200b\u642d\u5efa\u200b\u73af\u5883\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u8be6\u60c5\u8bf7\u200b\u53c2\u8003\u200b MicroK8S\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200bHelm\u200b\u4e00\u952e\u200b\u5b89\u88c5\u200b\u6240\u6709\u200b\u7ec4\u4ef6\u200b\u548c\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b</p> <pre><code># \u200b\u6dfb\u52a0\u200bPaddleCloud Chart\u200b\u4ed3\u5e93\u200b\n$ helm repo add paddlecloud https://paddleflow-public.hkg.bcebos.com/charts\n$ helm repo update\n\n# \u200b\u5b89\u88c5\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\n$ helm install pdc paddlecloud/paddlecloud --set tags.all-dep=true --namespace paddlecloud --create-namespace\n\n# \u200b\u68c0\u67e5\u200b\u6240\u6709\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u662f\u5426\u200b\u6210\u529f\u200b\u542f\u52a8\u200b\uff0c\u200b\u547d\u540d\u200b\u7a7a\u95f4\u200b\u4e0b\u200b\u7684\u200b\u6240\u6709\u200bPod\u200b\u90fd\u200b\u4e3a\u200bRuning\u200b\u72b6\u6001\u200b\u5219\u200b\u5b89\u88c5\u200b\u6210\u529f\u200b\u3002\n$ kubectl get pods -n paddlecloud\nNAME                                                 READY   STATUS    RESTARTS   AGE\npdc-hostpath-5b6bd6787d-bxvxg                        1/1     Running   0          10h\njuicefs-csi-node-pkldt                               3/3     Running   0          10h\njuicefs-csi-controller-0                             3/3     Running   0          10h\npdc-paddlecloud-sampleset-767bdf6947-pb6zm           1/1     Running   0          10h\npdc-paddlecloud-paddlejob-7cc8b7bfc6-7gqnh           1/1     Running   0          10h\npdc-minio-7cc967669d-824q5                           1/1     Running   0          10h\npdc-redis-master-0                                   1/1     Running   0          10h\n</code></pre> <p>\u200b\u66f4\u200b\u591a\u200b\u5b89\u88c5\u200b\u53c2\u6570\u200b\u8bf7\u200b\u53c2\u8003\u200bPaddleCloud\u200b\u5b89\u88c5\u200b\u6307\u5357\u200b</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#22","title":"2.2 \u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u4ecb\u7ecd","text":"<ul> <li>\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3002 \u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u4f7f\u7528\u200bJuiceFS\u200b\u4f5c\u4e3a\u200b\u7f13\u5b58\u200b\u5f15\u64ce\u200b\uff0c\u200b\u80fd\u591f\u200b\u5c06\u200b\u8fdc\u7a0b\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u5230\u200b\u8bad\u7ec3\u200b\u96c6\u7fa4\u200b\u672c\u5730\u200b\uff0c\u200b\u5927\u5e45\u200b\u52a0\u901f\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</li> <li>\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3002 \u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u53c2\u6570\u200b\u670d\u52a1\u5668\u200b\uff08PS\uff09\u200b\u4e0e\u200b\u96c6\u5408\u200b\u901a\u4fe1\u200b\uff08Collective\uff09\u200b\u4e24\u79cd\u200b\u67b6\u6784\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u5728\u200b\u4e91\u4e0a\u200b\u5feb\u901f\u200b\u8fd0\u884c\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</li> </ul> <p>\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e2d\u200b\u90e8\u7f72\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#23-hiertext","title":"2.3 \u200b\u51c6\u5907\u200bhiertext\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7f16\u5199\u200bSampleSet Yaml\u200b\u6587\u4ef6\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># hiertext.yaml\napiVersion: batch.paddlepaddle.org/v1alpha1\nkind: SampleSet\nmetadata:\n  name: hiertext\n  namespace: paddlecloud\nspec:\n  partitions: 1\n  source:\n    uri: bos://paddleflow-public.hkg.bcebos.com/ppocr/hiertext\n    secretRef:\n      name: none\n  secretRef:\n    name: data-center\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200bkubectl\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u3002</p> <pre><code># \u200b\u521b\u5efa\u200bhiertext\u200b\u6570\u636e\u200b\u96c6\u200b\n$ kubectl apply -f hiertext.yaml\nsampleset.batch.paddlepaddle.org/hiertext created\n\n# \u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u72b6\u6001\u200b\n$ kubectl get sampleset hiertext -n paddlecloud\nNAME       TOTAL SIZE   CACHED SIZE   AVAIL SPACE   RUNTIME   PHASE   AGE\nhiertext   3.3 GiB       3.2 GiB      12 GiB        1/1       Ready   11m\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#24-pp-ocrv3","title":"2.4 \u200b\u8bad\u7ec3\u200bPP-OCRv3\u200b\u6a21\u578b","text":"<p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e0a\u200b\u8bad\u7ec3\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\uff0c\u200b\u7f16\u5199\u200bPaddleJob Yaml\u200b\u6587\u4ef6\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># ppocrv3.yaml\napiVersion: batch.paddlepaddle.org/v1\nkind: PaddleJob\nmetadata:\n  name: ppocrv3\n  namespace: paddlecloud\nspec:\n  cleanPodPolicy: OnCompletion\n  sampleSetRef:\n    name: hiertext\n    namespace: paddlecloud\n    mountPath: /mnt/hiertext\n  worker:\n    replicas: 1\n    template:\n      spec:\n        containers:\n          - name: ppocrv3\n            image: paddlecloud/paddleocr:2.5-gpu-cuda10.2-cudnn7-efbb0a\n            command:\n              - /bin/bash\n            args:\n              - \"-c\"\n              - &gt;\n                mkdir /home/PaddleOCR/pre_train &amp;&amp;\n                wget -P ./pre_train https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar &amp;&amp;\n                tar xf ./pre_train/ch_PP-OCRv3_det_distill_train.tar -C ./pre_train/ &amp;&amp;\n                python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o\n                Train.dataset.data_dir=/mnt/\n                Train.dataset.label_file_list=[\\\"/mnt/hiertext/label_hiertext_train.txt\\\"]\n                Eval.dataset.data_dir=/mnt/\n                Eval.dataset.label_file_list=[\\\"/mnt/hiertext/label_hiertext_val.txt\\\"]\n                Global.save_model_dir=./output/\n                Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n            resources:\n              limits:\n                nvidia.com/gpu: 1\n            volumeMounts:  # \u200b\u6dfb\u52a0\u200b shared memory \u200b\u6302\u8f7d\u200b\u4ee5\u200b\u9632\u6b62\u200b\u7f13\u5b58\u200b\u51fa\u9519\u200b\n              - mountPath: /dev/shm\n                name: dshm\n        volumes:\n          - name: dshm\n            emptyDir:\n              medium: Memory\n</code></pre> <p>\u200b\u672c\u200b\u6848\u4f8b\u200b\u91c7\u7528\u200bGPU\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u53ea\u6709\u200bCPU\u200b\u673a\u5668\u200b\uff0c\u200b\u5219\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u955c\u50cf\u200b\u66ff\u6362\u6210\u200bCPU\u200b\u7248\u672c\u200b <code>paddlecloud/paddleocr:2.5-cpu-efbb0a</code>\uff0c\u200b\u5e76\u200b\u5728\u200bargs\u200b\u4e2d\u200b\u52a0\u4e0a\u200b\u53c2\u6570\u200b<code>Global.use_gpu=false</code>\u3002</p> <pre><code># \u200b\u521b\u5efa\u200bPaddleJob\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\n$ kubectl apply -f ppocrv3.yaml\npaddlejob.batch.paddlepaddle.org/ppocrv3 created\n\n# \u200b\u67e5\u770b\u200bPaddleJob\u200b\u72b6\u6001\u200b\n$ kubectl get pods -n paddlecloud -l paddle-res-name=ppocrv3-worker-0\nNAME               READY   STATUS    RESTARTS   AGE\nppocrv3-worker-0   1/1     Running   0          4s\n\n# \u200b\u67e5\u770b\u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\n$ kubectl logs -f ppocrv3-worker-0 -n paddlecloud\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_cloud.html#_4","title":"\u66f4\u200b\u591a\u200b\u8d44\u6e90","text":"<p>\u200b\u6b22\u8fce\u200b\u5173\u6ce8\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u9879\u76ee\u200bPaddleCloud\uff0c\u200b\u6211\u4eec\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u4ee5\u53ca\u200b\u5168\u6808\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u90e8\u7f72\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u6709\u200b\u4efb\u4f55\u200b\u5173\u4e8e\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u7684\u200b\u90e8\u7f72\u200b\u95ee\u9898\u200b\uff0c\u200b\u8bf7\u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002 \u200b\u5982\u679c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4efb\u4f55\u200bPaddleCloud\u200b\u5b58\u5728\u200b\u7684\u200b\u95ee\u9898\u200b\u6216\u8005\u200b\u662f\u200b\u5efa\u8bae\u200b, \u200b\u6b22\u8fce\u200b\u901a\u8fc7\u200bGitHub Issues\u200b\u7ed9\u200b\u6211\u4eec\u200b\u63d0\u200bissues\u3002</p>"},{"location":"en/ppocr/infer_deploy/paddle_js.html","title":"Paddle.js Introduction","text":"<p>Paddle.js is a web project for Baidu PaddlePaddle, which is an open source deep learning framework running in the browser. Paddle.js can either load a pre-trained model, or transforming a model from paddle-hub with model transforming tools provided by Paddle.js. It could run in every browser with WebGL/WebGPU/WebAssembly supported. It could also run in Baidu Smartprogram and wechat miniprogram.</p>"},{"location":"en/ppocr/infer_deploy/paddle_js.html#web-demo","title":"Web Demo","text":"<p>Run OCR demo in browser refer to tutorial.</p> demo web demo dicrctory visualization PP-OCRv3 TextDetection\u3001TextRecognition"},{"location":"en/ppocr/infer_deploy/paddle_js.html#mini-program-demo","title":"Mini Program Demo","text":"<p>The Mini Program demo running tutorial eference Run OCR demo in wechat miniprogram refer to tutorial.</p> demo directory Text Detection ocrdetecXcx Text Recognition ocrXcx <p></p>"},{"location":"en/ppocr/infer_deploy/paddle_server.html","title":"Sever Deployment","text":""},{"location":"en/ppocr/infer_deploy/paddle_server.html#ocr-pipeline-webservice","title":"OCR Pipeline WebService","text":"<p>PaddleOCR provides two service deployment methods:</p> <ul> <li>Based on PaddleHub Serving: Code path is \"<code>./deploy/hubserving</code>\". Please refer to the tutorial</li> <li>Based on PaddleServing: Code path is \"<code>./deploy/pdserving</code>\". Please follow this tutorial.</li> </ul>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#service-deployment-based-on-paddleserving","title":"Service deployment based on PaddleServing","text":"<p>This document will introduce how to use the PaddleServing to deploy the PPOCR dynamic graph model as a pipeline online service.</p> <p>Some Key Features of Paddle Serving:</p> <ul> <li>Integrate with Paddle training pipeline seamlessly, most paddle models can be deployed with one line command.</li> <li>Industrial serving features supported, such as models management, online loading, online A/B testing etc.</li> <li>Highly concurrent and efficient communication between clients and servers supported.</li> </ul> <p>PaddleServing supports deployment in multiple languages. In this example, two deployment methods, python pipeline and C++, are provided. The comparison between the two is as follows:</p> Language Speed Secondary development Do you need to compile C++ fast Slightly difficult Single model prediction does not need to be compiled, multi-model concatenation needs to be compiled python general easy single-model/multi-model no compilation required <p>The introduction and tutorial of Paddle Serving service deployment framework reference document.</p>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#environmental-preparation","title":"Environmental preparation","text":"<p>PaddleOCR operating environment and Paddle Serving operating environment are needed.</p> <ol> <li> <p>Please prepare PaddleOCR operating environment reference link.    Download the corresponding paddlepaddle whl package according to the environment, it is recommended to install version 2.2.2.</p> </li> <li> <p>The steps of PaddleServing operating environment prepare are as follows:</p> <pre><code># Install serving which used to start the service\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server_gpu-0.8.3.post102-py3-none-any.whl\npip3 install paddle_serving_server_gpu-0.8.3.post102-py3-none-any.whl\n\n# Install paddle-serving-server for cuda10.1\n# wget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl\n# pip3 install paddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl\n\n# Install serving which used to start the service\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_client-0.8.3-cp37-none-any.whl\npip3 install paddle_serving_client-0.8.3-cp37-none-any.whl\n\n# Install serving-app\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_app-0.8.3-py3-none-any.whl\npip3 install paddle_serving_app-0.8.3-py3-none-any.whl\n</code></pre> </li> </ol> <p>note: If you want to install the latest version of PaddleServing, refer to link.</p>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#model-conversion","title":"Model conversion","text":"<p>When using PaddleServing for service deployment, you need to convert the saved inference model into a serving model that is easy to deploy.</p> <p>Firstly, download the inference model of PPOCR</p> <pre><code># Download and unzip the OCR text detection model\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar -O ch_PP-OCRv3_det_infer.tar &amp;&amp; tar -xf ch_PP-OCRv3_det_infer.tar\n# Download and unzip the OCR text recognition model\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar -O ch_PP-OCRv3_rec_infer.tar &amp;&amp;  tar -xf ch_PP-OCRv3_rec_infer.tar\n</code></pre> <p>Then, you can use installed paddle_serving_client tool to convert inference model to mobile model.</p> <pre><code>#  Detection model conversion\npython3 -m paddle_serving_client.convert --dirname ./ch_PP-OCRv3_det_infer/ \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_det_v3_serving/ \\\n                                         --serving_client ./ppocr_det_v3_client/\n\n#  Recognition model conversion\npython3 -m paddle_serving_client.convert --dirname ./ch_PP-OCRv3_rec_infer/ \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_rec_v3_serving/  \\\n                                         --serving_client ./ppocr_rec_v3_client/\n</code></pre> <p>After the detection model is converted, there will be additional folders of <code>ppocr_det_v3_serving</code> and <code>ppocr_det_v3_client</code> in the current folder, with the following format:</p> <pre><code>|- ppocr_det_v3_serving/\n  |- __model__\n  |- __params__\n  |- serving_server_conf.prototxt\n  |- serving_server_conf.stream.prototxt\n\n|- ppocr_det_v3_client\n  |- serving_client_conf.prototxt\n  |- serving_client_conf.stream.prototxt\n</code></pre> <p>The recognition model is the same.</p>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#paddle-serving-pipeline-deployment","title":"Paddle Serving pipeline deployment","text":"<ol> <li> <p>Download the PaddleOCR code, if you have already downloaded it, you can skip this step.</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR\n\n# Enter the working directory\ncd PaddleOCR/deploy/pdserving/\n</code></pre> <p>The pdserver directory contains the code to start the pipeline service and send prediction requests, including:</p> <pre><code>__init__.py\nconfig.yml # Start the service configuration file\nocr_reader.py # OCR model pre-processing and post-processing code implementation\npipeline_http_client.py # Script to send pipeline prediction request\nweb_service.py # Start the script of the pipeline server\n</code></pre> </li> <li> <p>Run the following command to start the service.</p> <pre><code># Start the service and save the running log in log.txt\npython3 web_service.py --config=config.yml &amp;&gt;log.txt &amp;\n</code></pre> <p>After the service is successfully started, a log similar to the following will be printed in log.txt</p> <p></p> </li> <li> <p>Send service request</p> <pre><code>python3 pipeline_http_client.py\n</code></pre> <p>After successfully running, the predicted result of the model will be printed in the cmd window. An example of the result is:</p> <p></p> <p>Adjust the number of concurrency in config.yml to get the largest QPS. Generally, the number of concurrent detection and recognition is 2:1</p> <pre><code>det:\n    concurrency: 8\n    ...\nrec:\n    concurrency: 4\n    ...\n</code></pre> <p>Multiple service requests can be sent at the same time if necessary.</p> <p>The predicted performance data will be automatically written into the <code>PipelineServingLogs/pipeline.tracer</code> file.</p> <p>Tested on 200 real pictures, and limited the detection long side to 960. The average QPS on T4 GPU can reach around 23:</p> <pre><code>2021-05-13 03:42:36,895 ==================== TRACER ======================\n2021-05-13 03:42:36,975 Op(rec):\n2021-05-13 03:42:36,976         in[14.472382882882883 ms]\n2021-05-13 03:42:36,976         prep[9.556855855855856 ms]\n2021-05-13 03:42:36,976         midp[59.921905405405404 ms]\n2021-05-13 03:42:36,976         postp[15.345945945945946 ms]\n2021-05-13 03:42:36,976         out[1.9921216216216215 ms]\n2021-05-13 03:42:36,976         idle[0.16254943864471572]\n2021-05-13 03:42:36,976 Op(det):\n2021-05-13 03:42:36,976         in[315.4468035714286 ms]\n2021-05-13 03:42:36,976         prep[69.5980625 ms]\n2021-05-13 03:42:36,976         midp[18.989535714285715 ms]\n2021-05-13 03:42:36,976         postp[18.857803571428573 ms]\n2021-05-13 03:42:36,977         out[3.1337544642857145 ms]\n2021-05-13 03:42:36,977         idle[0.7477961159203756]\n2021-05-13 03:42:36,977 DAGExecutor:\n2021-05-13 03:42:36,977         Query count[224]\n2021-05-13 03:42:36,977         QPS[22.4 q/s]\n2021-05-13 03:42:36,977         Succ[0.9910714285714286]\n2021-05-13 03:42:36,977         Error req[169, 170]\n2021-05-13 03:42:36,977         Latency:\n2021-05-13 03:42:36,977                 ave[535.1678348214285 ms]\n2021-05-13 03:42:36,977                 .50[172.651 ms]\n2021-05-13 03:42:36,977                 .60[187.904 ms]\n2021-05-13 03:42:36,977                 .70[245.675 ms]\n2021-05-13 03:42:36,977                 .80[526.684 ms]\n2021-05-13 03:42:36,977                 .90[854.596 ms]\n2021-05-13 03:42:36,977                 .95[1722.728 ms]\n2021-05-13 03:42:36,977                 .99[3990.292 ms]\n2021-05-13 03:42:36,978 Channel (server worker num[10]):\n2021-05-13 03:42:36,978         chl0(In: ['@DAGExecutor'], Out: ['det']) size[0/0]\n2021-05-13 03:42:36,979         chl1(In: ['det'], Out: ['rec']) size[6/0]\n2021-05-13 03:42:36,979         chl2(In: ['rec'], Out: ['@DAGExecutor']) size[0/0]\n</code></pre> </li> </ol>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#c-serving","title":"C++ Serving","text":"<p>Service deployment based on python obviously has the advantage of convenient secondary development. However, the real application often needs to pursue better performance. PaddleServing also provides a more performant C++ deployment version.</p> <p>The C++ service deployment is the same as python in the environment setup and data preparation stages, the difference is when the service is started and the client sends requests.</p> <ol> <li>Compile Serving</li> </ol> <p>To improve predictive performance, C++ services also provide multiple model concatenation services. Unlike Python Pipeline services, multiple model concatenation requires the pre - and post-model processing code to be written on the server side, so local recompilation is required to generate serving. Specific may refer to the official document: how to compile Serving</p> <ol> <li> <p>Run the following command to start the service.</p> <pre><code># Start the service and save the running log in log.txt\npython3 -m paddle_serving_server.serve --model ppocr_det_v3_serving ppocr_rec_v3_serving --op GeneralDetectionOp GeneralInferOp --port 8181 &amp;&gt;log.txt &amp;\n</code></pre> <p>After the service is successfully started, a log similar to the following will be printed in log.txt </p> </li> <li> <p>Send service request</p> </li> </ol> <p>Due to the need for pre and post-processing in the C++Server part, in order to speed up the input to the C++Server is only the base64 encoded string of the picture, it needs to be manually modified    Change the feed_type field and shape field in ppocr_det_v3_client/serving_client_conf.prototxt to the following:</p> <pre><code> feed_var {\n name: \"x\"\n alias_name: \"x\"\n is_lod_tensor: false\n feed_type: 20\n shape: 1\n }\n</code></pre> <p>start the client:</p> <pre><code>```bash linenums=\"1\"\npython3 ocr_cpp_client.py ppocr_det_v3_client ppocr_rec_v3_client\n```\n\nAfter successfully running, the predicted result of the model will be printed in the cmd window. An example of the result is:\n\n![](./images/results.png)\n</code></pre>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#windows-users","title":"WINDOWS Users","text":"<p>Windows does not support Pipeline Serving, if we want to lauch paddle serving on Windows, we should use Web Service, for more infomation please refer to Paddle Serving for Windows Users</p> <p>WINDOWS user can only use version 0.5.0 CPU Mode</p> <p>Prepare Stage:</p> <pre><code>pip3 install paddle-serving-server==0.5.0\npip3 install paddle-serving-app==0.3.1\n</code></pre> <ol> <li> <p>Start Server</p> <pre><code>cd win\npython3 ocr_web_server.py gpu(for gpu user)\nor\npython3 ocr_web_server.py cpu(for cpu user)\n</code></pre> </li> <li> <p>Client Send Requests</p> <pre><code>python3 ocr_web_client.py\n</code></pre> </li> </ol>"},{"location":"en/ppocr/infer_deploy/paddle_server.html#faq","title":"FAQ","text":"<p>Q1: No result return after sending the request.</p> <p>A1: Do not set the proxy when starting the service and sending the request. You can close the proxy before starting the service and before sending the request. The command to close the proxy is:</p> <pre><code>unset https_proxy\nunset http_proxy\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html","title":"Python Inference for PP-OCR Model Zoo","text":"<p>This article introduces the use of the Python inference engine for the PP-OCR model library. The content is in order of text detection, text recognition, direction classifier and the prediction method of the three in series on the CPU and GPU.</p>"},{"location":"en/ppocr/infer_deploy/python_infer.html#text-detection-model-inference","title":"Text Detection Model Inference","text":"<p>The default configuration is based on the inference setting of the DB text detection model. For lightweight Chinese detection model inference, you can execute the following commands:</p> <pre><code># download DB text detection inference model\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\ntar xf ch_PP-OCRv3_det_infer.tar\n# run inference\npython3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\"\n</code></pre> <p>The visual text detection results are saved to the ./inference_results folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>You can use the parameters <code>limit_type</code> and <code>det_limit_side_len</code> to limit the size of the input image, The optional parameters of <code>limit_type</code> are [<code>max</code>, <code>min</code>], and <code>det_limit_size_len</code> is a positive integer, generally set to a multiple of 32, such as 960.</p> <p>The default setting of the parameters is <code>limit_type='max', det_limit_side_len=960</code>. Indicates that the longest side of the network input image cannot exceed 960, If this value is exceeded, the image will be resized with the same width ratio to ensure that the longest side is <code>det_limit_side_len</code>. Set as <code>limit_type='min', det_limit_side_len=960</code>, it means that the shortest side of the image is limited to 960.</p> <p>If the resolution of the input picture is relatively large and you want to use a larger resolution prediction, you can set det_limit_side_len to the desired value, such as 1216:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --det_limit_type=max --det_limit_side_len=1216\n</code></pre> <p>If you want to use the CPU for prediction, execute the command as follows</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\"  --use_gpu=False\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html#text-recognition-model-inference","title":"Text Recognition Model Inference","text":""},{"location":"en/ppocr/infer_deploy/python_infer.html#1-lightweight-chinese-recognition-model-inference","title":"1. Lightweight Chinese Recognition Model Inference","text":"<p>Note: The input shape used by the recognition model of <code>PP-OCRv3</code> is <code>3, 48, 320</code>. If you use other recognition models, you need to set the parameter <code>--rec_image_shape</code> according to the model. In addition, the <code>rec_algorithm</code> used by the recognition model of <code>PP-OCRv3</code> is <code>SVTR_LCNet</code> by default. Note the difference from the original <code>SVTR</code>.</p> <p>For lightweight Chinese recognition model inference, you can execute the following commands:</p> <pre><code># download CRNN text recognition inference model\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\ntar xf ch_PP-OCRv3_rec_infer.tar\n# run inference\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_10.png\" --rec_model_dir=\"./ch_PP-OCRv3_rec_infer/\" --rec_image_shape=3,48,320\n</code></pre> <p></p> <p>After executing the command, the prediction results (recognized text and score) of the above image will be printed on the screen.</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('PAIN', 0.988671)\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html#2-english-recognition-model-inference","title":"2. English Recognition Model Inference","text":"<p>For English recognition model inference, you can execute the following commands,you need to specify the dictionary path used by <code>--rec_char_dict_path</code>:</p> <pre><code># download en model\uff1a\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar\ntar xf en_PP-OCRv3_rec_infer.tar\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./en_PP-OCRv3_rec_infer/\" --rec_char_dict_path=\"ppocr/utils/en_dict.txt\"\n</code></pre> <p></p> <p>After executing the command, the prediction result of the above figure is:</p> <pre><code>Predicts of ./doc/imgs_words/en/word_1.png: ('JOINT', 0.998160719871521)\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html#3-multilingual-model-inference","title":"3. Multilingual Model Inference","text":"<p>If you need to predict other language models, when using inference model prediction, you need to specify the dictionary path used by <code>--rec_char_dict_path</code>. At the same time, in order to get the correct visualization results, You need to specify the visual font path through <code>--vis_font_path</code>. There are small language fonts provided by default under the <code>doc/fonts</code> path, such as Korean recognition:</p> <pre><code>wget wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/korean_mobile_v2.0_rec_infer.tar\n\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/korean/1.jpg\" --rec_model_dir=\"./your inference model\" --rec_char_dict_path=\"ppocr/utils/dict/korean_dict.txt\" --vis_font_path=\"doc/fonts/korean.ttf\"\n</code></pre> <p></p> <p>After executing the command, the prediction result of the above figure is:</p> <pre><code>Predicts of ./doc/imgs_words/korean/1.jpg:('\ubc14\ud0d5\uc73c\ub85c', 0.9948904)\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html#angle-classification-model-inference","title":"Angle Classification Model Inference","text":"<p>For angle classification model inference, you can execute the following commands:</p> <pre><code># download text angle class inference model\uff1a\nwget  https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\ntar xf ch_ppocr_mobile_v2.0_cls_infer.tar\npython3 tools/infer/predict_cls.py --image_dir=\"./doc/imgs_words_en/word_10.png\" --cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer\"\n</code></pre> <p></p> <p>After executing the command, the prediction results (classification angle and score) of the above image will be printed on the screen.</p> <pre><code> Predicts of ./doc/imgs_words_en/word_10.png:['0', 0.9999995]\n</code></pre>"},{"location":"en/ppocr/infer_deploy/python_infer.html#text-detection-angle-classification-and-recognition-inference-concatenation","title":"Text Detection Angle Classification and Recognition Inference Concatenation","text":"<p>Note: The input shape used by the recognition model of <code>PP-OCRv3</code> is <code>3, 48, 320</code>. If you use other recognition models, you need to set the parameter <code>--rec_image_shape</code> according to the model. In addition, the <code>rec_algorithm</code> used by the recognition model of <code>PP-OCRv3</code> is <code>SVTR_LCNet</code> by default. Note the difference from the original <code>SVTR</code>.</p> <p>When performing prediction, you need to specify the path of a single image or a folder of images through the parameter <code>image_dir</code>, pdf file is also supported, the parameter <code>det_model_dir</code> specifies the path to detect the inference model, the parameter <code>cls_model_dir</code> specifies the path to angle classification inference model and the parameter <code>rec_model_dir</code> specifies the path to identify the inference model. The parameter <code>use_angle_cls</code> is used to control whether to enable the angle classification model. The parameter <code>use_mp</code> specifies whether to use multi-process to infer <code>total_process_num</code> specifies process number when using multi-process. The parameter . The visualized recognition results are saved to the <code>./inference_results</code> folder by default.</p> <pre><code># use direction classifier\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --cls_model_dir=\"./cls/\" --rec_model_dir=\"./ch_PP-OCRv3_rec_infer/\" --use_angle_cls=true\n# not use use direction classifier\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --rec_model_dir=\"./ch_PP-OCRv3_rec_infer/\" --use_angle_cls=false\n# use multi-process\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --rec_model_dir=\"./ch_PP-OCRv3_rec_infer/\" --use_angle_cls=false --use_mp=True --total_process_num=6\n# use PDF files, you can infer the first few pages by using the `page_num` parameter, the default is 0, which means infer all pages\npython3 tools/infer/predict_system.py --image_dir=\"./xxx.pdf\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --cls_model_dir=\"./cls/\" --rec_model_dir=\"./ch_PP-OCRv3_rec_infer/\" --use_angle_cls=true --page_num=2\n</code></pre> <p>After executing the command, the recognition result image is as follows:</p> <p></p> <p>For more configuration and explanation of inference parameters, please refer to\uff1aModel Inference Parameters Explained Tutorial\u3002</p>"},{"location":"en/ppocr/infer_deploy/python_infer.html#tensorrt-inference","title":"TensorRT Inference","text":"<p>Paddle Inference ensembles TensorRT using subgraph mode. For GPU deployment scenarios, TensorRT can optimize some subgraphs, including horizontal and vertical integration of OPs, filter redundant OPs, and automatically select the optimal OP kernels for to speed up inference.</p> <p>You need to do the following 2 steps for inference using TRT.</p> <ul> <li>(1) Collect the dynamic shape information of the model about a specific dataset and store it in a file.</li> <li>(2) Load the dynamic shape information file for TRT inference.</li> </ul> <p>Taking the text detection model as an example. Firstly, you can use the following command to generate a dynamic shape file, which will eventually be named as <code>det_trt_dynamic_shape.txt</code> and stored in the <code>ch_PP-OCRv3_det_infer</code> folder.</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --use_tensorrt=True\n</code></pre> <p>The above command is only used to collect dynamic shape information, and TRT is not used during inference.</p> <p>Then, you can use the following command to perform TRT inference.</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./ch_PP-OCRv3_det_infer/\" --use_tensorrt=True\n</code></pre> <p>Note:</p> <ul> <li>In the first step, if the dynamic shape information file already exists, it does not need to be collected again. If you want to regenerate the dynamic shape information file, you need to delete the dynamic shape information file in the model folder firstly, and then regenerate it.</li> <li>In general, dynamic shape information file only needs to be generated once. In the actual deployment process, it is recommended that the dynamic shape information file can be generated on offline validation set or test set, and then the file can be directly loaded for online TRT inference.</li> </ul>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html","title":"Visual Studio 2019 Community CMake Compilation Guide","text":"<p>PaddleOCR is tested on Windows based on <code>Visual Studio 2019 Community</code>. Microsoft has supported direct management of <code>CMake</code> cross-platform compilation projects since <code>Visual Studio 2017</code>, but it was not until <code>2019</code> that stable and complete support was provided, so if you want to use CMake to manage project compilation and build, we recommend that you use the <code>Visual Studio 2019</code> environment to build.</p> <p>All the examples below are demonstrated with the working directory as <code>D:\\projects\\cpp</code>.</p>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#1-environment-preparation","title":"1. Environment Preparation","text":""},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#11-install-the-required-environment","title":"1.1 Install the required environment","text":"<ul> <li>Visual Studio 2019</li> <li>CUDA 10.2, cudnn 7+ (only required when using the GPU version of the prediction library)</li> <li>CMake 3.22+</li> </ul> <p>Please make sure the system has the above basic software installed. We use the community version of <code>VS2019</code>.</p>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#12-download-paddlepaddle-c-prediction-library-and-opencv","title":"1.2 Download PaddlePaddle C++ prediction library and Opencv","text":""},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#121-download-paddlepaddle-c-prediction-library","title":"1.2.1 Download PaddlePaddle C++ prediction library","text":"<p>PaddlePaddle C++ prediction library provides different precompiled versions for different <code>CPU</code> and <code>CUDA</code> versions. Please download according to the actual situation: C++ prediction library download list</p> <p>After decompression, the <code>D:\\projects\\paddle_inference</code> directory contains the following contents:</p> <pre><code>paddle_inference\n\u251c\u2500\u2500 paddle # paddle core library and header files\n|\n\u251c\u2500\u2500 third_party # third-party dependent libraries and header files\n|\n\u2514\u2500\u2500 version.txt # version and compilation information\n</code></pre>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#122-install-and-configure-opencv","title":"1.2.2 Install and configure OpenCV","text":"<ol> <li>Download Opencv for Windows platform from the OpenCV official website, Download address</li> <li>Run the downloaded executable file and unzip OpenCV to the specified directory, such as <code>D:\\projects\\cpp\\opencv</code></li> </ol>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#123-download-paddleocr-code","title":"1.2.3 Download PaddleOCR code","text":"<pre><code>git clone -b dygraph https://github.com/PaddlePaddle/PaddleOCR\n</code></pre>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#2-start-running","title":"2. Start running","text":""},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#step1-build-visual-studio-project","title":"Step1: Build Visual Studio project","text":"<p>After cmake is installed, there will be a cmake-gui program in the system. Open cmake-gui, fill in the source code path in the first input box, and fill in the compilation output path in the second input box</p> <p></p>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#step2-execute-cmake-configuration","title":"Step2: Execute cmake configuration","text":"<p>Click the <code>Configure</code> button at the bottom of the interface. The first click will pop up a prompt box for Visual Studio configuration, as shown below. Select your Visual Studio version is fine, and the target platform is x64. Then click the <code>finish</code> button to start the automatic configuration.</p> <p></p> <p>The first execution will report an error, which is normal. Next, configure Opencv and the prediction library</p> <ul> <li> <p>For cpu version, only the three parameters OPENCV_DIR, OpenCV_DIR, and PADDLE_LIB need to be considered</p> </li> <li> <p>OPENCV_DIR: Fill in the location of the opencv lib folder</p> </li> <li> <p>OpenCV_DIR: Fill in the location of the opencv lib folder</p> </li> <li> <p>PADDLE_LIB: The location of the paddle_inference folder</p> </li> <li> <p>For GPU version, on the basis of the cpu version, the following variables need to be filled in CUDA_LIB, CUDNN_LIB, TENSORRT_DIR, WITH_GPU, WITH_TENSORRT</p> </li> <li> <p>CUDA_LIB: CUDA address, such as <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\lib\\x64</code></p> </li> <li> <p>CUDNN_LIB: The same as CUDA_LIB</p> </li> <li> <p>TENSORRT_DIR: The location where TRT is unzipped after downloading, such as <code>D:\\TensorRT-8.0.1.6</code></p> </li> <li>WITH_GPU: Check</li> <li>WITH_TENSORRT: Check</li> </ul> <p>The configured screenshot is as follows</p> <p></p> <p>After the configuration is completed, click the <code>Configure</code> button again.</p> <p>Note:</p> <ol> <li>If you are using the <code>openblas</code> version, please uncheck <code>WITH_MKL</code></li> <li>If you encounter the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.</code>, change the github address in <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to https://gitee.com/Double_V/AutoLog address.</li> </ol>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#step3-generate-visual-studio-project","title":"Step3: Generate Visual Studio Project","text":"<p>Click the <code>Generate</code> button to generate the sln file of the Visual Studio project. </p> <p>Click the <code>Open Project</code> button to open the project in Visual Studio. The screenshot after opening is as follows</p> <p></p> <p>Before starting to generate the solution, perform the following steps:</p> <ol> <li> <p>Change <code>Debug</code> to <code>Release</code></p> </li> <li> <p>Download dirent.h and copy it to the include folder of Visual Studio, such as <code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\VS\\include</code>.</p> </li> </ol> <p>Click <code>Build-&gt;Generate Solution</code>, and you can see the <code>ppocr.exe</code> file in the <code>build/Release/</code> folder.</p> <p>Before running, copy the following files to the <code>build/Release/</code> folder</p> <ol> <li> <p><code>paddle_inference/paddle/lib/paddle_inference.dll</code></p> </li> <li> <p><code>paddle_inference/third_party/install/onnxruntime/lib/onnxruntime.dll</code></p> </li> <li> <p><code>paddle_inference/third_party/install/paddle2onnx/lib/paddle2onnx.dll</code></p> </li> <li> <p><code>opencv/build/x64/vc15/bin/opencv_world455.dll</code></p> </li> <li> <p>If you use the prediction library of the openblas version, you also need to copy <code>paddle_inference/third_party/install/openblas/lib/openblas.dll</code></p> </li> </ol>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#step4-prediction","title":"Step4: Prediction","text":"<p>The above `Visual Studio The executable file compiled by 2019 is in the directory of build/Release/. Open cmd and switch to D:\\projects\\cpp\\PaddleOCR\\deploy\\cpp_infer:</p> <p>cd /d D:\\projects\\cpp\\PaddleOCR\\deploy\\cpp_infer</p> <p>The executable file ppocr.exe is the sample prediction program. Its main usage is as follows. For more usage, please refer to the Instructions section of running demo.</p> <pre><code># Switch terminal encoding to utf8\nCHCP 65001\n# Execute prediction\n.\\build\\Release\\ppocr.exe system --det_model_dir=D:\\projects\\cpp\\ch_PP-OCRv2_det_slim_quant_infer --rec_model_dir=D:\\projects\\cpp\\ch_PP-OCRv2_rec_slim_quant_infer --image_dir=D:\\projects\\cpp\\PaddleOCR\\doc\\imgs\\11.jpg\n</code></pre> <p>The recognition result is as follows </p>"},{"location":"en/ppocr/infer_deploy/windows_vs2019_build.html#faq","title":"FAQ","text":"<ul> <li>When running, a pop-up window prompts <code>The application cannot be started normally (0xc0000142)</code>, and the <code>cmd</code> window prompts <code>You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found.</code>, copy all the dll files in the lib in the tensor directory to the release directory, and run it again.</li> </ul>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html","title":"Knowledge Distillation","text":""},{"location":"en/ppocr/model_compress/knowledge_distillation.html#1-introduction","title":"1. Introduction","text":""},{"location":"en/ppocr/model_compress/knowledge_distillation.html#11-introduction-to-knowledge-distillation","title":"1.1 Introduction to Knowledge Distillation","text":"<p>In recent years, deep neural networks have been proved to be an extremely effective method for solving problems in the fields of computer vision and natural language processing. By constructing a suitable neural network and training it, the performance metrics of the final network model will basically exceed the traditional algorithm. When the amount of data is large enough, increasing the amount of parameters by constructing a reasonable network model can significantly improve the performance of the model, but this brings about the problem of a sharp increase in the complexity of the model. Large models are more expensive to use in actual scenarios. Deep neural networks generally have more parameter redundancy. At present, there are several main methods to compress the model and reduce the amount of its parameters. Such as pruning, quantification, knowledge distillation, etc., where knowledge distillation refers to the use of teacher models to guide student models to learn specific tasks, to ensure that the small model obtains a relatively large performance improvement under the condition of unchanged parameters. In addition, in the knowledge distillation task, a mutual learning model training method was also derived. The paper Deep Mutual Learning pointed out that using two identical models to supervise each other during the training process can achieve better results than a single model training.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#12-introduction-to-paddleocr-knowledge-distillation","title":"1.2 Introduction to PaddleOCR Knowledge Distillation","text":"<p>Whether it is a large model distilling a small model, or a small model learning from each other and updating parameters, they are essentially the output between different models or mutual supervision between feature maps. The only difference is (1) whether the model requires fixed parameters. (2) Whether the model needs to be loaded with a pre-trained model. For the case where a large model distills a small model, the large model generally needs to load the pre-trained model and fix the parameters. For the situation where small models distill each other, the small models generally do not load the pre-trained model, and the parameters are also in a learnable state.</p> <p>In the task of knowledge distillation, it is not only the distillation between two models, but also the situation where multiple models learn from each other. Therefore, in the knowledge distillation code framework, it is also necessary to support this type of distillation method.</p> <p>The algorithm of knowledge distillation is integrated in PaddleOCR. Specifically, it has the following main features:</p> <ul> <li>It supports mutual learning of any network, and does not require the sub-network structure to be completely consistent or to have a pre-trained model. At the same time, there is no limit to the number of sub-networks, just add it in the configuration file.</li> <li>Support arbitrarily configuring the loss function through the configuration file, not only can use a certain loss, but also a combination of multiple losses.</li> <li>Support all model-related environments such as knowledge distillation training, prediction, evaluation, and export, which is convenient for use and deployment.</li> </ul> <p>Through knowledge distillation, in the common Chinese and English text recognition task, without adding any time-consuming prediction, the accuracy of the model can be improved by more than 3%. Combining the learning rate adjustment strategy and the model structure fine-tuning strategy, the final improvement is more than 5%.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#2-configuration-file-analysis","title":"2. Configuration File Analysis","text":"<p>In the process of knowledge distillation training, there is no change in data preprocessing, optimizer, learning rate, and some global attributes. The configuration files of the model structure, loss function, post-processing, metric calculation and other modules need to be fine-tuned.</p> <p>The following takes the knowledge distillation configuration file for recognition and detection as an example to analyze the training and configuration of knowledge distillation.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#21-recognition-model-configuration-file-analysis","title":"2.1 Recognition Model Configuration File Analysis","text":"<p>The configuration file is in ch_PP-OCRv2_rec_distillation.yml.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#211-model-structure","title":"2.1.1 Model Structure","text":"<p>In the knowledge distillation task, the model structure configuration is as follows.</p> <pre><code>Architecture:\n  model_type: &amp;model_type \"rec\"    # Model category, recognition, detection, etc.\n  name: DistillationModel          # Structure name, in the distillation task, it is DistillationModel\n  algorithm: Distillation          # Algorithm name\n  Models:                          # Model, including the configuration information of the subnet\n    Teacher:                       # The name of the subnet, it must include at least the `pretrained` and `freeze_params` parameters, and the other parameters are the construction parameters of the subnet\n      pretrained:                  # Does this sub-network need to load pre-training weights\n      freeze_params: false         # Do you need fixed parameters\n      return_all_feats: true       # Do you need to return all features, if it is False, only the final output is returned\n      model_type: *model_type      # Model category\n      algorithm: SVTR              # The algorithm name of the sub-network. The remaining parameters of the sub-network are consistent with the general model training configuration\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student:                       # Another sub-network, here is a distillation example of DML, the two sub-networks have the same structure, and both need to learn parameters\n      pretrained:                  # The following parameters are the same as above\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n</code></pre> <p>If you want to add more sub-networks for training, you can also add the corresponding fields in the configuration file according to the way of adding <code>Student</code> and <code>Teacher</code>. For example, if you want 3 models to supervise each other and train together, then <code>Architecture</code> can be written in the following format.</p> <pre><code>Architecture:\n  model_type: &amp;model_type \"rec\"\n  name: DistillationModel\n  algorithm: Distillation\n  Models:\n    Teacher:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student2:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n</code></pre> <p>When the model is finally trained, it contains 3 sub-networks: <code>Teacher</code>, <code>Student</code>, <code>Student2</code>.</p> <p>The specific implementation code of the <code>DistillationModel</code> class can refer to distillation_model.py. The final model output is a dictionary, the key is the name of all the sub-networks, for example, here are <code>Student</code> and <code>Teacher</code>, and the value is the output of the corresponding sub-network, which can be <code>Tensor</code> (only the last layer of the network is returned) and <code>dict</code> (also returns the characteristic information in the middle). In the recognition task, in order to add more loss functions and ensure the scalability of the distillation method, the output of each sub-network is saved as a <code>dict</code>, which contains the sub-module output. Take the recognition model as an example. The output result of each sub-network is <code>dict</code>, the key contains <code>backbone_out</code>, <code>neck_out</code>, <code>head_out</code>, and <code>value</code> is the tensor of the corresponding module. Finally, for the above configuration file, <code>DistillationModel</code> The output format is as follows.</p> <pre><code>{\n  \"Teacher\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  },\n  \"Student\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  }\n}\n</code></pre>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#212-loss-function","title":"2.1.2 Loss Function","text":"<p>In the knowledge distillation task, the loss function configuration is as follows.</p> <pre><code>Loss:\n  name: CombinedLoss                           # Loss function name\n  loss_config_list:                            # List of loss function configuration files, mandatory functions for CombinedLoss\n  - DistillationCTCLoss:                       # CTC loss function based on distillation, inherited from standard CTC loss\n      weight: 1.0                              # The weight of the loss function. In loss_config_list, each loss function must include this field\n      model_name_list: [\"Student\", \"Teacher\"]  # For the prediction results of the distillation model, extract the output of these two sub-networks and calculate the CTC loss with gt\n      key: head_out                            # In the sub-network output dict, take the corresponding tensor\n  - DistillationDMLLoss:                       # DML loss function, inherited from the standard DMLLoss\n      weight: 1.0\n      act: \"softmax\"                           # Activation function, use it to process the input, can be softmax, sigmoid or None, the default is None\n      model_name_pairs:                        # The subnet name pair used to calculate DML loss. If you want to calculate the DML loss of other subnets, you can continue to add it below the list\n      - [\"Student\", \"Teacher\"]\n      key: head_out\n      multi_head: True                         # whether to use mult_head\n      dis_head: ctc                            # assign the head name to calculate loss\n      name: dml_ctc                            # prefix name of the loss\n  - DistillationDMLLoss:                       # DML loss function, inherited from the standard DMLLoss\n      weight: 0.5\n      act: \"softmax\"                           # Activation function, use it to process the input, can be softmax, sigmoid or None, the default is None\n      model_name_pairs:                        # The subnet name pair used to calculate DML loss. If you want to calculate the DML loss of other subnets, you can continue to add it below the list\n      - [\"Student\", \"Teacher\"]\n      key: head_out\n      multi_head: True                         # whether to use mult_head\n      dis_head: sar                            # assign the head name to calculate loss\n      name: dml_sar                            # prefix name of the loss\n  - DistillationDistanceLoss:                  # Distilled distance loss function\n      weight: 1.0\n      mode: \"l2\"                               # Support l1, l2 or smooth_l1\n      model_name_pairs:                        # Calculate the distance loss of the subnet name pair\n      - [\"Student\", \"Teacher\"]\n      key: backbone_out\n  - DistillationSARLoss:                       # SAR loss function based on distillation, inherited from standard SAR loss\n      weight: 1.0                              # The weight of the loss function. In loss_config_list, each loss function must include this field\n      model_name_list: [\"Student\", \"Teacher\"]  # For the prediction results of the distillation model, extract the output of these two sub-networks and calculate the SAR loss with gt\n      key: head_out                            # In the sub-network output dict, take the corresponding tensor\n      multi_head: True                         # whether it is multi-head or not, if true, SAR branch is used to calculate the loss\n</code></pre> <p>Among the above loss functions, all distillation loss functions are inherited from the standard loss function class. The main functions are: Analyze the output of the distillation model, find the intermediate node (tensor) used to calculate the loss, and then use the standard loss function class to calculate.</p> <p>Taking the above configuration as an example, the final distillation training loss function contains the following five parts.</p> <ul> <li>CTC branch of the final output <code>head_out</code> for <code>Student</code> and <code>Teacher</code> calculates the CTC loss with gt (loss weight equals 1.0). Here, because both sub-networks need to update the parameters, both of them need to calculate the loss with gt.</li> <li>SAR branch of the final output <code>head_out</code> for <code>Student</code> and <code>Teacher</code> calculates the SAR loss with gt (loss weight equals 1.0). Here, because both sub-networks need to update the parameters, both of them need to calculate the loss with gt.</li> <li>DML loss between CTC branch of  <code>Student</code> and <code>Teacher</code>'s final output <code>head_out</code> (loss weight equals 1.0).</li> <li>DML loss between SAR branch of <code>Student</code> and <code>Teacher</code>'s final output <code>head_out</code> (loss weight equals 0.5).</li> <li>L2 loss between <code>Student</code> and <code>Teacher</code>'s backbone network output <code>backbone_out</code> (loss weight equals 1.0).</li> </ul> <p>For more specific implementation of <code>CombinedLoss</code>, please refer to: combined_loss.py. For more specific implementations of distillation loss functions such as <code>DistillationCTCLoss</code>, please refer to distillation_loss.py</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#213-post-processing","title":"2.1.3 Post-processing","text":"<p>In the knowledge distillation task, the post-processing configuration is as follows.</p> <pre><code>PostProcess:\n  name: DistillationCTCLabelDecode       # CTC decoding post-processing of distillation tasks, inherited from the standard CTCLabelDecode class\n  model_name: [\"Student\", \"Teacher\"]     # For the prediction results of the distillation model, extract the outputs of these two sub-networks and decode them\n  key: head_out                          # Take the corresponding tensor in the subnet output dict\n  multi_head: True                       # whether it is multi-head or not, if true, CTC branch is used to calculate the loss\n</code></pre> <p>Taking the above configuration as an example, the CTC decoding output of the two sub-networks <code>Student</code> and <code>Teahcer</code> will be calculated at the same time. Among them, <code>key</code> is the name of the subnet, and <code>value</code> is the list of subnets.</p> <p>For more specific implementation of <code>DistillationCTCLabelDecode</code>, please refer to: rec_postprocess.py</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#214-metric-calculation","title":"2.1.4 Metric Calculation","text":"<p>In the knowledge distillation task, the metric calculation configuration is as follows.</p> <pre><code>Metric:\n  name: DistillationMetric         # CTC decoding post-processing of distillation tasks, inherited from the standard CTCLabelDecode class\n  base_metric_name: RecMetric      # The base class of indicator calculation. For the output of the model, the indicator will be calculated based on this class\n  main_indicator: acc              # The name of the indicator\n  key: \"Student\"                   # Select the main_indicator of this subnet as the criterion for saving the best model\n  ignore_space: False              # whether to ignore space during evaulation\n</code></pre> <p>Taking the above configuration as an example, the accuracy metric of the <code>Student</code> subnet will be used as the judgment metric for saving the best model. At the same time, the accuracy metric of all subnets will be printed out in the log.</p> <p>For more specific implementation of <code>DistillationMetric</code>, please refer to: distillation_metric.py.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#215-fine-tuning-distillation-model","title":"2.1.5 Fine-tuning Distillation Model","text":"<p>There are two ways to fine-tune the recognition distillation task.</p> <ol> <li>Fine-tuning based on knowledge distillation: this situation is relatively simple, download the pre-trained model. Then configure the pre-training model path and your own data path in ch_PP-OCRv2_rec_distillation.yml to perform fine-tuning training of the model.</li> <li> <p>Do not use knowledge distillation in fine-tuning: In this case, you need to first extract the student model parameters from the pre-training model. The specific steps are as follows.</p> </li> <li> <p>First download the pre-trained model and unzip it.</p> </li> </ol> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\ntar -xf ch_PP-OCRv3_rec_train.tar\n</code></pre> <ul> <li>Then use python to extract the student model parameters</li> </ul> <pre><code>import paddle\n# Load the pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_rec_train/best_accuracy.pdparams\")\n# View the keys of the weight parameter\nprint(all_params.keys())\n# Weight extraction of student model\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the weight parameters of the student model\nprint(s_params.keys())\n# Save weight parameters\npaddle.save(s_params, \"ch_PP-OCRv3_rec_train/student.pdparams\")\n</code></pre> <p>After the extraction is complete, use ch_PP-OCRv3_rec.yml to modify the path of the pre-trained model (the path of the exported <code>student.pdparams</code> model) and your own data path to fine-tune the model.</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#22-detection-model-configuration-file-analysis","title":"2.2 Detection Model Configuration File Analysis","text":"<p>The configuration file of the detection model distillation is in the <code>PaddleOCR/configs/det/ch_PP-OCRv3/</code> directory, which contains three distillation configuration files:</p> <ul> <li><code>ch_PP-OCRv3_det_cml.yml</code>, Use one large model to distill two small models, and the two small models learn from each other</li> <li><code>ch_PP-OCRv3_det_dml.yml</code>, Method of mutual distillation of two student models</li> </ul>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#221-model-structure","title":"2.2.1 Model Structure","text":"<p>In the knowledge distillation task, the model structure configuration is as follows:</p> <pre><code>Architecture:\n  name: DistillationModel          # Structure name, in the distillation task, it is DistillationModel\n  algorithm: Distillation          # Algorithm name\n  Models:                          # Model, including the configuration information of the subnet\n    Student:                       # The name of the subnet, it must include at least the `pretrained` and `freeze_params` parameters, and the other parameters are the construction parameters of the subnet\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained  # Does this sub-network need to load pre-training weights\n      freeze_params: false         # Do you need fixed parameters\n      return_all_feats: false      # Do you need to return all features, if it is False, only the final output is returned\n      model_type: det\n      algorithm: DB\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n    Teacher:                      # Another sub-network, here is a distillation example of a large model distill a small model\n      pretrained: ./pretrain_models/ch_ppocr_server_v2.0_det_train/best_accuracy\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n</code></pre> <p>If DML is used, that is, the method of two small models learning from each other, the Teacher network structure in the above configuration file needs to be set to the same configuration as the Student model. Refer to the configuration file for details. ch_PP-OCRv3_det_dml.yml</p> <p>The following describes the configuration file parameters ch_PP-OCRv3_det_cml.yml:</p> <pre><code>Architecture:\n  name: DistillationModel\n  algorithm: Distillation\n  model_type: det\n  Models:\n    Teacher:                         # Teacher model configuration of CML distillation\n      pretrained: ./pretrain_models/ch_ppocr_server_v2.0_det_train/best_accuracy\n      freeze_params: true            # Teacher does not train\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n    Student:                         # Student model configuration for CML distillation\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n      freeze_params: false\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Backbone:\n        name: MobileNetV3\n        scale: 0.5\n        model_name: large\n        disable_se: true\n      Neck:\n        name: RSEFPN\n        out_channels: 96\n        shortcut: True\n      Head:\n        name: DBHead\n        k: 50\n    Student2:                          # Student2 model configuration for CML distillation\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n      freeze_params: false\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: MobileNetV3\n        scale: 0.5\n        model_name: large\n        disable_se: true\n      Neck:\n        name: RSEFPN\n        out_channels: 96\n        shortcut: True\n      Head:\n        name: DBHead\n        k: 50\n</code></pre> <p>The specific implementation code of the distillation model <code>DistillationModel</code> class can refer to distillation_model.py.</p> <p>The final model output is a dictionary, the key is the name of all the sub-networks, for example, here are <code>Student</code> and <code>Teacher</code>, and the value is the output of the corresponding sub-network, which can be <code>Tensor</code> (only the last layer of the network is returned) and <code>dict</code> (also returns the characteristic information in the middle).</p> <p>In the distillation task, in order to facilitate the addition of the distillation loss function, the output of each network is saved as a <code>dict</code>, which contains the sub-module output. The key contains <code>backbone_out</code>, <code>neck_out</code>, <code>head_out</code>, and <code>value</code> is the tensor of the corresponding module. Finally, for the above configuration file, the output format of <code>DistillationModel</code> is as follows.</p> <pre><code>{\n  \"Teacher\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  },\n  \"Student\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  }\n}\n</code></pre>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#222-loss-function","title":"2.2.2 Loss Function","text":"<p>The distillation loss function configuration(<code>ch_PP-OCRv3_det_cml.yml</code>) is shown below.</p> <pre><code>Loss:\n  name: CombinedLoss\n  loss_config_list:\n  - DistillationDilaDBLoss:\n      weight: 1.0\n      model_name_pairs:\n      - [\"Student\", \"Teacher\"]\n      - [\"Student2\", \"Teacher\"]                  # 1. Calculate the loss of two Student and Teacher\n      key: maps\n      balance_loss: true\n      main_loss_type: DiceLoss\n      alpha: 5\n      beta: 10\n      ohem_ratio: 3\n  - DistillationDMLLoss:                         # 2. Add to calculate the loss between two students\n      model_name_pairs:\n      - [\"Student\", \"Student2\"]\n      maps_name: \"thrink_maps\"\n      weight: 1.0\n      # act: None\n      key: maps\n  - DistillationDBLoss:\n      weight: 1.0\n      model_name_list: [\"Student\", \"Student2\"]   # 3. Calculate the loss between two students and GT\n      balance_loss: true\n      main_loss_type: DiceLoss\n      alpha: 5\n      beta: 10\n      ohem_ratio: 3\n</code></pre> <p>For more specific implementation of <code>DistillationDilaDBLoss</code>, please refer to: distillation_loss.py. For more specific implementations of distillation loss functions such as <code>DistillationDBLoss</code>, please refer to: distillation_loss.py</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#223-post-processing","title":"2.2.3 Post-processing","text":"<p>In the task of detecting knowledge distillation, the post-processing configuration of detecting distillation is as follows.</p> <pre><code>PostProcess:\n  name: DistillationDBPostProcess                  # The post-processing of the DB detection distillation task, inherited from the standard DBPostProcess class\n  model_name: [\"Student\", \"Student2\", \"Teacher\"]   # Extract the output of multiple sub-networks and decode them. The network that does not require post-processing is not set in model_name\n  thresh: 0.3\n  box_thresh: 0.6\n  max_candidates: 1000\n  unclip_ratio: 1.5\n</code></pre> <p>Taking the above configuration as an example, the output of the three subnets <code>Student</code>, <code>Student2</code> and <code>Teacher</code> will be calculated at the same time for post-processing calculations. Since there are multiple inputs, there are also multiple outputs returned by post-processing. For a more specific implementation of <code>DistillationDBPostProcess</code>, please refer to: db_postprocess.py</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#224-metric-calculation","title":"2.2.4 Metric Calculation","text":"<p>In the knowledge distillation task, the metric calculation configuration is as follows.</p> <pre><code>Metric:\n  name: DistillationMetric\n  base_metric_name: DetMetric\n  main_indicator: hmean\n  key: \"Student\"\n</code></pre> <p>Since distillation needs to include multiple networks, only one network metrics needs to be calculated when calculating the metrics. The <code>key</code> field is set to <code>Student</code>, it means that only the metrics of the <code>Student</code> network is calculated. Model Structure</p>"},{"location":"en/ppocr/model_compress/knowledge_distillation.html#225-fine-tuning-distillation-model","title":"2.2.5 Fine-tuning Distillation Model","text":"<p>There are three ways to fine-tune the detection distillation task:</p> <ul> <li><code>ch_PP-OCRv3_det_distill.yml</code>, The teacher model is set to the model provided by PaddleOCR or the large model you have trained.</li> <li><code>ch_PP-OCRv3_det_cml.yml</code>, Use cml distillation. Similarly, the Teacher model is set to the model provided by PaddleOCR or the large model you have trained.</li> <li><code>ch_PP-OCRv3_det_dml.yml</code>, Distillation using DML. The method of mutual distillation of the two Student models has an accuracy improvement of about 1.7% on the data set used by PaddleOCR.</li> </ul> <p>In fine-tune, you need to set the pre-trained model to be loaded in the <code>pretrained</code> parameter of the network structure.</p> <p>In terms of accuracy improvement, <code>cml</code> &gt; <code>dml</code> &gt; <code>distill</code>. When the amount of data is insufficient or the accuracy of the teacher model is similar to that of the student, this conclusion may change.</p> <p>In addition, since the distillation pre-training model provided by PaddleOCR contains multiple model parameters, if you want to extract the parameters of the student model, you can refer to the following code:</p> <pre><code># Download the parameters of the distillation training model\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\n</code></pre> <pre><code>import paddle\n# Load the pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# View the keys of the weight parameter\nprint(all_params.keys())\n# Extract the weights of the student model\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the weight parameters of the student model\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"ch_PP-OCRv3_det_distill_train/student.pdparams\")\n</code></pre> <p>Finally, the parameters of the student model will be saved in <code>ch_PP-OCRv3_det_distill_train/student.pdparams</code> for the fine-tune of the model.</p>"},{"location":"en/ppocr/model_compress/prune.html","title":"PP-OCR Models Pruning","text":"<p>Generally, a more complex model would achieve better performance in the task, but it also leads to some redundancy in the model. Model Pruning is a technique that reduces this redundancy by removing the sub-models in the neural network model, so as to reduce model calculation complexity and improve model inference performance.</p> <p>This example uses PaddleSlim providedAPIs of Pruning to compress the OCR model. PaddleSlim, an open source library which integrates model pruning, quantization (including quantization training and offline quantization), distillation, neural network architecture search, and many other commonly used and leading model compression technique in the industry.</p> <p>It is recommended that you could understand following pages before reading this example\uff1a</p> <ol> <li>PaddleOCR training methods</li> <li>The demo of prune</li> </ol>"},{"location":"en/ppocr/model_compress/prune.html#quick-start","title":"Quick start","text":""},{"location":"en/ppocr/model_compress/prune.html#1-install-paddleslim","title":"1. Install PaddleSlim","text":"<pre><code>git clone https://github.com/PaddlePaddle/PaddleSlim.git\ncd PaddleSlim\ngit checkout develop\npython3 setup.py install\n</code></pre>"},{"location":"en/ppocr/model_compress/prune.html#2-download-pre-trained-model","title":"2. Download Pre-trained Model","text":"<p>Model prune needs to load pre-trained models. PaddleOCR also provides a series of models. Developers can choose their own models or use their own models according to their needs.</p>"},{"location":"en/ppocr/model_compress/prune.html#3-pruning-sensitivity-analysis","title":"3. Pruning sensitivity analysis","text":"<p>After the pre-trained model is loaded, sensitivity analysis is performed on each network layer of the model to understand the redundancy of each network layer, and save a sensitivity file which named: sen.pickle.  After that, user could load the sensitivity file via the methods provided by PaddleSlim and determining the pruning ratio of each network layer automatically. For specific details of sensitivity analysis, see\uff1aSensitivity analysis The data format of sensitivity file\uff1a</p> <pre><code>sen.pickle(Dict){\n              'layer_weight_name_0': sens_of_each_ratio(Dict){'pruning_ratio_0': acc_loss, 'pruning_ratio_1': acc_loss}\n              'layer_weight_name_1': sens_of_each_ratio(Dict){'pruning_ratio_0': acc_loss, 'pruning_ratio_1': acc_loss}\n          }\n</code></pre> <p>example\uff1a</p> <pre><code>{\n    'conv10_expand_weights': {0.1: 0.006509952684312718, 0.2: 0.01827734339798862, 0.3: 0.014528405644659832, 0.6: 0.06536008804270439, 0.8: 0.11798612250664964, 0.7: 0.12391408417493704, 0.4: 0.030615754498018757, 0.5: 0.047105205602406594}\n    'conv10_linear_weights': {0.1: 0.05113190831455035, 0.2: 0.07705573833558801, 0.3: 0.12096721757739311, 0.6: 0.5135061352930738, 0.8: 0.7908166677143281, 0.7: 0.7272187676899062, 0.4: 0.1819252083008504, 0.5: 0.3728054727792405}\n}\n</code></pre> <p>The function would return a dict after loading the sensitivity file. The keys of the dict are name of parameters in each layer. And the value of key is the information about pruning sensitivity of corresponding layer. In example, pruning 10% filter of the layer corresponding to conv10_expand_weights would lead to 0.65% degradation of model performance. The details could be seen at: Sensitivity analysis</p> <p>The function would return a dict after loading the sensitivity file. The keys of the dict are name of parameters in each layer. And the value of key is the information about pruning sensitivity of corresponding layer. In example, pruning 10% filter of the layer corresponding to conv10_expand_weights would lead to 0.65% degradation of model performance. The details could be seen at: Sensitivity analysis</p> <p>Enter the PaddleOCR root directory\uff0cperform sensitivity analysis on the model with the following command\uff1a</p> <pre><code>python3.7 deploy/slim/prune/sensitivity_anal.py -c configs/det/ch_ppocr_v2.0/ch_det_mv3_db_v2.0.yml -o Global.pretrained_model=\"your trained model\"  Global.save_model_dir=./output/prune_model/\n</code></pre>"},{"location":"en/ppocr/model_compress/prune.html#5-export-inference-model-and-deploy-it","title":"5. Export inference model and deploy it","text":"<p>We can export the pruned model as inference_model for deployment:</p> <pre><code>python deploy/slim/prune/export_prune_model.py -c configs/det/ch_ppocr_v2.0/ch_det_mv3_db_v2.0.yml  -o Global.pretrained_model=./output/det_db/best_accuracy  Global.save_inference_dir=./prune/prune_inference_model\n</code></pre> <p>Reference for prediction and deployment of inference model:</p> <ol> <li>inference model python prediction</li> <li>inference model C++ prediction</li> </ol>"},{"location":"en/ppocr/model_compress/quantization.html","title":"PP-OCR Models Quantization","text":"<p>Generally, a more complex model would achieve better performance in the task, but it also leads to some redundancy in the model. Quantization is a technique that reduces this redundancy by reducing the full precision data to a fixed number, so as to reduce model calculation complexity and improve model inference performance.</p> <p>This example uses PaddleSlim provided APIs of Quantization to compress the OCR model.</p> <p>It is recommended that you could understand following pages before reading this example\uff1a</p> <ul> <li>The training strategy of OCR model</li> <li>PaddleSlim Document</li> </ul>"},{"location":"en/ppocr/model_compress/quantization.html#quick-start","title":"Quick Start","text":"<p>Quantization is mostly suitable for the deployment of lightweight models on mobile terminals. After training, if you want to further compress the model size and accelerate the prediction, you can use quantization methods to compress the model according to the following steps.</p> <ol> <li>Install PaddleSlim</li> <li>Prepare trained model</li> <li>Quantization-Aware Training</li> <li>Export inference model</li> <li>Deploy quantization inference model</li> </ol>"},{"location":"en/ppocr/model_compress/quantization.html#1-install-paddleslim","title":"1. Install PaddleSlim","text":"<pre><code>pip3 install paddleslim==2.3.2\n</code></pre>"},{"location":"en/ppocr/model_compress/quantization.html#2-download-pre-trained-model","title":"2. Download Pre-trained Model","text":"<p>PaddleOCR provides a series of pre-trained models. If the model to be quantified is not in the list, you need to follow the Regular Training method to get the trained model.</p>"},{"location":"en/ppocr/model_compress/quantization.html#3-quant-aware-training","title":"3. Quant-Aware Training","text":"<p>Quantization training includes offline quantization training and online quantization training. Online quantization training is more effective. It is necessary to load the pre-trained model. After the quantization strategy is defined, the model can be quantified.</p> <p>The code for quantization training is located in <code>slim/quantization/quant.py</code>. For example, the training instructions of slim PPOCRv3 detection model are as follows:</p> <pre><code># download provided model\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n\npython deploy/slim/quantization/quant.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model='./ch_PP-OCRv3_det_distill_train/best_accuracy'   Global.save_model_dir=./output/quant_model_distill/\n</code></pre> <p>If you want to quantify the text recognition model, you can modify the configuration file and loaded model parameters.</p>"},{"location":"en/ppocr/model_compress/quantization.html#4-export-inference-model","title":"4. Export inference model","text":"<p>Once we got the model after pruning and fine-tuning, we can export it as an inference model for the deployment of predictive tasks:</p> <pre><code>python deploy/slim/quantization/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.checkpoints=output/quant_model/best_accuracy Global.save_inference_dir=./output/quant_inference_model\n</code></pre>"},{"location":"en/ppocr/model_compress/quantization.html#5-deploy","title":"5. Deploy","text":"<p>The numerical range of the quantized model parameters derived from the above steps is still FP32, but the numerical range of the parameters is int8. The derived model can be converted through the <code>opt tool</code> of PaddleLite.</p> <p>For quantitative model deployment, please refer to Mobile terminal model deployment</p>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html","title":"PP-OCRv3 text detection model training","text":""},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#1-introduction","title":"1. Introduction","text":"<p>PP-OCRv3 is a further upgrade of PP-OCRv2. This section introduces the training steps of the PP-OCRv3 detection model. For an introduction to the PP-OCRv3 strategy, refer to document.</p>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#2-detection-training","title":"2. Detection training","text":"<p>The PP-OCRv3 detection model is an upgrade of the CML (Collaborative Mutual Learning) collaborative mutual learning text detection distillation strategy in PP-OCRv2. PP-OCRv3 further optimizes the detection teacher model and student model. Among them, when optimizing the teacher model, the PAN structure LK-PAN with a large receptive field and the DML (Deep Mutual Learning) distillation strategy are proposed; when optimizing the student model, the FPN structure RSE-FPN with a residual attention mechanism is proposed.</p> <p>PP-OCRv3 detection training includes two steps:</p> <ul> <li> <p>Step 1: Use DML distillation method to train detection teacher model</p> </li> <li> <p>Step 2: Use the teacher model obtained in step 1 to train a lightweight student model using CML method</p> </li> </ul>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#21-prepare-data-and-operating-environment","title":"2.1 Prepare data and operating environment","text":"<p>The training data uses icdar2015 data. For the steps of preparing the training set, refer to ocr_dataset.</p> <p>For the preparation of the operating environment, refer to document.</p>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#22-train-the-teacher-model","title":"2.2 Train the teacher model","text":"<p>The configuration file for teacher model training is ch_PP-OCRv3_det_dml.yml. The Backbone, Neck, and Head of the teacher model structure are Resnet50, LKPAN, and DBHead respectively, and are trained using the DML distillation method. For a detailed introduction to the configuration file, refer to Document.</p> <p>Download ImageNet pre-trained model:</p> <pre><code># Download ResNet50_vd pre-trained model\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet50_vd_ssld_pretrained.pdparams\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>The model saved during training is in the output directory, which contains the following files:</p> <pre><code>best_accuracy.states\nbest_accuracy.pdparams # The model parameters with the best accuracy are saved by default\nbest_accuracy.pdopt # The optimizer-related parameters with the best accuracy are saved by default\nlatest.states\nlatest.pdparams # The latest model parameters saved by default\nlatest.pdopt # The optimizer-related parameters of the latest model saved by default\n</code></pre> <p>Among them, best_accuracy is the model parameter with the highest accuracy saved, and the model can be directly used for evaluation.</p> <p>The model evaluation command is as follows:</p> <pre><code>python3 tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre> <p>The trained teacher model has a larger structure and higher accuracy, which is used to improve the accuracy of the student model.</p> <p>Extract teacher model parameters best_accuracy contains the parameters of two models, corresponding to Student and Student2 in the configuration file. The method to extract the parameters of Student is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./pretrain_models/dml_teacher.pdparams\")\n</code></pre> <p>The extracted model parameters can be used for further fine-tuning or distillation training of the model.</p>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#23-training-the-student-model","title":"2.3 Training the student model","text":"<p>The configuration file for training the student model is ch_PP-OCRv3_det_cml.yml The teacher model trained in the previous section is used as supervision, and the CML method is used to train a lightweight student model.</p> <p>Download the ImageNet pre-trained model of the student model:</p> <pre><code># Download the pre-trained model of MobileNetV3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/MobileNetV3_large_x0_5_pretrained.pdparams\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Teacher.pretrained=./pretrain_models/dml_teacher \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Teacher.pretrained=./pretrain_models/dml_teacher \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>The model saved during the training process is in the output directory. The model evaluation command is as follows:</p> <pre><code>python3 tools/eval.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre> <p>best_accuracy contains the parameters of three models, corresponding to Student, Student2, and Teacher in the configuration file. The method to extract Student parameters is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./pretrain_models/cml_student.pdparams\")\n</code></pre> <p>The extracted Student parameters can be used for model deployment or further fine-tuning training.</p>"},{"location":"en/ppocr/model_train/PPOCRv3_det_train.html#3-fine-tune-training-based-on-pp-ocrv3-detection","title":"3. Fine-tune training based on PP-OCRv3 detection","text":"<p>This section describes how to use the PP-OCRv3 detection model for fine-tune training in other scenarios.</p> <p>Fine-tune training is applicable to three scenarios:</p> <ul> <li> <p>Fine-tune training based on the CML distillation method is applicable to scenarios where the teacher model has higher accuracy than the PP-OCRv3 detection model in the usage scenario and a lightweight detection model is desired.</p> </li> <li> <p>Fine-tune training based on the PP-OCRv3 lightweight detection model does not require the training of the teacher model and is intended to improve the accuracy of the usage scenario based on the PP-OCRv3 detection model.</p> </li> <li> <p>Fine-tune training based on the DML distillation method is applicable to scenarios where the DML method is used to further improve accuracy.</p> </li> </ul> <p>Finetune training based on CML distillation method</p> <p>Download PP-OCRv3 training model:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n</code></pre> <p>ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams contains the parameters of Student, Student2, and Teacher models in the CML configuration file.</p> <p>Start training:</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml \\\n-o Global.pretrained_model=./ch_PP-OCRv3_det_distill_train/best_accuracy \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml \\\n-o Global.pretrained_model=./ch_PP-OCRv3_det_distill_train/best_accuracy \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>Finetune training based on PP-OCRv3 lightweight detection model</p> <p>Download PP-OCRv3 training model and extract model parameters of Student structure:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n</code></pre> <p>The method to extract Student parameters is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./student.pdparams\")\n</code></pre> <p>Train using the configuration file ch_PP-OCRv3_det_student.yml.</p> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml \\\n-o Global.pretrained_model=./student \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml \\\n-o Global.pretrained_model=./student \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>Finetune training based on DML distillation method</p> <p>Take the Teacher model in ch_PP-OCRv3_det_distill_train as an example. First, extract the parameters of the Teacher structure. The method is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Teacher.\"):]: all_params[key] for key in all_params if \"Teacher.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./teacher.pdparams\")\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./teacher \\\nArchitecture.Models.Student2.pretrained=./teacher \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./teacher \\\nArchitecture.Models.Student2.pretrained=./teacher \\\nGlobal.save_model_dir=./output/\n</code></pre>"},{"location":"en/ppocr/model_train/angle_class.html","title":"Text Direction Classification","text":""},{"location":"en/ppocr/model_train/angle_class.html#1-method-introduction","title":"1. Method Introduction","text":"<p>The angle classification is used in the scene where the image is not 0 degrees. In this scene, it is necessary to perform a correction operation on the text line detected in the picture. In the PaddleOCR system, The text line image obtained after text detection is sent to the recognition model after affine transformation. At this time, only a 0 and 180 degree angle classification of the text is required, so the built-in PaddleOCR text angle classifier only supports 0 and 180 degree classification. If you want to support more angles, you can modify the algorithm yourself to support.</p> <p>Example of 0 and 180 degree data samples\uff1a</p> <p></p>"},{"location":"en/ppocr/model_train/angle_class.html#2-data-preparation","title":"2. Data Preparation","text":"<p>Please organize the dataset as follows:</p> <p>The default storage path for training data is <code>PaddleOCR/train_data/cls</code>, if you already have a dataset on your disk, just create a soft link to the dataset directory:</p> <pre><code>ln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/cls/dataset\n</code></pre> <p>please refer to the following to organize your data.</p>"},{"location":"en/ppocr/model_train/angle_class.html#training-set","title":"Training set","text":"<p>First put the training images in the same folder (train_images), and use a txt file (cls_gt_train.txt) to store the image path and label.</p> <ul> <li>Note: by default, the image path and image label are split with <code>\\t</code>, if you use other methods to split, it will cause training error</li> </ul> <p>0 and 180 indicate that the angle of the image is 0 degrees and 180 degrees, respectively.</p> <pre><code>\" Image file name           Image annotation \"\n\ntrain/word_001.jpg   0\ntrain/word_002.jpg   180\n</code></pre> <p>The final training set should have the following file structure:</p> <pre><code>|-train_data\n    |-cls\n        |- cls_gt_train.txt\n        |- train\n            |- word_001.png\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/ppocr/model_train/angle_class.html#test-set","title":"Test set","text":"<p>Similar to the training set, the test set also needs to be provided a folder containing all images (test) and a cls_gt_test.txt. The structure of the test set is as follows:</p> <pre><code>|-train_data\n    |-cls\n        |- cls_gt_test.txt\n        |- test\n            |- word_001.jpg\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/ppocr/model_train/angle_class.html#3-training","title":"3. Training","text":"<p>Write the prepared txt file and image folder path into the configuration file under the <code>Train/Eval.dataset.label_file_list</code> and <code>Train/Eval.dataset.data_dir</code> fields, the absolute path of the image consists of the <code>Train/Eval.dataset.data_dir</code> field and the image name recorded in the txt file.</p> <p>PaddleOCR provides training scripts, evaluation scripts, and prediction scripts.</p>"},{"location":"en/ppocr/model_train/angle_class.html#start-training","title":"Start training","text":"<pre><code># Set PYTHONPATH path\nexport PYTHONPATH=$PYTHONPATH:.\n# GPU training Support single card and multi-card training, specify the card number through --gpus.\n# Start training, the following command has been written into the train.sh file, just modify the configuration file path in the file\npython3 -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/cls/cls_mv3.yml\n</code></pre>"},{"location":"en/ppocr/model_train/angle_class.html#data-augmentation","title":"Data Augmentation","text":"<p>PaddleOCR provides a variety of data augmentation methods. If you want to add disturbance during training, Please uncomment the <code>RecAug</code> and <code>RandAugment</code> fields under <code>Train.dataset.transforms</code> in the configuration file.</p> <p>The default perturbation methods are: cvtColor, blur, jitter, Gauss noise, random crop, perspective, color reverse, RandAugment.</p> <p>Except for RandAugment, each disturbance method is selected with a 50% probability during the training process. For specific code implementation, please refer to: rec_img_aug.py randaugment.py</p>"},{"location":"en/ppocr/model_train/angle_class.html#training","title":"Training","text":"<p>PaddleOCR supports alternating training and evaluation. You can modify <code>eval_batch_step</code> in <code>configs/cls/cls_mv3.yml</code> to set the evaluation frequency. By default, it is evaluated every 1000 iter. The following content will be saved during training:</p> <pre><code>\u251c\u2500\u2500 best_accuracy.pdopt # Optimizer parameters for the best model\n\u251c\u2500\u2500 best_accuracy.pdparams # Parameters of the best model\n\u251c\u2500\u2500 best_accuracy.states # Metric info and epochs of the best model\n\u251c\u2500\u2500 config.yml # Configuration file for this experiment\n\u251c\u2500\u2500 latest.pdopt # Optimizer parameters for the latest model\n\u251c\u2500\u2500 latest.pdparams # Parameters of the latest model\n\u251c\u2500\u2500 latest.states # Metric info and epochs of the latest model\n\u2514\u2500\u2500 train.log # Training log\n</code></pre> <p>If the evaluation set is large, the test will be time-consuming. It is recommended to reduce the number of evaluations, or evaluate after training.</p> <p>Note that the configuration file for prediction/evaluation must be consistent with the training.</p>"},{"location":"en/ppocr/model_train/angle_class.html#4-evaluation","title":"4. Evaluation","text":"<p>The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/cls/cls_mv3.yml</code> file.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\n# GPU evaluation, Global.checkpoints is the weight to be tested\npython3 tools/eval.py -c configs/cls/cls_mv3.yml -o Global.checkpoints={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/ppocr/model_train/angle_class.html#5-prediction","title":"5. Prediction","text":""},{"location":"en/ppocr/model_train/angle_class.html#training-engine-prediction","title":"Training engine prediction","text":"<p>Using the model trained by paddleocr, you can quickly get prediction through the following script.</p> <p>Use <code>Global.infer_img</code> to specify the path of the predicted picture or folder, and use <code>Global.checkpoints</code> to specify the weight:</p> <pre><code># Predict English results\npython3 tools/infer_cls.py -c configs/cls/cls_mv3.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.load_static_weights=false Global.infer_img=doc/imgs_words_en/word_10.png\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words_en/word_10.png\n     result: ('0', 0.9999995)\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html","title":"Text Detection","text":"<p>This section uses the icdar2015 dataset as an example to introduce the training, evaluation, and testing of the detection model in PaddleOCR.</p>"},{"location":"en/ppocr/model_train/detection.html#1-data-and-weights-preparation","title":"1. Data and Weights Preparation","text":""},{"location":"en/ppocr/model_train/detection.html#11-data-preparation","title":"1.1 Data Preparation","text":"<p>To prepare datasets, refer to ocr_datasets .</p>"},{"location":"en/ppocr/model_train/detection.html#12-download-pre-trained-model","title":"1.2 Download Pre-trained Model","text":"<p>First download the pre-trained model. The detection model of PaddleOCR currently supports 3 backbones, namely MobileNetV3, ResNet18_vd and ResNet50_vd. You can use the model in PaddleClas to replace backbone according to your needs. And the responding download link of backbone pre-trained weights can be found in (https://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.0/README_cn.md#resnet%E5%8F%8A%E5%85%B6vd%E7%B3%BB%E5%88%97).</p> <pre><code>cd PaddleOCR/\n# Download the pre-trained model of MobileNetV3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/MobileNetV3_large_x0_5_pretrained.pdparams\n# or, download the pre-trained model of ResNet18_vd\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet18_vd_pretrained.pdparams\n# or, download the pre-trained model of ResNet50_vd\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet50_vd_ssld_pretrained.pdparams\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html#2-training","title":"2. Training","text":""},{"location":"en/ppocr/model_train/detection.html#21-start-training","title":"2.1 Start Training","text":"<p>If CPU version installed, please set the parameter <code>use_gpu</code> to <code>false</code> in the configuration.</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml  \\\n         -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>In the above instruction, use <code>-c</code> to select the training to use the <code>configs/det/det_mv3_db.yml</code> configuration file. For a detailed explanation of the configuration file, please refer to config.</p> <p>You can also use <code>-o</code> to change the training parameters without modifying the yml file. For example, adjust the training learning rate to 0.0001</p> <pre><code># single GPU training\npython3 tools/train.py -c configs/det/det_mv3_db.yml -o   \\\n         Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained  \\\n         Optimizer.base_lr=0.0001\n\n# multi-GPU training\n# Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n\n# multi-Node, multi-GPU training\n# Set the IPs of your nodes used by the '--ips' parameter. Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>Note: For multi-Node multi-GPU training, you need to replace the <code>ips</code> value in the preceding command with the address of your machine, and the machines must be able to ping each other. In addition, it requires activating commands separately on multiple machines when we start the training. The command for viewing the IP address of the machine is <code>ifconfig</code>.</p> <p>If you want to further speed up the training, you can use automatic mixed precision training. for single card training, the command is as follows:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html#22-load-trained-model-and-continue-training","title":"2.2 Load Trained Model and Continue Training","text":"<p>If you expect to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <p>For example:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrained_model</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrained_model</code> will be loaded.</p>"},{"location":"en/ppocr/model_train/detection.html#23-training-with-new-backbone","title":"2.3 Training with New Backbone","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>If the Backbone to be replaced has a corresponding implementation in PaddleOCR, you can directly modify the parameters in the <code>Backbone</code> part of the configuration yml file.</p> <p>However, if you want to use a new Backbone, an example of replacing the backbones is as follows:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li>Add code in the my_backbone.py file, the sample code is as follows:</li> </ol> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> <ol> <li>Import the added module in the ppocr/modeling/backbones/_init_.py file.</li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>  Backbone:\n    name: MyBackbone\n    args1: args1\n</code></pre> <p>NOTE: More details about replace Backbone and other mudule can be found in doc.</p>"},{"location":"en/ppocr/model_train/detection.html#24-mixed-precision-training","title":"2.4 Mixed Precision Training","text":"<p>If you want to speed up your training further, you can use Auto Mixed Precision Training, taking a single machine and a single gpu as an example, the commands are as follows:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html#25-distributed-training","title":"2.5 Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>Note: (1) When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. (2) Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>. (3) For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/ppocr/model_train/detection.html#26-training-with-knowledge-distillation","title":"2.6 Training with knowledge distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for text detection training process. For more details, please refer to doc.</p>"},{"location":"en/ppocr/model_train/detection.html#27-training-on-other-platformwindowsmacoslinux-dcu","title":"2.7 Training on other platform(Windows/macOS/Linux DCU)","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/ppocr/model_train/detection.html#28-fine-tuning","title":"2.8 Fine-tuning","text":"<p>In actual use, it is recommended to load the official pre-trained model and fine-tune it in your own data set. For the fine-tuning method of the detection model, please refer to: Model Fine-tuning Tutorial.</p>"},{"location":"en/ppocr/model_train/detection.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/ppocr/model_train/detection.html#31-evaluation","title":"3.1 Evaluation","text":"<p>PaddleOCR calculates three indicators for evaluating performance of OCR detection task: Precision, Recall, and Hmean(F-Score).</p> <p>Run the following code to calculate the evaluation indicators. The result will be saved in the test result file specified by <code>save_res_path</code> in the configuration file <code>det_db_mv3.yml</code></p> <p>When evaluating, set post-processing parameters <code>box_thresh=0.6</code>, <code>unclip_ratio=1.5</code>. If you use different datasets, different models for training, these two parameters should be adjusted for better result.</p> <p>The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file.</p> <pre><code>python3 tools/eval.py -c configs/det/det_mv3_db.yml  -o Global.checkpoints=\"{path/to/weights}/best_accuracy\" PostProcess.box_thresh=0.6 PostProcess.unclip_ratio=1.5\n</code></pre> <ul> <li>Note: <code>box_thresh</code> and <code>unclip_ratio</code> are parameters required for DB post-processing, and not need to be set when evaluating the EAST and SAST model.</li> </ul>"},{"location":"en/ppocr/model_train/detection.html#32-test","title":"3.2 Test","text":"<p>Test the detection result on a single image:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/det_db/best_accuracy\"\n</code></pre> <p>When testing the DB model, adjust the post-processing threshold:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/det_db/best_accuracy\"  PostProcess.box_thresh=0.6 PostProcess.unclip_ratio=2.0\n</code></pre> <p>Test the detection result on all images in the folder:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/\" Global.pretrained_model=\"./output/det_db/best_accuracy\"\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html#4-inference","title":"4. Inference","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>Firstly, we can convert DB trained model to inference model:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=\"./output/det_db/best_accuracy\" Global.save_inference_dir=\"./output/det_db_inference/\"\n</code></pre> <p>The detection inference model prediction\uff1a</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"DB\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True\n</code></pre> <p>If it is other detection algorithms, such as the EAST, the det_algorithm parameter needs to be modified to EAST, and the default is the DB algorithm:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"EAST\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True\n</code></pre>"},{"location":"en/ppocr/model_train/detection.html#5-faq","title":"5. FAQ","text":"<p>Q1: The prediction results of trained model and inference model are inconsistent?</p> <p>A: Most of the problems are caused by the inconsistency of the pre-processing and post-processing parameters during the prediction of the trained model and the pre-processing and post-processing parameters during the prediction of the inference model. Taking the model trained by the det_mv3_db.yml configuration file as an example, the solution to the problem of inconsistent prediction results between the training model and the inference model is as follows:</p> <ul> <li>Check whether the trained model preprocessing is consistent with the prediction preprocessing function of the inference model. When the algorithm is evaluated, the input image size will affect the accuracy. In order to be consistent with the paper, the image is resized to [736, 1280] in the training icdar15 configuration file, but there is only a set of default parameters when the inference model predicts, which will be considered To predict the speed problem, the longest side of the image is limited to 960 for resize by default. The preprocessing function of the training model preprocessing and the inference model is located in ppocr/data/imaug/operators.py</li> <li>Check whether the post-processing of the trained model is consistent with the post-processing parameters of the inference.</li> </ul>"},{"location":"en/ppocr/model_train/finetune.html","title":"Fine-tune","text":""},{"location":"en/ppocr/model_train/finetune.html#1-background-and-meaning","title":"1. background and meaning","text":"<p>The PP-OCR series models provided by PaddleOCR have excellent performance in general scenarios and can solve detection and recognition problems in most cases. In vertical scenarios, if you want to obtain better model, you can further improve the accuracy of the PP-OCR series detection and recognition models through fine-tune.</p> <p>This article mainly introduces some precautions when fine-tuning the text detection and recognition model. Finally, you can obtain a text detection and recognition model with higher accuracy through model fine-tuning in your own scenarios.</p> <p>The core points of this article are as follows:</p> <ol> <li>The pre-trained model provided by PP-OCR has better generalization ability</li> <li>Adding a small amount of real data (detection:&gt;=500, recognition:&gt;=5000) will greatly improve the detection and recognition effect of vertical scenes</li> <li>When fine-tuning the model, adding real general scene data can further improve the model accuracy and generalization performance</li> <li>In the text detection task, increasing the prediction shape of the image can further improve the detection effect of the smaller text area</li> <li>When fine-tuning the model, it is necessary to properly adjust the hyperparameters (learning rate, batch size are the most important) to obtain a better fine-tuning effect.</li> </ol> <p>For more details, please refer to Chapter 2 and Chapter 3\u3002</p>"},{"location":"en/ppocr/model_train/finetune.html#2-text-detection-model-fine-tuning","title":"2. Text detection model fine-tuning","text":""},{"location":"en/ppocr/model_train/finetune.html#21-dataset","title":"2.1 Dataset","text":"<ul> <li> <p>Dataset: It is recommended to prepare at least 500 text detection datasets for model fine-tuning.</p> </li> <li> <p>Dataset annotation: single-line text annotation format, it is recommended that the labeled detection frame be consistent with the actual semantic content. For example, in the train ticket scene, the surname and first name may be far apart, but they belong to the same detection field semantically. Here, the entire name also needs to be marked as a detection frame.</p> </li> </ul>"},{"location":"en/ppocr/model_train/finetune.html#22-model","title":"2.2 Model","text":"<p>It is recommended to choose the PP-OCRv3 model (configuration file: ch_PP-OCRv3_det_student.yml\uff0cpre-trained model: ch_PP-OCRv3_det_distill_train.tar, its accuracy and generalization performance is the best pre-training model currently available.</p> <p>For more PP-OCR series models, please refer to PP-OCR Series Model Library\u3002</p> <p>Note: When using the above pre-trained model, you need to use the <code>student.pdparams</code> file in the folder as the pre-trained model, that is, only use the student model.</p>"},{"location":"en/ppocr/model_train/finetune.html#23-training-hyperparameter","title":"2.3 Training hyperparameter","text":"<p>When fine-tuning the model, the most important hyperparameter is the pre-training model path <code>pretrained_model</code>, <code>learning_rate</code>\u200b\u4e0e\u200b<code>batch_size</code>\uff0csome hyperparameters are as follows:</p> <pre><code>Global:\n  pretrained_model: ./ch_PP-OCRv3_det_distill_train/student.pdparams # pre-training model path\nOptimizer:\n  lr:\n    name: Cosine\n    learning_rate: 0.001 # learning_rate\n    warmup_epoch: 2\n  regularizer:\n    name: 'L2'\n    factor: 0\n\nTrain:\n  loader:\n    shuffle: True\n    drop_last: False\n    batch_size_per_card: 8  # single gpu batch size\n    num_workers: 4\n</code></pre> <p>In the above configuration file, you need to specify the <code>pretrained_model</code> field as the <code>student.pdparams</code> file path.</p> <p>The configuration file provided by PaddleOCR is for 8-gpu training (equivalent to a total batch size of <code>8*8=64</code>) and no pre-trained model is loaded. Therefore, in your scenario, the learning rate is the same as the total The batch size needs to be adjusted linearly, for example</p> <ul> <li>If your scenario is single-gpu training, single gpu batch_size=8, then the total batch_size=8, it is recommended to adjust the learning rate to about <code>1e-4</code>.</li> <li>If your scenario is for single-gpu training, due to memory limitations, you can only set batch_size=4 for a single gpu, and the total batch_size=4. It is recommended to adjust the learning rate to about <code>5e-5</code>.</li> </ul>"},{"location":"en/ppocr/model_train/finetune.html#24-prediction-hyperparameter","title":"2.4 Prediction hyperparameter","text":"<p>When exporting and inferring the trained model, you can further adjust the predicted image scale to improve the detection effect of small-area text. The following are some hyperparameters during DBNet inference, which can be adjusted appropriately to improve the effect.</p> hyperparameter type default meaning det_db_thresh float 0.3 In the probability map output by DB, pixels with a score greater than the threshold will be considered as text pixels det_db_box_thresh float 0.6 When the average score of all pixels within the frame of the detection result is greater than the threshold, the result will be considered as a text area det_db_unclip_ratio float 1.5 The expansion coefficient of <code>Vatti clipping</code>, using this method to expand the text area max_batch_size int 10 batch size use_dilation bool False Whether to expand the segmentation results to obtain better detection results det_db_score_mode str \"fast\" DB's detection result score calculation method supports <code>fast</code> and <code>slow</code>. <code>fast</code> calculates the average score based on all pixels in the polygon\u2019s circumscribed rectangle border, and <code>slow</code> calculates the average score based on all pixels in the original polygon. The calculation speed is relatively slower, but more accurate. <p>For more information on inference methods, please refer toPaddle Inference doc\u3002</p>"},{"location":"en/ppocr/model_train/finetune.html#3-text-recognition-model-fine-tuning","title":"3. Text recognition model fine-tuning","text":""},{"location":"en/ppocr/model_train/finetune.html#31-dataset","title":"3.1 Dataset","text":"<ul> <li> <p>Dataset\uff1aIf the dictionary is not changed, it is recommended to prepare at least 5,000 text recognition datasets for model fine-tuning; if the dictionary is changed (not recommended), more quantities are required.</p> </li> <li> <p>Data distribution: It is recommended that the distribution be as consistent as possible with the actual measurement scenario. If the actual scene contains a lot of short text, it is recommended to include more short text in the training data. If the actual scene has high requirements for the recognition effect of spaces, it is recommended to include more text content with spaces in the training data.</p> </li> <li> <p>Data synthesis: In the case of some character recognition errors, it is recommended to obtain a batch of specific character dataset, add it to the original dataset and use a small learning rate for fine-tuning. The ratio of original dataset to new dataset can be 10:1 to 5:1 to avoid overfitting of the model caused by too much data in a single scene. At the same time, try to balance the word frequency of the corpus to ensure that the frequency of common words will not be too low.</p> </li> </ul> <p>Specific characters can be generated using the TextRenderer tool, for synthesis examples, please refer to data synthesis   . The synthetic data corpus should come from real usage scenarios as much as possible, and keep the richness of fonts and backgrounds on the basis of being close to the real scene, which will help improve the model effect.</p> <ul> <li>Common Chinese and English data: During training, common real data can be added to the training set (for example, in the fine-tuning scenario without changing the dictionary, it is recommended to add real data such as LSVT, RCTW, MTWI) to further improve the generalization performance of the model.</li> </ul>"},{"location":"en/ppocr/model_train/finetune.html#32-model","title":"3.2 Model","text":"<p>It is recommended to choose the PP-OCRv3 model (configuration file: ch_PP-OCRv3_rec_distillation.yml\uff0cpre-trained model: ch_PP-OCRv3_rec_train.tar\uff0cits accuracy and generalization performance is the best pre-training model currently available.</p> <p>For more PP-OCR series models, please refer to PP-OCR Series Model Library\u3002</p> <p>The PP-OCRv3 model uses the GTC strategy. The SAR branch has a large number of parameters. When the training data is a simple scene, the model is easy to overfit, resulting in poor fine-tuning effect. It is recommended to remove the GTC strategy. The configuration file of the model structure is modified as follows:</p> <pre><code>Architecture:\n  model_type: rec\n  algorithm: SVTR\n  Transform:\n  Backbone:\n    name: MobileNetV1Enhance\n    scale: 0.5\n    last_conv_stride: [1, 2]\n    last_pool_type: avg\n  Neck:\n    name: SequenceEncoder\n    encoder_type: svtr\n    dims: 64\n    depth: 2\n    hidden_dims: 120\n    use_guide: False\n  Head:\n    name: CTCHead\n    fc_decay: 0.00001\nLoss:\n  name: CTCLoss\n\nTrain:\n  dataset:\n  ......\n    transforms:\n    # remove RecConAug\n    # - RecConAug:\n    #     prob: 0.5\n    #     ext_data_num: 2\n    #     image_shape: [48, 320, 3]\n    #     max_text_length: *max_text_length\n    - RecAug:\n    # modify Encode\n    - CTCLabelEncode:\n    - KeepKeys:\n        keep_keys:\n        - image\n        - label\n        - length\n...\n\nEval:\n  dataset:\n  ...\n    transforms:\n    ...\n    - CTCLabelEncode:\n    - KeepKeys:\n        keep_keys:\n        - image\n        - label\n        - length\n...\n</code></pre>"},{"location":"en/ppocr/model_train/finetune.html#33-training-hyperparameter","title":"3.3 Training hyperparameter","text":"<p>Similar to text detection task fine-tuning, when fine-tuning the recognition model, the most important hyperparameters are the pre-trained model path <code>pretrained_model</code>, <code>learning_rate</code> and <code>batch_size</code>, some default configuration files are shown below.</p> <pre><code>Global:\n  pretrained_model:  # pre-training model path\nOptimizer:\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]  # learning_rate\n    warmup_epoch: 5\n  regularizer:\n    name: 'L2'\n    factor: 0\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - ./train_data/train_list.txt\n    ratio_list: [1.0] # Sampling ratio, the default value is [1.0]\n  loader:\n    shuffle: True\n    drop_last: False\n    batch_size_per_card: 128 # single gpu batch size\n    num_workers: 8\n</code></pre> <p>In the above configuration file, you first need to specify the <code>pretrained_model</code> field as the <code>ch_PP-OCRv3_rec_train/best_accuracy.pdparams</code> file path decompressed in Chapter 3.2.</p> <p>The configuration file provided by PaddleOCR is for 8-gpu training (equivalent to a total batch size of <code>8*128=1024</code>) and no pre-trained model is loaded. Therefore, in your scenario, the learning rate is the same as the total The batch size needs to be adjusted linearly, for example:</p> <ul> <li>If your scenario is single-gpu training, single gpu batch_size=128, then the total batch_size=128, in the case of loading the pre-trained model, it is recommended to adjust the learning rate to about <code>[1e-4, 2e-5]</code> (For the piecewise learning rate strategy, two values need to be set, the same below).</li> <li>If your scenario is for single-gpu training, due to memory limitations, you can only set batch_size=64 for a single gpu, and the total batch_size=64. When loading the pre-trained model, it is recommended to adjust the learning rate to <code>[5e-5 , 1e-5]</code>about.</li> </ul> <p>If there is general real scene data added, it is recommended that in each epoch, the amount of vertical scene data and real scene data should be kept at about 1:1.</p> <p>For example: your own vertical scene recognition data volume is 1W, the data label file is <code>vertical.txt</code>, the collected general scene recognition data volume is 10W, and the data label file is <code>general.txt</code>.</p> <p>Then, the <code>label_file_list</code> and <code>ratio_list</code> parameters can be set as shown below. In each epoch, <code>vertical.txt</code> will be fully sampled (sampling ratio is 1.0), including 1W pieces of data; <code>general.txt</code> will be sampled according to a sampling ratio of 0.1, including <code>10W*0.1=1W</code> pieces of data, the final ratio of the two is <code>1:1</code>.</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - vertical.txt\n    - general.txt\n    ratio_list: [1.0, 0.1]\n</code></pre>"},{"location":"en/ppocr/model_train/finetune.html#34-training-optimization","title":"3.4 training optimization","text":"<p>The training process does not happen overnight. After completing a stage of training evaluation, it is recommended to collect and analyze the badcase of the current model in the real scene, adjust the proportion of training data in a targeted manner, or further add synthetic data. Through multiple iterations of training, the model effect is continuously optimized.</p> <p>If you modify the custom dictionary during training, since the parameters of the last layer of FC cannot be loaded, it is normal for acc=0 at the beginning of the iteration. Don't worry, loading the pre-trained model can still speed up the model convergence.</p>"},{"location":"en/ppocr/model_train/kie.html","title":"Key Information Extraction","text":"<p>This tutorial provides a guide to the whole process of key information extraction using PaddleOCR, including data preparation, model training, optimization, evaluation, prediction of semantic entity recognition (SER) and relationship extraction (RE) tasks.</p>"},{"location":"en/ppocr/model_train/kie.html#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"en/ppocr/model_train/kie.html#11-prepare-for-dataset","title":"1.1. Prepare for dataset","text":"<p>PaddleOCR supports the following data format when training KIE models.</p> <ul> <li><code>general data</code> is used to train a dataset whose annotation is stored in a text file (SimpleDataset).</li> </ul> <p>The default storage path of training data is <code>PaddleOCR/train_data</code>. If you already have datasets on your disk, you only need to create a soft link to the dataset directory.</p> <pre><code># linux and mac os\nln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/dataset\n# windows\nmklink /d &lt;path/to/paddle_ocr&gt;/train_data/dataset &lt;path/to/dataset&gt;\n</code></pre>"},{"location":"en/ppocr/model_train/kie.html#12-custom-dataset","title":"1.2. Custom Dataset","text":"<p>The training process generally includes the training set and the evaluation set. The data formats of the two sets are same.</p>"},{"location":"en/ppocr/model_train/kie.html#1-training-set","title":"(1) Training set","text":"<p>It is recommended to put the training images into the same folder, record the path and annotation of images in a text file. The contents of the text file are as follows:</p> <pre><code>\" image path                 annotation information \"\nzh_train_0.jpg   [{\"transcription\": \"\u200b\u6c47\u4e30\u200b\u664b\u4fe1\u200b\", \"label\": \"other\", \"points\": [[104, 114], [530, 114], [530, 175], [104, 175]], \"id\": 1, \"linking\": []}, {\"transcription\": \"\u200b\u53d7\u7406\u200b\u65f6\u95f4\u200b:\", \"label\": \"question\", \"points\": [[126, 267], [266, 267], [266, 305], [126, 305]], \"id\": 7, \"linking\": [[7, 13]]}, {\"transcription\": \"2020.6.15\", \"label\": \"answer\", \"points\": [[321, 239], [537, 239], [537, 285], [321, 285]], \"id\": 13, \"linking\": [[7, 13]]}]\nzh_train_1.jpg   [{\"transcription\": \"\u200b\u4e2d\u56fd\u200b\u4eba\u4f53\u5668\u5b98\u200b\u6350\u732e\u200b\", \"label\": \"other\", \"points\": [[544, 459], [954, 459], [954, 517], [544, 517]], \"id\": 1, \"linking\": []}, {\"transcription\": \"&gt;\u200b\u7f16\u53f7\u200b:MC545715483585\", \"label\": \"other\", \"points\": [[1462, 470], [2054, 470], [2054, 543], [1462, 543]], \"id\": 10, \"linking\": []}, {\"transcription\": \"CHINAORGANDONATION\", \"label\": \"other\", \"points\": [[543, 516], [958, 516], [958, 551], [543, 551]], \"id\": 14, \"linking\": []}, {\"transcription\": \"\u200b\u4e2d\u56fd\u200b\u4eba\u4f53\u5668\u5b98\u200b\u6350\u732e\u200b\u5fd7\u613f\u200b\u767b\u8bb0\u8868\u200b\", \"label\": \"header\", \"points\": [[635, 793], [1892, 793], [1892, 904], [635, 904]], \"id\": 18, \"linking\": []}]\n...\n</code></pre> <p>Note: In the text file, please split the image path and annotation with <code>\\t</code>. Otherwise, error will happen when training.</p> <p>The annotation can be parsed by <code>json</code> into a list of sub-annotations. Each element in the list is a dict, which stores the required information of each text line. The required fields are as follows.</p> <ul> <li>transcription: stores the text content of the text line</li> <li>label: the category of the text line content</li> <li>points: stores the four point position information of the text line</li> <li>id: stores the ID information of the text line for RE model training</li> <li>linking: stores the connection information between text lines for RE model training</li> </ul>"},{"location":"en/ppocr/model_train/kie.html#2-evaluation-set","title":"(2) Evaluation set","text":"<p>The evaluation set is constructed in the same way as the training set.</p>"},{"location":"en/ppocr/model_train/kie.html#3-dictionary-file","title":"(3) Dictionary file","text":"<p>The textlines in the training set and the evaluation set contain label information. The list of all labels is stored in the dictionary file (such as <code>class_list.txt</code>). Each line in the dictionary file is represented as a label name.</p> <p>For example, FUND_zh data contains four categories. The contents of the dictionary file are as follows.</p> <pre><code>OTHER\nQUESTION\nANSWER\nHEADER\n</code></pre> <p>In the annotation file, the annotation information of the <code>label</code> field of the text line content of each annotation needs to belong to the dictionary content.</p> <p>The final dataset shall have the following file structure.</p> <pre><code>|-train_data\n  |-data_name\n    |- train.json\n    |- train\n        |- zh_train_0.png\n        |- zh_train_1.jpg\n        | ...\n    |- val.json\n    |- val\n        |- zh_val_0.png\n        |- zh_val_1.jpg\n        | ...\n</code></pre> <p>Note:</p> <p>-The category information in the annotation file is not case sensitive. For example, 'HEADER' and 'header' will be seen as the same category ID.</p> <ul> <li>In the dictionary file, it is recommended to put the <code>other</code> category (other textlines that need not be paid attention to can be labeled as <code>other</code>) on the first line. When parsing, the category ID of the 'other' category will be resolved to 0, and the textlines predicted as <code>other</code> will not be visualized later.</li> </ul>"},{"location":"en/ppocr/model_train/kie.html#13-download-data","title":"1.3. Download data","text":"<p>If you do not have local dataset, you can donwload the source files of XFUND or FUNSD and use the scripts of XFUND or FUNSD for tranform them into PaddleOCR format. Then you can use the public dataset to quick experience KIE.</p> <p>For more information about public KIE datasets, please refer to KIE dataset tutorial.</p> <p>PaddleOCR also supports the annotation of KIE models. Please refer to PPOCRLabel tutorial.</p>"},{"location":"en/ppocr/model_train/kie.html#2-training","title":"2. Training","text":"<p>PaddleOCR provides training scripts, evaluation scripts and inference scripts. We will introduce based on VI-LayoutXLM model in this section. This section will take the VI layoutxlm multimodal pre training model as an example to explain.</p> <p>If you want to use the SDMGR based KIE algorithm, please refer to: SDMGR tutorial.</p>"},{"location":"en/ppocr/model_train/kie.html#21-start-training","title":"2.1. Start Training","text":"<p>If you do not use a custom dataset, you can use XFUND_zh that has been processed in PaddleOCR dataset for quick experience.</p> <pre><code>mkdir train_data\ncd train_data\nwget https://paddleocr.bj.bcebos.com/ppstructure/dataset/XFUND.tar &amp;&amp; tar -xf XFUND.tar\ncd ..\n</code></pre> <p>If you don't want to train, and want to directly experience the process of model evaluation, prediction, and inference, you can download the training model provided in PaddleOCR and skip section 2.1.</p> <p>Use the following command to download the trained model.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# download and uncompress SER model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar &amp; tar -xf ser_vi_layoutxlm_xfund_pretrained.tar\n\n# download and uncompress RE model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar &amp; tar -xf re_vi_layoutxlm_xfund_pretrained.tar\n</code></pre> <p>Start training:</p> <ul> <li>If your paddlepaddle version is <code>CPU</code>, you need to set <code>Global.use_gpu=False</code> in your config file.</li> <li>During training, PaddleOCR will download the VI-LayoutXLM pretraining model by default. There is no need to download it in advance.</li> </ul> <pre><code># GPU training, support single card and multi-cards\n# The training log will be save in \"{Global.save_model_dir}/train.log\"\n\n# train SER model using single card\npython3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n\n# train SER model using multi-cards, you can use --gpus to assign the GPU ids.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n\n# train RE model using single card\npython3 tools/train.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml\n</code></pre> <p>Take the SER model training as an example. After the training is started, you will see the following log output.</p> <pre><code>[2022/08/08 16:28:28] ppocr INFO: epoch: [1/200], global_step: 10, lr: 0.000006, loss: 1.871535, avg_reader_cost: 0.28200 s, avg_batch_cost: 0.82318 s, avg_samples: 8.0, ips: 9.71838 samples/s, eta: 0:51:59\n[2022/08/08 16:28:33] ppocr INFO: epoch: [1/200], global_step: 19, lr: 0.000018, loss: 1.461939, avg_reader_cost: 0.00042 s, avg_batch_cost: 0.32037 s, avg_samples: 6.9, ips: 21.53773 samples/s, eta: 0:37:55\n[2022/08/08 16:28:39] ppocr INFO: cur metric, precision: 0.11526348939743859, recall: 0.19776657060518732, hmean: 0.14564265817747712, fps: 34.008392345050055\n[2022/08/08 16:28:45] ppocr INFO: save best model is to ./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n[2022/08/08 16:28:45] ppocr INFO: best metric, hmean: 0.14564265817747712, precision: 0.11526348939743859, recall: 0.19776657060518732, fps: 34.008392345050055, best_epoch: 1\n[2022/08/08 16:28:51] ppocr INFO: save model in ./output/ser_vi_layoutxlm_xfund_zh/latest\n</code></pre> <p>The following information will be automatically printed.</p> Field meaning epoch current iteration round iter current iteration times lr current learning rate loss current loss function reader_cost current batch data processing time batch_ Cost total current batch time samples number of samples in the current batch ips number of samples processed per second <p>PaddleOCR supports evaluation during training. you can modify <code>eval_batch_step</code> in the config file <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> (default as 19 iters). Trained model with best hmean will be saved as <code>output/ser_vi_layoutxlm_xfund_zh/best_accuracy/</code>.</p> <p>If the evaluation dataset is very large, it's recommended to enlarge the eval interval or evaluate the model after training.</p> <p>Note: for more KIE models training and configuration files, you can go into <code>configs/kie/</code> or refer to Frontier KIE algorithms.</p> <p>If you want to train model on your own dataset, you need to modify the data path, dictionary file and category number in the configuration file.</p> <p>Take <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> as an example, contents we need to fix is as follows.</p> <pre><code>Architecture:\n  # ...\n  Backbone:\n    name: LayoutXLMForSer\n    pretrained: True\n    mode: vi\n    # Assuming that n categroies are included in the dictionary file (other is included), the the num_classes is set as 2n-1\n    num_classes: &amp;num_classes 7\n\nPostProcess:\n  name: kieSerTokenLayoutLMPostProcess\n  # Modify the dictionary file path for your custom dataset\n  class_path: &amp;class_path train_data/XFUND/class_list_xfun.txt\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    # Modify the data path for your training dataset\n    data_dir: train_data/XFUND/zh_train/image\n    # Modify the data annotation path for your training dataset\n    label_file_list:\n      - train_data/XFUND/zh_train/train.json\n    ...\n  loader:\n    # batch size for single card when training\n    batch_size_per_card: 8\n    ...\n\nEval:\n  dataset:\n    name: SimpleDataSet\n    # Modify the data path for your evaluation dataset\n    data_dir: train_data/XFUND/zh_val/image\n    # Modify the data annotation path for your evaluation dataset\n    label_file_list:\n      - train_data/XFUND/zh_val/val.json\n    ...\n  loader:\n    # batch size for single card when evaluation\n    batch_size_per_card: 8\n</code></pre> <p>Note that the configuration file for prediction/evaluation must be consistent with the training file.</p>"},{"location":"en/ppocr/model_train/kie.html#22-resume-training","title":"2.2. Resume Training","text":"<p>If the training process is interrupted and you want to load the saved model to resume training, you can specify the path of the model to be loaded by specifying <code>Architecture.Backbone.checkpoints</code>.</p> <pre><code>python3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n</code></pre> <p>Note:</p> <ul> <li>Priority of <code>Architecture.Backbone.checkpoints</code> is higher than<code>Architecture.Backbone.pretrained</code>. You need to set <code>Architecture.Backbone.checkpoints</code> for model finetuning, resume and evalution. If you want to train with the NLP pretrained model, you need to set <code>Architecture.Backbone.pretrained</code> as <code>True</code> and set <code>Architecture.Backbone.checkpoints</code> as null (<code>null</code>).</li> <li>PaddleNLP pretrained models are used here for LayoutXLM series models, the model loading and saving logic is same as those in PaddleNLP. Therefore we do not need to set <code>Global.pretrained_model</code> or <code>Global.checkpoints</code> here.</li> <li>If you use knowledge distillation to train the LayoutXLM series models, resuming training is not supported now.</li> </ul>"},{"location":"en/ppocr/model_train/kie.html#23-mixed-precision-training","title":"2.3. Mixed Precision Training","text":"<p>coming soon!</p>"},{"location":"en/ppocr/model_train/kie.html#24-distributed-training","title":"2.4. Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n</code></pre> <p>Note: (1) When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. (2) Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>. (3) For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/ppocr/model_train/kie.html#25-train-with-knowledge-distillation","title":"2.5. Train with Knowledge Distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for KIE model training process. The configuration file is ser_vi_layoutxlm_xfund_zh_udml.yml. For more information, please refer to doc.</p> <p>Note: The saving and loading logic of the LayoutXLM series KIE models in PaddleOCR is consistent with PaddleNLP, so only the parameters of the student model are saved in the distillation process. If you want to use the saved model for evaluation, you need to use the configuration of the student model (the student model corresponding to the distillation file above is ser_vi_layoutxlm_xfund_zh.yml.</p>"},{"location":"en/ppocr/model_train/kie.html#26-training-on-other-platform","title":"2.6. Training on other platform","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/ppocr/model_train/kie.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/ppocr/model_train/kie.html#31-evaluation","title":"3.1. Evaluation","text":"<p>The trained model will be saved in <code>Global.save_model_dir</code>. When evaluation, you need to set <code>Architecture.Backbone.checkpoints</code> as your model directroy. The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> file.</p> <pre><code># GPU evaluation, Global.checkpoints is the weight to be tested\npython3 tools/eval.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n</code></pre> <p>The following information will be printed such as precision, recall, hmean and so on.</p> <pre><code>[2022/08/09 07:59:28] ppocr INFO: metric eval ***************\n[2022/08/09 07:59:28] ppocr INFO: precision:0.697476609016161\n[2022/08/09 07:59:28] ppocr INFO: recall:0.8861671469740634\n[2022/08/09 07:59:28] ppocr INFO: hmean:0.7805806758686339\n[2022/08/09 07:59:28] ppocr INFO: fps:17.367364606899105\n</code></pre>"},{"location":"en/ppocr/model_train/kie.html#32-test","title":"3.2. Test","text":"<p>Using the model trained by PaddleOCR, we can quickly get prediction through the following script.</p> <p>The default prediction image is stored in <code>Global.infer_img</code>, and the trained model weight is specified via <code>-o Global.checkpoints</code>.</p> <p>According to the <code>Global.save_model_dir</code> and <code>save_epoch_step</code> fields set in the configuration file, the following parameters will be saved.</p> <pre><code>output/ser_vi_layoutxlm_xfund_zh/\n\u251c\u2500\u2500 best_accuracy\n       \u251c\u2500\u2500 metric.states\n       \u251c\u2500\u2500 model_config.json\n       \u251c\u2500\u2500 model_state.pdparams\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 latest\n       \u251c\u2500\u2500 metric.states\n       \u251c\u2500\u2500 model_config.json\n       \u251c\u2500\u2500 model_state.pdparams\n\u251c\u2500\u2500 latest.pdopt\n</code></pre> <p>Among them, best_accuracy.is the best model on the evaluation set; latest. is the model of the last epoch.</p> <p>The configuration file for prediction must be consistent with the training file. If you finish the training process using <code>python3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code>. You can use the following command for prediction.</p> <pre><code>python3 tools/infer_kie_token_ser.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.infer_img=./ppstructure/docs/kie/input/zh_val_42.jpg\n</code></pre> <p>The output image is as follows, which is also saved in <code>Global.save_res_path</code>.</p> <p></p> <p>During the prediction process, the detection and recognition model of PP-OCRv3 will be loaded by default for information extraction of OCR. If you want to load the OCR results obtained in advance, you can use the following method to predict, and specify <code>Global.infer_img</code> as the annotation file, which contains the image path and OCR information, and specifies <code>Global.infer_mode</code> as False, indicating that the OCR inference engine is not used at this time.</p> <pre><code>python3 tools/infer_kie_token_ser.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.infer_img=./train_data/XFUND/zh_val/val.json Global.infer_mode=False\n</code></pre> <p>For the above image, if information extraction is performed using the labeled OCR results, the prediction results are as follows.</p> <p></p> <p>It can be seen that part of the detection information is more accurate, but the overall information extraction results are basically the same.</p> <p>In RE model prediction, the SER model result needs to be given first, so the configuration file and model weight of SER need to be loaded at the same time, as shown in the following example.</p> <pre><code>python3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrain_models/re_vi_layoutxlm_udml_xfund_zh/best_accuracy/ \\\n  Global.infer_img=./train_data/XFUND/zh_val/image/ \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=pretrain_models/ \\\n  ser_vi_layoutxlm_udml_xfund_zh/best_accuracy/\n</code></pre> <p>The result is as follows.</p> <p></p> <p>If you want to load the OCR results obtained in advance, you can use the following method to predict, and specify <code>Global.infer_img</code> as the annotation file, which contains the image path and OCR information, and specifies <code>Global.infer_mode</code> as False, indicating that the OCR inference engine is not used at this time.</p> <pre><code>python3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrain_models/re_vi_layoutxlm_udml_xfund_zh/best_accuracy/ \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=pretrain_models/ser_vi_layoutxlm_udml_xfund_zh/best_accuracy/\n</code></pre> <p><code>c_ser</code> denotes SER configurations file, <code>o_ser</code> denotes the SER model configurations that will override corresponding content in the file.</p> <p>The result is as follows.</p> <p></p> <p>It can be seen that the re prediction results directly using the annotated OCR results are more accurate.</p>"},{"location":"en/ppocr/model_train/kie.html#4-model-inference","title":"4. Model inference","text":""},{"location":"en/ppocr/model_train/kie.html#41-export-the-model","title":"4.1 Export the model","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>The SER model can be converted to the inference model using the following command.</p> <pre><code># -c Set the training algorithm yml configuration file.\n# -o Set optional parameters.\n# Architecture.Backbone.checkpoints Set the training model address.\n# Global.save_inference_dir Set the address where the converted model will be saved.\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/ser_vi_layoutxlm\n</code></pre> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/ser_vi_layoutxlm/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition\n</code></pre> <p>The RE model can be converted to the inference model using the following command.</p> <pre><code># -c Set the training algorithm yml configuration file.\n# -o Set optional parameters.\n# Architecture.Backbone.checkpoints Set the training model address.\n# Global.save_inference_dir Set the address where the converted model will be saved.\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/re_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/re_vi_layoutxlm\n</code></pre> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/re_vi_layoutxlm/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition\n</code></pre>"},{"location":"en/ppocr/model_train/kie.html#42-model-inference","title":"4.2 Model inference","text":"<p>The VI layoutxlm model performs reasoning based on the ser task, and can execute the following commands:</p> <p>Using the following command to infer the VI-LayoutXLM SER model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visualized result will be saved in <code>./output</code>, which is shown as follows.</p> <p></p> <p>Using the following command to infer the VI-LayoutXLM RE model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visualized result will be saved in <code>./output</code>, which is shown as follows.</p> <p></p>"},{"location":"en/ppocr/model_train/kie.html#5-faq","title":"5. FAQ","text":"<p>Q1: After the training model is transferred to the inference model, the prediction effect is inconsistent?</p> <p>A\uff1aThe problems are mostly caused by inconsistent preprocessing and postprocessing parameters when the trained model predicts and the preprocessing and postprocessing parameters when the inference model predicts. You can compare whether there are differences in preprocessing, postprocessing, and prediction in the configuration files used for training.</p>"},{"location":"en/ppocr/model_train/recognition.html","title":"Text Recognition","text":""},{"location":"en/ppocr/model_train/recognition.html#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"en/ppocr/model_train/recognition.html#11-dataset-preparation","title":"1.1 DataSet Preparation","text":"<p>To prepare datasets, refer to ocr_datasets .</p> <p>PaddleOCR provides label files for training the icdar2015 dataset, which can be downloaded in the following ways:</p> <pre><code># Training set label\nwget -P ./train_data/ic15_data  https://paddleocr.bj.bcebos.com/dataset/rec_gt_train.txt\n# Test Set Label\nwget -P ./train_data/ic15_data  https://paddleocr.bj.bcebos.com/dataset/rec_gt_test.txt\n</code></pre> <p>PaddleOCR also provides a data format conversion script, which can convert ICDAR official website label to a data format supported by PaddleOCR. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># convert the official gt to rec_gt_label.txt\npython gen_label.py --mode=\"rec\" --input_path=\"{path/of/origin/label}\" --output_label=\"rec_gt_label.txt\"\n</code></pre> <p>The data format is as follows, (a) is the original picture, (b) is the Ground Truth text file corresponding to each picture:</p> <p></p> <ul> <li>Multilingual dataset</li> </ul> <p>The multi-language model training method is the same as the Chinese model. The training data set is 100w synthetic data. A small amount of fonts and test data can be downloaded using the following two methods.</p> <ul> <li>Baidu Netdisk ,Extraction code:frgi.</li> <li>Google drive</li> </ul>"},{"location":"en/ppocr/model_train/recognition.html#12-dictionary","title":"1.2 Dictionary","text":"<p>Finally, a dictionary ({word_dict_name}.txt) needs to be provided so that when the model is trained, all the characters that appear can be mapped to the dictionary index.</p> <p>Therefore, the dictionary needs to contain all the characters that you want to be recognized correctly. {word_dict_name}.txt needs to be written in the following format and saved in the <code>utf-8</code> encoding format:</p> <pre><code>l\nd\na\nd\nr\nn\n</code></pre> <p>In <code>word_dict.txt</code>, there is a single word in each line, which maps characters and numeric indexes together, e.g \"and\" will be mapped to [2 5 1]</p> <p>PaddleOCR has built-in dictionaries, which can be used on demand.</p> <p><code>ppocr/utils/ppocr_keys_v1.txt</code> is a Chinese dictionary with 6623 characters.</p> <p><code>ppocr/utils/ic15_dict.txt</code> is an English dictionary with 36 characters</p> <p><code>ppocr/utils/dict/french_dict.txt</code> is a French dictionary with 118 characters</p> <p><code>ppocr/utils/dict/japan_dict.txt</code> is a Japanese dictionary with 4399 characters</p> <p><code>ppocr/utils/dict/korean_dict.txt</code> is a Korean dictionary with 3636 characters</p> <p><code>ppocr/utils/dict/german_dict.txt</code> is a German dictionary with 131 characters</p> <p><code>ppocr/utils/en_dict.txt</code> is a English dictionary with 96 characters</p> <p>The current multi-language model is still in the demo stage and will continue to optimize the model and add languages. You are very welcome to provide us with dictionaries and fonts in other languages, If you like, you can submit the dictionary file to dict and we will thank you in the Repo.</p> <p>To customize the dict file, please modify the <code>character_dict_path</code> field in <code>configs/rec/rec_icdar15_train.yml</code> .</p> <ul> <li>Custom dictionary</li> </ul> <p>If you need to customize dic file, please add character_dict_path field in configs/rec/rec_icdar15_train.yml to point to your dictionary path. And set character_type to ch.</p>"},{"location":"en/ppocr/model_train/recognition.html#14-add-space-category","title":"1.4 Add Space Category","text":"<p>If you want to support the recognition of the <code>space</code> category, please set the <code>use_space_char</code> field in the yml file to <code>True</code>.</p>"},{"location":"en/ppocr/model_train/recognition.html#15-data-augmentation","title":"1.5 Data Augmentation","text":"<p>PaddleOCR provides a variety of data augmentation methods. All the augmentation methods are enabled by default.</p> <p>The default perturbation methods are: cvtColor, blur, jitter, Gasuss noise, random crop, perspective, color reverse, TIA augmentation.</p> <p>Each disturbance method is selected with a 40% probability during the training process. For specific code implementation, please refer to: rec_img_aug.py</p>"},{"location":"en/ppocr/model_train/recognition.html#2training","title":"2.Training","text":"<p>PaddleOCR provides training scripts, evaluation scripts, and prediction scripts. In this section, the CRNN recognition model will be used as an example:</p>"},{"location":"en/ppocr/model_train/recognition.html#21-start-training","title":"2.1 Start Training","text":"<p>First download the pretrain model, you can download the trained model to finetune on the icdar2015 data:</p> <pre><code>cd PaddleOCR/\n# Download the pre-trained model of en_PP-OCRv3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar\n# Decompress model parameters\ncd pretrain_models\ntar -xf en_PP-OCRv3_rec_train.tar &amp;&amp; rm -rf en_PP-OCRv3_rec_train.tar\n</code></pre> <p>Start training:</p> <pre><code># GPU training Support single card and multi-card training\n# Training icdar15 English data and The training log will be automatically saved as train.log under \"{save_model_dir}\"\n\n#specify the single card training(Long training time, not recommended)\npython3 tools/train.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy\n\n#specify the card number through --gpus\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy\n</code></pre> <p>PaddleOCR supports alternating training and evaluation. You can modify <code>eval_batch_step</code> in <code>configs/rec/rec_icdar15_train.yml</code> to set the evaluation frequency. By default, it is evaluated every 500 iter and the best acc model is saved under <code>output/rec_CRNN/best_accuracy</code> during the evaluation process.</p> <p>If the evaluation set is large, the test will be time-consuming. It is recommended to reduce the number of evaluations, or evaluate after training.</p> <ul> <li>Tip: You can use the <code>-c</code> parameter to select multiple model configurations under the <code>configs/rec/</code> path for training. The recognition algorithms supported at rec_algorithm:</li> </ul> <p>For training Chinese data, it is recommended to use ch_PP-OCRv3_rec_distillation.yml. If you want to try the result of other algorithms on the Chinese data set, please refer to the following instructions to modify the configuration file:</p> <p>Take <code>ch_PP-OCRv3_rec_distillation.yml</code> as an example:</p> <pre><code>Global:\n  ...\n  # Add a custom dictionary, such as modify the dictionary, please point the path to the new dictionary\n  character_dict_path: ppocr/utils/ppocr_keys_v1.txt\n  # Modify character type\n  ...\n  # Whether to recognize spaces\n  use_space_char: True\n\n\nOptimizer:\n  ...\n  # Add learning rate decay strategy\n  lr:\n    name: Cosine\n    learning_rate: 0.001\n  ...\n\n...\n\nTrain:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data/\n    # Path of train list\n    label_file_list: [\"./train_data/train_list.txt\"]\n    transforms:\n      ...\n      - RecResizeImg:\n          # Modify image_shape to fit long text\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    ...\n    # Train batch_size for Single card\n    batch_size_per_card: 256\n    ...\n\nEval:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data\n    # Path of eval list\n    label_file_list: [\"./train_data/val_list.txt\"]\n    transforms:\n      ...\n      - RecResizeImg:\n          # Modify image_shape to fit long text\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    # Eval batch_size for Single card\n    batch_size_per_card: 256\n    ...\n</code></pre> <p>Note that the configuration file for prediction/evaluation must be consistent with the training.</p>"},{"location":"en/ppocr/model_train/recognition.html#22-load-trained-model-and-continue-training","title":"2.2 Load Trained Model and Continue Training","text":"<p>If you expect to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <p>For example:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_icdar15_train.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrained_model</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrained_model</code> will be loaded.</p>"},{"location":"en/ppocr/model_train/recognition.html#23-training-with-new-backbone","title":"2.3 Training with New Backbone","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>If the Backbone to be replaced has a corresponding implementation in PaddleOCR, you can directly modify the parameters in the <code>Backbone</code> part of the configuration yml file.</p> <p>However, if you want to use a new Backbone, an example of replacing the backbones is as follows:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li>Add code in the my_backbone.py file, the sample code is as follows:</li> </ol> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> <ol> <li>Import the added module in the ppocr/modeling/backbones/_init_.py file.</li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>  Backbone:\n    name: MyBackbone\n    args1: args1\n</code></pre> <p>NOTE: More details about replace Backbone and other mudule can be found in doc.</p>"},{"location":"en/ppocr/model_train/recognition.html#24-mixed-precision-training","title":"2.4 Mixed Precision Training","text":"<p>If you want to speed up your training further, you can use Auto Mixed Precision Training, taking a single machine and a single gpu as an example, the commands are as follows:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_icdar15_train.yml \\\n     -o Global.pretrained_model=./pretrain_models/rec_mv3_none_bilstm_ctc_v2.0_train \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/ppocr/model_train/recognition.html#25-distributed-training","title":"2.5 Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_icdar15_train.yml \\\n     -o Global.pretrained_model=./pretrain_models/rec_mv3_none_bilstm_ctc_v2.0_train\n</code></pre> <p>Note: (1) When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. (2) Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>. (3) For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/ppocr/model_train/recognition.html#26-training-with-knowledge-distillation","title":"2.6 Training with Knowledge Distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for text recognition training process. For more details, please refer to doc.</p>"},{"location":"en/ppocr/model_train/recognition.html#27-multi-language-training","title":"2.7 Multi-language Training","text":"<p>Currently, the multi-language algorithms supported by PaddleOCR are:</p> Configuration file Algorithm name backbone trans seq pred language rec_chinese_cht_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc chinese traditional rec_en_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc English(Case sensitive) rec_french_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc French rec_ger_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc German rec_japan_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Japanese rec_korean_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Korean rec_latin_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Latin rec_arabic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc arabic rec_cyrillic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc cyrillic rec_devanagari_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc devanagari <p>For more supported languages, please refer to : Multi-language model</p> <p>If you want to finetune on the basis of the existing model effect, please refer to the following instructions to modify the configuration file:</p> <p>Take <code>rec_french_lite_train</code> as an example:</p> <pre><code>Global:\n  ...\n  # Add a custom dictionary, such as modify the dictionary, please point the path to the new dictionary\n  character_dict_path: ./ppocr/utils/dict/french_dict.txt\n  ...\n  # Whether to recognize spaces\n  use_space_char: True\n\n...\n\nTrain:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data/\n    # Path of train list\n    label_file_list: [\"./train_data/french_train.txt\"]\n    ...\n\nEval:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data\n    # Path of eval list\n    label_file_list: [\"./train_data/french_val.txt\"]\n    ...\n</code></pre>"},{"location":"en/ppocr/model_train/recognition.html#28-training-on-other-platformwindowsmacoslinux-dcu","title":"2.8 Training on other platform(Windows/macOS/Linux DCU)","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/ppocr/model_train/recognition.html#29-fine-tuning","title":"2.9 Fine-tuning","text":"<p>In actual use, it is recommended to load the official pre-trained model and fine-tune it in your own data set. For the fine-tuning method of the recognition model, please refer to: Model Fine-tuning Tutorial.</p>"},{"location":"en/ppocr/model_train/recognition.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/ppocr/model_train/recognition.html#31-evaluation","title":"3.1 Evaluation","text":"<p>The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file. The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml</code> file.</p> <pre><code># GPU evaluation, Global.checkpoints is the weight to be tested\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.checkpoints={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/ppocr/model_train/recognition.html#32-test","title":"3.2 Test","text":"<p>Using the model trained by paddleocr, you can quickly get prediction through the following script.</p> <p>The default prediction picture is stored in <code>infer_img</code>, and the trained weight is specified via <code>-o Global.checkpoints</code>:</p> <p>According to the <code>save_model_dir</code> and <code>save_epoch_step</code> fields set in the configuration file, the following parameters will be saved:</p> <pre><code>output/rec/\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.pdparams\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 iter_epoch_3.pdopt\n\u251c\u2500\u2500 iter_epoch_3.pdparams\n\u251c\u2500\u2500 iter_epoch_3.states\n\u251c\u2500\u2500 latest.pdopt\n\u251c\u2500\u2500 latest.pdparams\n\u251c\u2500\u2500 latest.states\n\u2514\u2500\u2500 train.log\n</code></pre> <p>Among them, best_accuracy.is the best model on the evaluation set; iter_epoch_x. is the model saved at intervals of <code>save_epoch_step</code>; latest.* is the model of the last epoch.</p> <pre><code># Predict English results\npython3 tools/infer_rec.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words/en/word_1.png\n        result: ('joint', 0.9998967)\n</code></pre> <p>The configuration file used for prediction must be consistent with the training. For example, you completed the training of the Chinese model with <code>python3 tools/train.py -c configs/rec/ch_ppocr_v2.0/rec_chinese_lite_train_v2.0.yml</code>, you can use the following command to predict the Chinese model:</p> <pre><code># Predict Chinese results\npython3 tools/infer_rec.py -c configs/rec/ch_ppocr_v2.0/rec_chinese_lite_train_v2.0.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/ch/word_1.jpg\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words/ch/word_1.jpg\n        result: ('\u200b\u97e9\u56fd\u200b\u5c0f\u9986\u200b', 0.997218)\n</code></pre>"},{"location":"en/ppocr/model_train/recognition.html#4-inference","title":"4. Inference","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>The recognition model is converted to the inference model in the same way as the detection, as follows:</p> <pre><code># -c Set the training algorithm yml configuration file\n# -o Set optional parameters\n# Global.pretrained_model parameter Set the training model address to be converted without adding the file suffix .pdmodel, .pdopt or .pdparams.\n# Global.save_inference_dir Set the address where the converted model will be saved.\n\npython3 tools/export_model.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy  Global.save_inference_dir=./inference/en_PP-OCRv3_rec/\n</code></pre> <p>If you have a model trained on your own dataset with a different dictionary file, please make sure that you modify the <code>character_dict_path</code> in the configuration file to your dictionary file path.</p> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/en_PP-OCRv3_rec/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition model\n</code></pre> <ul> <li>Text recognition model Inference using custom characters dictionary</li> </ul> <p>If the text dictionary is modified during training, when using the inference model to predict, you need to specify the dictionary path used by <code>--rec_char_dict_path</code></p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./your inference model\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"your text dict path\"\n</code></pre>"},{"location":"en/ppocr/model_train/recognition.html#5-faq","title":"5. FAQ","text":"<p>Q1: After the training model is transferred to the inference model, the prediction effect is inconsistent?</p> <p>A: There are many such problems, and the problems are mostly caused by inconsistent preprocessing and postprocessing parameters when the trained model predicts and the preprocessing and postprocessing parameters when the inference model predicts. You can compare whether there are differences in preprocessing, postprocessing, and prediction in the configuration files used for training.</p>"},{"location":"en/ppocr/model_train/training.html","title":"Model Training","text":"<p>This article will introduce the basic concepts that is necessary for model training and tuning.</p> <p>At the same time, it will briefly introduce the structure of the training data and how to prepare the data to fine-tune model in vertical scenes.</p>"},{"location":"en/ppocr/model_train/training.html#1-yml-configuration","title":"1. Yml Configuration","text":"<p>The PaddleOCR uses configuration files to control network training and evaluation parameters. In the configuration file, you can set the model, optimizer, loss function, and pre- and post-processing parameters of the model. PaddleOCR reads these parameters from the configuration file, and then builds a complete training process to train the model. Fine-tuning can also be completed by modifying the parameters in the configuration file, which is simple and convenient.</p> <p>For the complete configuration file description, please refer to Configuration File</p>"},{"location":"en/ppocr/model_train/training.html#2-basic-concepts","title":"2. Basic Concepts","text":"<p>During the model training process, some hyper-parameters can be manually specified to obtain the optimal result at the least cost. Different data volumes may require different hyper-parameters. When you want to fine-tune the model based on your own data, there are several parameter adjustment strategies for reference:</p>"},{"location":"en/ppocr/model_train/training.html#21-learning-rate","title":"2.1 Learning Rate","text":"<p>The learning rate is one of the most important hyper-parameters for training neural networks. It represents the step length of the gradient moving towards the optimal solution of the loss function in each iteration. A variety of learning rate update strategies are provided by PaddleOCR, which can be specified in configuration files. For example,</p> <pre><code>Optimizer:\n  ...\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]\n    warmup_epoch: 5\n</code></pre> <p><code>Piecewise</code> stands for piece-wise constant attenuation. Different learning rates are specified in different learning stages, and the learning rate stay the same in each stage.</p> <p><code>warmup_epoch</code> means that in the first 5 epochs, the learning rate will be increased gradually from 0 to base_lr. For all strategies, please refer to the code learning_rate.py.</p>"},{"location":"en/ppocr/model_train/training.html#22-regularization","title":"2.2 Regularization","text":"<p>Regularization can effectively avoid algorithm over-fitting. PaddleOCR provides L1 and L2 regularization methods. L1 and L2 regularization are the most widely used regularization methods. L1 regularization adds a regularization term to the objective function to reduce the sum of absolute values of the parameters; while in L2 regularization, the purpose of adding a regularization term is to reduce the sum of squared parameters. The configuration method is as follows:</p> <pre><code>Optimizer:\n  ...\n  regularizer:\n    name: L2\n    factor: 2.0e-05\n</code></pre>"},{"location":"en/ppocr/model_train/training.html#23-evaluation-indicators","title":"2.3 Evaluation Indicators","text":"<p>(1) Detection stage: First, evaluate according to the IOU of the detection frame and the labeled frame. If the IOU is greater than a certain threshold, it is judged that the detection is accurate. Here, the detection frame and the label frame are different from the general general target detection frame, and they are represented by polygons. Detection accuracy: the percentage of the correct detection frame number in all detection frames is mainly used to judge the detection index. Detection recall rate: the percentage of correct detection frames in all marked frames, which is mainly an indicator of missed detection.</p> <p>(2) Recognition stage: Character recognition accuracy, that is, the ratio of correctly recognized text lines to the number of marked text lines. Only the entire line of text recognition pairs can be regarded as correct recognition.</p> <p>(3) End-to-end statistics: End-to-end recall rate: accurately detect and correctly identify the proportion of text lines in all labeled text lines; End-to-end accuracy rate: accurately detect and correctly identify the number of text lines in the detected text lines The standard for accurate detection is that the IOU of the detection box and the labeled box is greater than a certain threshold, and the text in the correctly identified detection box is the same as the labeled text.</p>"},{"location":"en/ppocr/model_train/training.html#3-data-and-vertical-scenes","title":"3. Data and Vertical Scenes","text":""},{"location":"en/ppocr/model_train/training.html#31-training-data","title":"3.1 Training Data","text":"<p>The current open source models, data sets and magnitudes are as follows:</p> <ul> <li>Detection:</li> <li>English data set, ICDAR2015</li> <li> <p>Chinese data set, LSVT street view data set training data 3w pictures</p> </li> <li> <p>Identification:</p> </li> <li>English data set, MJSynth and SynthText synthetic data, the data volume is tens of millions.</li> <li>Chinese data set, LSVT street view data set crops the image according to the truth value, and performs position calibration, a total of 30w images. In addition, based on the LSVT corpus, 500w of synthesized data.</li> <li>Small language data set, using different corpora and fonts, respectively generated 100w synthetic data set, and using ICDAR-MLT as the verification set.</li> </ul> <p>Among them, the public data sets are all open source, users can search and download by themselves, or refer to Chinese data set, synthetic data is not open source, users can use open source synthesis tools to synthesize by themselves. Synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator etc.</p>"},{"location":"en/ppocr/model_train/training.html#32-vertical-scene","title":"3.2 Vertical Scene","text":"<p>PaddleOCR mainly focuses on general OCR. If you have vertical requirements, you can use PaddleOCR + vertical data to train yourself; If there is a lack of labeled data, or if you do not want to invest in research and development costs, it is recommended to directly call the open API, which covers some of the more common vertical categories.</p>"},{"location":"en/ppocr/model_train/training.html#33-build-your-own-dataset","title":"3.3 Build Your Own Dataset","text":"<p>There are several experiences for reference when constructing the data set:</p> <p>(1) The amount of data in the training set:</p> <p>a. The data required for detection is relatively small. For Fine-tune based on the PaddleOCR model, 500 sheets are generally required to achieve good results.</p> <p>b. Recognition is divided into English and Chinese. Generally, English scenarios require hundreds of thousands of data to achieve good results, while Chinese requires several million or more.</p> <p>(2) When the amount of training data is small, you can try the following three ways to get more data:</p> <p>a. Manually collect more training data, the most direct and effective way.</p> <p>b. Basic image processing or transformation based on PIL and opencv. For example, the three modules of ImageFont, Image, ImageDraw in PIL write text into the background, opencv's rotating affine transformation, Gaussian filtering and so on.</p> <p>c. Use data generation algorithms to synthesize data, such as algorithms such as pix2pix.</p>"},{"location":"en/ppocr/model_train/training.html#4-faq","title":"4. FAQ","text":"<p>Q: How to choose a suitable network input shape when training CRNN recognition?</p> <pre><code>A: The general height is 32, the longest width is selected, there are two methods:\n\n(1) Calculate the aspect ratio distribution of training sample images. The selection of the maximum aspect ratio considers 80% of the training samples.\n\n(2) Count the number of texts in training samples. The selection of the longest number of characters considers the training sample that satisfies 80%. Then the aspect ratio of Chinese characters is approximately considered to be 1, and that of English is 3:1, and the longest width is estimated.\n</code></pre> <p>Q: During the recognition training, the accuracy of the training set has reached 90, but the accuracy of the verification set has been kept at 70, what should I do?</p> <pre><code>A: If the accuracy of the training set is 90 and the test set is more than 70, it should be over-fitting. There are two methods to try:\n\n(1) Add more augmentation methods or increase the [probability] of augmented prob (https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppocr/data/imaug/rec_img_aug.py#L341), The default is 0.4.\n\n(2) Increase the [l2 dcay value] of the system (https://github.com/PaddlePaddle/PaddleOCR/blob/a501603d54ff5513fc4fc760319472e59da25424/configs/rec/ch_ppocr_v1.1/rec_chinese_lite_train_v1.1.yml#L47)\n</code></pre> <p>Q: When the recognition model is trained, loss can drop normally, but acc is always 0</p> <pre><code>A: It is normal for the acc to be 0 at the beginning of the recognition model training, and the indicator will come up after a longer training period.\n</code></pre> <p>Click the following links for detailed training tutorial:</p> <ul> <li>text detection model training</li> <li>text recognition model training</li> <li>text direction classification model training</li> </ul>"},{"location":"en/ppstructure/models_list.html","title":"PP-Structure Model list","text":""},{"location":"en/ppstructure/models_list.html#1-layout-analysis","title":"1. Layout Analysis","text":"model name description inference model size download dict path picodet_lcnet_x1_0_fgd_layout The layout analysis English model trained on the PubLayNet dataset based on PicoDet LCNet_x1_0 and FGD . the model can recognition 5 types of areas such as Text, Title, Table, Picture and List 9.7M inference model / trained model PubLayNet dict ppyolov2_r50vd_dcn_365e_publaynet The layout analysis English model trained on the PubLayNet dataset based on PP-YOLOv2 221.0M inference_moel / trained model same as above picodet_lcnet_x1_0_fgd_layout_cdla The layout analysis Chinese model trained on the CDLA dataset, the model can recognition 10 types of areas such as Table\u3001Figure\u3001Figure caption\u3001Table\u3001Table caption\u3001Header\u3001Footer\u3001Reference\u3001Equation 9.7M inference model / trained model CDLA dict picodet_lcnet_x1_0_fgd_layout_table The layout analysis model trained on the table dataset, the model can detect tables in Chinese and English documents 9.7M inference model / trained model Table dict ppyolov2_r50vd_dcn_365e_tableBank_word The layout analysis model trained on the TableBank Word dataset based on PP-YOLOv2, the model can detect  tables  in English documents 221.0M inference model same as above ppyolov2_r50vd_dcn_365e_tableBank_latex The layout analysis model trained on the TableBank Latex dataset based on PP-YOLOv2, the model can detect  tables  in English documents 221.0M inference model same as above"},{"location":"en/ppstructure/models_list.html#2-ocr-and-table-recognition","title":"2. OCR and Table Recognition","text":""},{"location":"en/ppstructure/models_list.html#21-ocr","title":"2.1 OCR","text":"model name description inference model size download en_ppocr_mobile_v2.0_table_det Text detection model of English table scenes trained on PubTabNet dataset 4.7M inference model / trained model en_ppocr_mobile_v2.0_table_rec Text recognition model of English table scenes trained on PubTabNet dataset 6.9M inference model / trained model <p>If you need to use other OCR models, you can download the model in PP-OCR model_list or use the model you trained yourself to configure to <code>det_model_dir</code>, <code>rec_model_dir</code> field.</p>"},{"location":"en/ppstructure/models_list.html#22-table-recognition","title":"2.2 Table Recognition","text":"model description inference model size download en_ppocr_mobile_v2.0_table_structure English table recognition model trained on PubTabNet dataset based on TableRec-RARE 6.8M inference model / trained model en_ppstructure_mobile_v2.0_SLANet English table recognition model trained on PubTabNet dataset based on SLANet 9.2M inference model / trained model ch_ppstructure_mobile_v2.0_SLANet Chinese table recognition model based on SLANet 9.3M inference model / trained model"},{"location":"en/ppstructure/models_list.html#3-kie","title":"3. KIE","text":"<p>On XFUND_zh dataset, Accuracy and time cost of different models on V100 GPU are as follows.</p> Model Backbone Task Config Hmean Time cost(ms) Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% 15.49 trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% 19.49 trained model LayoutLM LayoutLM-base SER ser_layoutlm_xfund_zh.yml 77.31% - trained model LayoutLMv2 LayoutLMv2-base SER ser_layoutlmv2_xfund_zh.yml 85.44% 31.46 trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% 15.49 trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% 19.49 trained model LayoutLMv2 LayoutLMv2-base RE re_layoutlmv2_xfund_zh.yml 67.77% 31.46 trained model <ul> <li>Note: The above time cost information just considers inference time without preprocess or postprocess, test environment: <code>V100 GPU + CUDA 10.2 + CUDNN 8.1.1 + TRT 7.2.3.4</code></li> </ul> <p>On wildreceipt dataset, the algorithm result is as follows:</p> Model Backbone Config Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model"},{"location":"en/ppstructure/overview.html","title":"PP-Structure","text":""},{"location":"en/ppstructure/overview.html#1-introduction","title":"1. Introduction","text":"<p>PP-Structure is an intelligent document analysis system developed by the PaddleOCR team, which aims to help developers better complete tasks related to document understanding such as layout analysis and table recognition.</p> <p>The pipeline of PP-StructureV2 system is shown below. The document image first passes through the image direction correction module to identify the direction of the entire image and complete the direction correction. Then, two tasks of layout information analysis and key information extraction can be completed.</p> <ul> <li>In the layout analysis task, the image first goes through the layout analysis model to divide the image into different areas such as text, table, and figure, and then analyze these areas separately. For example, the table area is sent to the form recognition module for structured recognition, and the text area is sent to the OCR engine for text recognition. Finally, the layout recovery module restores it to a word or pdf file with the same layout as the original image;</li> <li>In the key information extraction task, the OCR engine is first used to extract the text content, and then the SER(semantic entity recognition) module obtains the semantic entities in the image, and finally the RE(relationship extraction) module obtains the correspondence between the semantic entities, thereby extracting the required key information.</li> </ul> <p></p> <p>More technical details: \ud83d\udc49 PP-StructureV2 Technical Report</p> <p>PP-StructureV2 supports independent use or flexible collocation of each module. For example, you can use layout analysis alone or table recognition alone. Click the corresponding link below to get the tutorial for each independent module:</p> <ul> <li>Layout Analysis</li> <li>Table Recognition</li> <li>Key Information Extraction</li> <li>Layout Recovery</li> </ul>"},{"location":"en/ppstructure/overview.html#2-features","title":"2. Features","text":"<p>The main features of PP-StructureV2 are as follows:</p> <ul> <li>Support layout analysis of documents in the form of images/pdfs, which can be divided into areas such as text, titles, tables, figures, formulas, etc.;</li> <li>Support common Chinese and English table detection tasks;</li> <li>Support structured table recognition, and output the final result to Excel file;</li> <li>Support multimodal-based Key Information Extraction (KIE) tasks - Semantic Entity Recognition (SER) and **Relation Extraction (RE);</li> <li>Support layout recovery, that is, restore the document in word or pdf format with the same layout as the original image;</li> <li>Support customized training and multiple inference deployment methods such as python whl package quick start;</li> <li>Connect with the semi-automatic data labeling tool PPOCRLabel, which supports the labeling of layout analysis, table recognition, and SER.</li> </ul>"},{"location":"en/ppstructure/overview.html#3-results","title":"3. Results","text":"<p>PP-StructureV2 supports the independent use or flexible collocation of each module. For example, layout analysis can be used alone, or table recognition can be used alone. Only the visualization effects of several representative usage methods are shown here.</p>"},{"location":"en/ppstructure/overview.html#31-layout-analysis-and-table-recognition","title":"3.1 Layout analysis and table recognition","text":"<p>The figure shows the pipeline of layout analysis + table recognition. The image is first divided into four areas of image, text, title and table by layout analysis, and then OCR detection and recognition is performed on the three areas of image, text and title, and the table is performed table recognition, where the image will also be stored for use.</p> <p></p>"},{"location":"en/ppstructure/overview.html#311-layout-recognition-returns-the-coordinates-of-a-single-word","title":"3.1.1 Layout recognition returns the coordinates of a single word","text":"<p>The following figure shows the result of layout analysis on single word\uff0c please refer to the doc.</p> <p></p>"},{"location":"en/ppstructure/overview.html#32-layout-recovery","title":"3.2 Layout recovery","text":"<p>The following figure shows the effect of layout recovery based on the results of layout analysis and table recognition in the previous section.</p> <p></p>"},{"location":"en/ppstructure/overview.html#33-kie","title":"3.3 KIE","text":"<ul> <li>SER</li> </ul> <p>Different colored boxes in the figure represent different categories.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <ul> <li>RE</li> </ul> <p>In the figure, the red box represents <code>Question</code>, the blue box represents <code>Answer</code>, and <code>Question</code> and <code>Answer</code> are connected by green lines.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"en/ppstructure/overview.html#4-quick-start","title":"4. Quick start","text":"<p>Start from Quick Start.</p>"},{"location":"en/ppstructure/overview.html#5-model-list","title":"5. Model List","text":"<p>Some tasks need to use both the structured analysis models and the OCR models. For example, the table recognition task needs to use the table recognition model for structured analysis, and the OCR model to recognize the text in the table. Please select the appropriate models according to your specific needs.</p> <p>For structural analysis related model downloads, please refer to:</p> <ul> <li>PP-Structure Model Zoo</li> </ul> <p>For OCR related model downloads, please refer to:</p> <ul> <li>PP-OCR Model Zoo</li> </ul>"},{"location":"en/ppstructure/ppstructure_model.html","title":"PP-Structure \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u5217\u8868","text":""},{"location":"en/ppstructure/ppstructure_model.html#1","title":"1. \u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b dict path picodet_lcnet_x1_0_fgd_layout \u200b\u57fa\u4e8e\u200bPicoDet LCNet_x1_0\u200b\u548c\u200bFGD\u200b\u84b8\u998f\u200b\u5728\u200bPubLayNet \u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5212\u5206\u200b\u6587\u5b57\u200b\u3001\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u7247\u200b\u4ee5\u53ca\u200b\u5217\u8868\u200b5\u200b\u7c7b\u200b\u533a\u57df\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PubLayNet dict ppyolov2_r50vd_dcn_365e_publaynet \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bPubLayNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a\u200b picodet_lcnet_x1_0_fgd_layout_cdla CDLA\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u4e2d\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5212\u5206\u200b\u4e3a\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u7247\u200b\u3001\u200b\u56fe\u7247\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u9898\u200b\u3001\u200b\u9875\u7709\u200b\u3001\u200b\u811a\u672c\u200b\u3001\u200b\u5f15\u7528\u200b\u3001\u200b\u516c\u5f0f\u200b10\u200b\u7c7b\u200b\u533a\u57df\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b CDLA dict picodet_lcnet_x1_0_fgd_layout_table \u200b\u8868\u683c\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u4e2d\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b Table dict ppyolov2_r50vd_dcn_365e_tableBank_word \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bTableBank Word \u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a\u200b ppyolov2_r50vd_dcn_365e_tableBank_latex \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bTableBank Latex\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a"},{"location":"en/ppstructure/ppstructure_model.html#2-ocr","title":"2. OCR\u200b\u548c\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":""},{"location":"en/ppstructure/ppstructure_model.html#21-ocr","title":"2.1 OCR","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b en_ppocr_mobile_v2.0_table_det PubTabNet\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b 4.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b en_ppocr_mobile_v2.0_table_rec PubTabNet\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b 6.9M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u5982\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200bOCR\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b PP-OCR model_list \u200b\u4e0b\u8f7d\u200b\u6a21\u578b\u200b\u6216\u8005\u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u914d\u7f6e\u200b\u5230\u200b <code>det_model_dir</code>, <code>rec_model_dir</code>\u200b\u4e24\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"en/ppstructure/ppstructure_model.html#22","title":"2.2 \u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b en_ppocr_mobile_v2.0_table_structure \u200b\u57fa\u4e8e\u200bTableRec-RARE\u200b\u5728\u200bPubTabNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 6.8M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b en_ppstructure_mobile_v2.0_SLANet \u200b\u57fa\u4e8e\u200bSLANet\u200b\u5728\u200bPubTabNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 9.2M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ch_ppstructure_mobile_v2.0_SLANet \u200b\u57fa\u4e8e\u200bSLANet\u200b\u7684\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 9.3M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/ppstructure/ppstructure_model.html#3-kie","title":"3. KIE\u200b\u6a21\u578b","text":"<p>\u200b\u5728\u200bXFUND_zh\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0c\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e0e\u200bV100 GPU\u200b\u4e0a\u200b\u901f\u5ea6\u200b\u4fe1\u606f\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b(hmean) \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b(ms) \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b ser_VI-LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 1.1G 93.19% 15.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_VI-LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 1.1G 83.92% 15.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 1.4G 90.38% 19.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 1.4G 74.83% 19.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutLMv2_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLMv2\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 778.0M 85.44% 31.46 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_LayoutLMv2_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLMv2\u200b\u5728\u200bxfun\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 765.0M 67.77% 31.46 \u200b\u63a8\u7406\u6a21\u578b\u200b coming soon / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 430.0M 77.31% - \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <ul> <li>\u200b\u6ce8\u200b\uff1a\u200b\u4e0a\u8ff0\u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\u4fe1\u606f\u200b\u4ec5\u200b\u5305\u542b\u200b\u4e86\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff0c\u200b\u6ca1\u6709\u200b\u7edf\u8ba1\u200b\u9884\u5904\u7406\u200b\u4e0e\u200b\u540e\u5904\u7406\u200b\u8017\u65f6\u200b\uff0c\u200b\u6d4b\u8bd5\u73af\u5883\u200b\u4e3a\u200b<code>V100 GPU + CUDA 10.2 + CUDNN 8.1.1 + TRT 7.2.3.4</code>\u3002</li> </ul> <p>\u200b\u5728\u200bwildreceipt\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0cSDMGR\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u4e0e\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b SDMGR \u200b\u5173\u952e\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u6a21\u578b\u200b 78.0M 86.70% \u200b\u63a8\u7406\u6a21\u578b\u200b coming soon / \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/ppstructure/quick_start.html","title":"PP-Structure Quick Start","text":""},{"location":"en/ppstructure/quick_start.html#1-environment-preparation","title":"1. Environment Preparation","text":""},{"location":"en/ppstructure/quick_start.html#11-install-paddlepaddle","title":"1.1 Install PaddlePaddle","text":"<p>If you do not have a Python environment, please refer to Environment Preparation.</p> <ul> <li>If you have CUDA 9 or CUDA 10 installed on your machine, please run the following command to install</li> </ul> <pre><code>python3 -m pip install paddlepaddle-gpu -i https://mirror.baidu.com/pypi/simple\n</code></pre> <ul> <li>If you have no available GPU on your machine, please run the following command to install the CPU version</li> </ul> <pre><code>python3 -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n</code></pre> <p>For more software version requirements, please refer to the instructions in Installation Document for operation.</p>"},{"location":"en/ppstructure/quick_start.html#12-install-paddleocr-whl-package","title":"1.2 Install PaddleOCR Whl Package","text":"<pre><code># Install paddleocr, version 2.6 is recommended\npip3 install \"paddleocr&gt;=2.6.0.3\"\n\n# Install the image direction classification dependency package paddleclas (if you do not use the image direction classification, you can skip it)\npip3 install paddleclas&gt;=2.4.3\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#2-quick-use","title":"2. Quick Use","text":""},{"location":"en/ppstructure/quick_start.html#21-use-by-command-line","title":"2.1 Use by command line","text":""},{"location":"en/ppstructure/quick_start.html#211-image-orientation-layout-analysis-table-recognition","title":"2.1.1 image orientation + layout analysis + table recognition","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --image_orientation=true\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#212-layout-analysis-table-recognition","title":"2.1.2 layout analysis + table recognition","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#213-layout-analysis","title":"2.1.3 layout analysis","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --table=false --ocr=false\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#214-table-recognition","title":"2.1.4 table recognition","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/table.jpg --type=structure --layout=false\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#215-key-information-extraction","title":"2.1.5 Key Information Extraction","text":"<p>Key information extraction does not currently support use by the whl package. For detailed usage tutorials, please refer to: inference document.</p>"},{"location":"en/ppstructure/quick_start.html#216-layout-recoverypdf-to-word","title":"2.1.6 layout recovery(PDF to Word)","text":"<p>Two layout recovery methods are provided, For detailed usage tutorials, please refer to: Layout Recovery.</p> <ul> <li>PDF parse</li> <li>OCR</li> </ul> <p>Recovery by using PDF parse (only support pdf as input):</p> <pre><code>paddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --use_pdf2docx_api=true\n</code></pre> <p>Recovery by using OCR\uff1a</p> <pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --recovery=true --lang='en'\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#22-use-by-python-script","title":"2.2 Use by python script","text":""},{"location":"en/ppstructure/quick_start.html#221-image-orientation-layout-analysis-table-recognition","title":"2.2.1 image orientation + layout analysis + table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,draw_structure_result,save_structure_res\n\ntable_engine = PPStructure(show_log=True, image_orientation=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder,os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nfrom PIL import Image\n\nfont_path = 'doc/fonts/simfang.ttf' # PaddleOCR\u200b\u4e0b\u200b\u63d0\u4f9b\u200b\u5b57\u4f53\u200b\u5305\u200b\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_structure_result(image, result,font_path=font_path)\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#222-layout-analysis-table-recognition","title":"2.2.2 layout analysis + table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,draw_structure_result,save_structure_res\n\ntable_engine = PPStructure(show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder,os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nfrom PIL import Image\n\nfont_path = 'doc/fonts/simfang.ttf' # font provided in PaddleOCR\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_structure_result(image, result,font_path=font_path)\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#223-layout-analysis","title":"2.2.3 layout analysis","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\ntable_engine = PPStructure(table=False, ocr=False, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n</code></pre> <pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\nocr_engine = PPStructure(table=False, ocr=True, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/recovery/UnrealText.pdf'\nresult = ocr_engine(img_path)\nfor index, res in enumerate(result):\n    save_structure_res(res, save_folder, os.path.basename(img_path).split('.')[0], index)\n\nfor res in result:\n    for line in res:\n        line.pop('img')\n        print(line)\n</code></pre> <pre><code>import os\nimport cv2\nimport numpy as np\nfrom paddleocr import PPStructure,save_structure_res\nfrom paddle.utils import try_import\nfrom PIL import Image\n\nocr_engine = PPStructure(table=False, ocr=True, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/recovery/UnrealText.pdf'\n\nfitz = try_import(\"fitz\")\nimgs = []\nwith fitz.open(img_path) as pdf:\n    for pg in range(0, pdf.page_count):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.get_pixmap(matrix=mat, alpha=False)\n\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\n\nfor index, img in enumerate(imgs):\n    result = ocr_engine(img)\n    save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0], index)\n    for line in result:\n        line.pop('img')\n        print(line)\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#224-table-recognition","title":"2.2.4 table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\ntable_engine = PPStructure(layout=False, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/table.jpg'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#225-key-information-extraction","title":"2.2.5 Key Information Extraction","text":"<p>Key information extraction does not currently support use by the whl package. For detailed usage tutorials, please refer to: Inference.</p>"},{"location":"en/ppstructure/quick_start.html#226-layout-recovery","title":"2.2.6 layout recovery","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\nfrom paddleocr.ppstructure.recovery.recovery_to_doc import sorted_layout_boxes, convert_info_docx\n\n# Chinese image\ntable_engine = PPStructure(recovery=True)\n# English image\n# table_engine = PPStructure(recovery=True, lang='en')\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nh, w, _ = img.shape\nres = sorted_layout_boxes(result, w)\nconvert_info_docx(img, res, save_folder, os.path.basename(img_path).split('.')[0])\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#23-result-description","title":"2.3 Result description","text":"<p>The return of PP-Structure is a list of dicts, the example is as follows:</p>"},{"location":"en/ppstructure/quick_start.html#231-layout-analysis-table-recognition","title":"2.3.1 layout analysis + table recognition","text":"<pre><code>[\n  {   'type': 'Text',\n      'bbox': [34, 432, 345, 462],\n      'res': ([[36.0, 437.0, 341.0, 437.0, 341.0, 446.0, 36.0, 447.0], [41.0, 454.0, 125.0, 453.0, 125.0, 459.0, 41.0, 460.0]],\n                [('Tigure-6. The performance of CNN and IPT models using difforen', 0.90060663), ('Tent  ', 0.465441)])\n  }\n]\n</code></pre> <p>Each field in dict is described as follows:</p> field description type Type of image area. bbox The coordinates of the image area in the original image, respectively [upper left corner x, upper left corner y, lower right corner x, lower right corner y]. res OCR or table recognition result of the image area.  table: a dict with field descriptions as follows:  <code>html</code>: html str of table.\u2003\u2003\u2003\u2003\u2003\u2003\u2003 In the code usage mode, set return_ocr_result_in_table=True whrn call can get the detection and recognition results of each text in the table area, corresponding to the following fields:  <code>boxes</code>: text detection boxes. <code>rec_res</code>: text recognition results. OCR: A tuple containing the detection boxes and recognition results of each single text. <p>After the recognition is completed, each image will have a directory with the same name under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel, and the picture area will be cropped and saved. The filename of  excel and picture is their coordinates in the image.</p> <pre><code>/output/table/1/\n  \u2514\u2500 res.txt\n  \u2514\u2500 [454, 360, 824, 658].xlsx        table recognition result\n  \u2514\u2500 [16, 2, 828, 305].jpg            picture in Image\n  \u2514\u2500 [17, 361, 404, 711].xlsx        table recognition result\n</code></pre>"},{"location":"en/ppstructure/quick_start.html#232-key-information-extraction","title":"2.3.2 Key Information Extraction","text":"<p>Please refer to: Key Information Extraction .</p>"},{"location":"en/ppstructure/quick_start.html#24-parameter-description","title":"2.4 Parameter Description","text":"field description default output result save path ./output/table table_max_len long side of the image resize in table structure model 488 table_model_dir Table structure model inference model path None table_char_dict_path The dictionary path of table structure model ../ppocr/utils/dict/table_structure_dict.txt merge_no_span_structure In the table recognition model, whether to merge '\\' and '\\' False layout_model_dir Layout analysis model inference model path None layout_dict_path The dictionary path of layout analysis model ../ppocr/utils/dict/layout_publaynet_dict.txt layout_score_threshold The box threshold path of layout analysis model 0.5 layout_nms_threshold The nms threshold path of layout analysis model 0.5 kie_algorithm kie model algorithm LayoutXLM ser_model_dir Ser model inference model path None ser_dict_path The dictionary path of Ser model ../train_data/XFUND/class_list_xfun.txt mode structure or kie structure image_orientation Whether to perform image orientation classification in forward False layout Whether to perform layout analysis in forward True table Whether to perform table recognition in forward True ocr Whether to perform ocr for non-table areas in layout analysis. When layout is False, it will be automatically set to False True recovery Whether to perform layout recovery in forward False save_pdf Whether to convert docx to pdf when recovery False structure_version Structure version, optional PP-structure and PP-structurev2 PP-structure <p>Most of the parameters are consistent with the PaddleOCR whl package, see whl package documentation</p>"},{"location":"en/ppstructure/quick_start.html#3-summary","title":"3. Summary","text":"<p>Through the content in this section, you can master the use of PP-Structure related functions through PaddleOCR whl package. Please refer to documentation tutorial for more detailed usage tutorials including model training, inference and deployment, etc.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html","title":"Key Information Extraction Pipeline","text":""},{"location":"en/ppstructure/blog/how_to_do_kie.html#1-introduction","title":"1. Introduction","text":""},{"location":"en/ppstructure/blog/how_to_do_kie.html#11-background","title":"1.1 Background","text":"<p>Key information extraction (KIE) refers to extracting key information from text or images. As the downstream task of OCR, KIE of document image has many practical application scenarios, such as form recognition, ticket information extraction, ID card information extraction, etc. However, it is time-consuming and laborious to extract  key information from these document images by manpower. It's challengable but also valuable to combine multi-modal features (visual, layout, text, etc) together and complete KIE tasks.</p> <p>For the document images in a specific scene, the position and layout of the key information are relatively fixed. Therefore, in the early stage of the research, there are many methods based on template matching to extract the key information. This method is still widely used in many simple scenarios at present. However, it takes long time to adjut the template for different scenarios.</p> <p>The KIE in the document image generally contains 2 subtasks, which is as shown follows.</p> <ul> <li> <p>(1) SER: semantic entity recognition, which classifies each detected textline, such as dividing it into name and ID No. As shown in the red boxes in the following figure.</p> </li> <li> <p>(2) RE: relationship extraction, which matches the question and answer based on SER results. As shown in the figure below, the yellow arrows match the question and answer.</p> </li> </ul> <p></p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#12-mainstream-deep-learning-solutions","title":"1.2 Mainstream Deep-learning Solutions","text":"<p>General KIE methods are based on Named Entity Recognition (NER), but such methods only use text information and ignore location and visual feature information, which leads to limited accuracy. In recent years, most scholars have started to combine mutil-modal features to improve the accuracy of KIE model. The main methods are as follows:</p> <ul> <li> <p>(1) Grid based methods. These methods mainly focus on the fusion of multi-modal information at the image level. Most texts are of character granularity. The text and structure information embedding method is simple, such as the algorithm of chargrid [1].</p> </li> <li> <p>(2) Token based methods. These methods refer to the NLP methods such as Bert, which encode the position, vision and other feature information into the multi-modal model, and conduct pre-training on large-scale datasets, so that in downstream tasks, only a small amount of annotation data is required to obtain excellent results. The representative algorithms are layoutlm [2], layoutlmv2 [3], layoutxlm [4], structext [5], etc.</p> </li> <li> <p>(3) GCN based methods. These methods try to learn the structural information between images and characters, so as to solve the problem of extracting open set information (templates not seen in the training set), such as GCN [6], SDMGR [7] and other algorithms.</p> </li> <li> <p>(4) End to end based methods: these methods put the existing OCR character recognition and KIE information extraction tasks into a unified network for common learning, and strengthen each other in the learning process. Such as TRIE [8].</p> </li> </ul> <p>For more detailed introduction of the algorithms, please refer to Chapter 6 of Diving into OCR.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#2-kie-pipeline","title":"2. KIE Pipeline","text":"<p>Token based methods such as LayoutXLM are implemented in PaddleOCR. What's more, in PP-StructureV2, we simplify the LayoutXLM model and proposed VI-LayoutXLM, in which the visual feature extraction module is removed for speed-up. The textline sorting strategy conforming to the human reading order and UDML knowledge distillation strategy are utilized for higher model accuracy.</p> <p>In the non end-to-end KIE method, KIE needs at least 2 steps. Firstly, the OCR model is used to extract the text and its position. Secondly, the KIE model is used to extract the key information according to the image, text position and text content.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#21-train-ocr-models","title":"2.1 Train OCR Models","text":""},{"location":"en/ppstructure/blog/how_to_do_kie.html#211-text-detection","title":"2.1.1 Text Detection","text":""},{"location":"en/ppstructure/blog/how_to_do_kie.html#1-data","title":"(1) Data","text":"<p>Most of the models provided in PaddleOCR are general models. In the process of text detection, the detection of adjacent text lines is generally based on the distance of the position. As shown in the figure above, when using PP-OCRv3 general English detection model for text detection, it is easy to detect the two fields representing different propoerties as one. Therefore, it is suggested to finetune a detection model according to your scenario firstly during the KIE task.</p> <p>During data annotation, the different key information needs to be separated. Otherwise, it will increase the difficulty of subsequent KIE tasks.</p> <p>For downstream tasks, generally speaking, <code>200~300</code> training images can guarantee the basic training effect. If there is not too much prior knowledge, <code>200~300</code> images can be labeled firstly for subsequent text detection model training.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#2-model","title":"(2) Model","text":"<p>In terms of model selection, PP-OCRv3 detection model is recommended. For more information about the training methods of the detection model, please refer to: Text detection tutorial and PP-OCRv3 detection model tutorial.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#212-text-recognition","title":"2.1.2 Text recognition","text":"<p>Compared with the natural scene, the text recognition in the document image is generally relatively easier (the background is not too complex), so it is suggested to try the PP-OCRv3 general text recognition model provided in PaddleOCR (PP-OCRv3 model list)</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#1-data_1","title":"(1) Data","text":"<p>However, there are also some challenges in some document scenarios, such as rare words in ID card scenarios and special fonts in invoice and other scenarios. These problems will increase the difficulty of text recognition. At this time, if you want to ensure or further improve the model accuracy, it is recommended to load PP-OCRv3 model based on the text recognition dataset of specific document scenarios for finetuning.</p> <p>In the process of model finetuning, it is recommended to prepare at least <code>5000</code> vertical scene text recognition images to ensure the basic model fine-tuning effect. If you want to improve the accuracy and generalization ability of the model, you can synthesize more text recognition images similar to the scene, collect general real text recognition data from the public data set, and add them to the text recognition training process. In the training process, it is suggested that the ratio of real data, synthetic data and general data of each epoch should be around <code>1:1:1</code>, which can be controlled by setting the sampling ratio of different data sources. If there are 3 training text files, including 10k, 20k and 50k pieces of data respectively, the data can be set in the configuration file as follows:</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - ./train_data/train_list_10k.txt\n    - ./train_data/train_list_10k.txt\n    - ./train_data/train_list_50k.txt\n    ratio_list: [1.0, 0.5, 0.2]\n    ...\n</code></pre>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#2-model_1","title":"(2) Model","text":"<p>In terms of model selection, PP-OCRv3 recognition model is recommended. For more information about the training methods of the recognition model, please refer to: Text recognition tutorial and PP-OCRv3 model list.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#22-train-kie-models","title":"2.2 Train KIE Models","text":"<p>There are two main methods to extract the key information from the recognized texts.</p> <p>(1) Directly use SER model to obtain the key information category. For example, in the ID card scenario, we mark \"name\" and \"Geoff Sample\" as \"name_key\" and \"name_value\", respectively. The text field corresponding to the category \"name_value\" finally identified is the key information we need.</p> <p>(2) Joint use SER and RE models. For this case, we firstly use SER model to obtain all questions (keys) and questions (values) for the image text, and then use RE model to match all keys and values to find the relationship, so as to complete the extraction of key information.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#221-ser","title":"2.2.1 SER","text":"<p>Take the ID card scenario as an example. The key information generally includes <code>name</code>, <code>DOB</code>, etc. We can directly mark the corresponding fields as specific categories, as shown in the following figure.</p> <p>Note:</p> <ul> <li>In the labeling process, text content without key information about KIE shall be labeled as<code>other</code>, which is equivalent to background information. For example, in the ID card scenario, if we do not pay attention to <code>DOB</code> information, we can mark the categories of <code>DOB</code> and <code>Area manager</code> as <code>other</code>.</li> <li>In the annotation process of, it is required to annotate the textline position rather than the character.</li> </ul> <p>In terms of data, generally speaking, for relatively fixed scenes, 50 training images can achieve acceptable effects. You can refer to PPOCRLabel for finish the labeling process.</p> <p>In terms of model, it is recommended to use the VI-layoutXLM model proposed in PP-StructureV2. It is improved based on the LayoutXLM model, removing the visual feature extraction module, and further improving the model inference speed without the significant reduction on model accuracy. For more tutorials, please refer to VI-LayoutXLM introduction and KIE tutorial.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#222-ser-re","title":"2.2.2 SER + RE","text":"<p>The SER model is mainly used to identify all keys and values in the document image, and the RE model is mainly used to match all keys and values.</p> <p>Taking the ID card scenario as an example, the key information generally includes key information such as <code>name</code>, <code>DOB</code>, etc. in the SER stage, we need to identify all questions (keys) and answers (values). The demo annotation is as follows. All keys can be annotated as <code>question</code>, and all values can be annotated as <code>answer</code>.</p> <p></p> <p>In the RE stage, the ID and connection information of each field need to be marked, as shown in the following figure.</p> <p></p> <p>For each textline, you need to add 'ID' and 'linking' field information. The 'ID' records the unique identifier of the textline. Different text contents in the same images cannot be repeated. The 'linking' is a list that records the connection information between different texts. If the ID of the field \"name\" is 0 and the ID of the field \"Geoff Sample\" is 1, then they all have [[0, 1]] 'linking' marks, indicating that the fields with <code>id=0</code> and <code>id=1</code> form a key value relationship (the fields such as DOB and Expires are similar, and will not be repeated here).</p> <p>Note:</p> <p>During annotation, if value is multiple text lines, a key-value pair can be added in linking, such as <code>[[0, 1], [0, 2]]</code>.</p> <p>In terms of data, generally speaking, for relatively fixed scenes, about 50 training images can achieve acceptable effects.</p> <p>In terms of model, it is recommended to use the VI-layoutXLM model proposed in PP-StructureV2. It is improved based on the LayoutXLM model, removing the visual feature extraction module, and further improving the model inference speed without the significant reduction on model accuracy. For more tutorials, please refer to VI-LayoutXLM introduction and KIE tutorial.</p>"},{"location":"en/ppstructure/blog/how_to_do_kie.html#3-reference","title":"3. Reference","text":"<p>[1] Katti A R, Reisswig C, Guder C, et al. Chargrid: Towards understanding 2d documents[J]. arXiv preprint arXiv:1809.08799, 2018.</p> <p>[2] Xu Y, Li M, Cui L, et al. Layoutlm: Pre-training of text and layout for document image understanding[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 1192-1200.</p> <p>[3] Xu Y, Xu Y, Lv T, et al. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding[J]. arXiv preprint arXiv:2012.14740, 2020.</p> <p>[4]: Xu Y, Lv T, Cui L, et al. Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding[J]. arXiv preprint arXiv:2104.08836, 2021.</p> <p>[5] Li Y, Qian Y, Yu Y, et al. StrucTexT: Structured Text Understanding with Multi-Modal Transformers[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 1912-1920.</p> <p>[6] Liu X, Gao F, Zhang Q, et al. Graph convolution for multimodal information extraction from visually rich documents[J]. arXiv preprint arXiv:1903.11279, 2019.</p> <p>[7] Sun H, Kuang Z, Yue X, et al. Spatial Dual-Modality Graph Reasoning for Key Information Extraction[J]. arXiv preprint arXiv:2103.14470, 2021.</p> <p>[8] Zhang P, Xu Y, Cheng Z, et al. Trie: End-to-end text reading and information extraction for document understanding[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020: 1413-1422.</p>"},{"location":"en/ppstructure/blog/return_word_pos.html","title":"Return recognition position","text":"<p>According to the horizontal document, the recognition model not only returns the recognized content, but also the position of each word.</p>"},{"location":"en/ppstructure/blog/return_word_pos.html#english-document-recovery","title":"English document recovery","text":""},{"location":"en/ppstructure/blog/return_word_pos.html#download-the-inference-model-first","title":"Download the inference model first","text":"<pre><code>cd PaddleOCR/ppstructure\n\n## download model\nmkdir inference &amp;&amp; cd inference\n## Download the detection model of the ultra-lightweight English PP-OCRv3 model and unzip it\nhttps://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar\n## Download the recognition model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar &amp;&amp; tar xf en_PP-OCRv3_rec_infer.tar\n## Download the ultra-lightweight English table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\n## Download the layout model of publaynet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_infer.tar\ncd ..\n</code></pre>"},{"location":"en/ppstructure/blog/return_word_pos.html#then-use-the-following-command-inference-in-the-ppstructure-directory","title":"Then use the following command inference in the /ppstructure/ directory","text":"<pre><code>python predict_system.py \\\n--image_dir=./docs/table/1.png \\\n--det_model_dir=inference/en_PP-OCRv3_det_infer \\\n--rec_model_dir=inference/en_PP-OCRv3_rec_infer \\\n--rec_char_dict_path=../ppocr/utils/en_dict.txt \\\n--table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n--table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n--layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_infer \\\n--layout_dict_path=../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt \\\n--vis_font_path=../doc/fonts/simfang.ttf \\\n--recovery=True \\\n--output=../output/ \\\n--return_word_box=True\n</code></pre>"},{"location":"en/ppstructure/blog/return_word_pos.html#view-the-visualization-of-the-inference-results-under-outputstructure1show_0jpg-as-shown-below","title":"View the visualization of the inference results under <code>../output/structure/1/show_0.jpg</code>, as shown below","text":""},{"location":"en/ppstructure/blog/return_word_pos.html#recover-chinese-documents","title":"Recover Chinese documents","text":""},{"location":"en/ppstructure/blog/return_word_pos.html#download-the-inference-model-first_1","title":"Download the inference model first","text":"<pre><code>cd PaddleOCR/ppstructure\n\n## download model\ncd inference\n## Download the detection model of the ultra-lightweight Chinese PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_det_infer.tar\n## Download the recognition model of the ultra-lightweight Chinese PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_rec_infer.tar\n## Download the ultra-lightweight Chinese table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\n## Download the layout model of CDLA dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\ncd ..\n</code></pre>"},{"location":"en/ppstructure/blog/return_word_pos.html#upload-the-following-test-image-2png-to-the-directory-docstable","title":"Upload the following test image \"2.png\" to the directory ./docs/table/","text":""},{"location":"en/ppstructure/blog/return_word_pos.html#then-use-the-following-command-inference-in-the-ppstructure-directory_1","title":"Then use the following command inference in the /ppstructure/ directory","text":"<pre><code>python predict_system.py \\\n--image_dir=./docs/table/2.png \\\n--det_model_dir=inference/ch_PP-OCRv3_det_infer \\\n--rec_model_dir=inference/ch_PP-OCRv3_rec_infer \\\n--rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n--table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n--table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n--layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_cdla_infer \\\n--layout_dict_path=../ppocr/utils/dict/layout_dict/layout_cdla_dict.txt \\\n--vis_font_path=../doc/fonts/chinese_cht.ttf \\\n--recovery=True \\\n--output=../output/ \\\n--return_word_box=True\n</code></pre>"},{"location":"en/ppstructure/blog/return_word_pos.html#view-the-visualization-of-the-inference-results-under-outputstructure2show_0jpg-as-shown-below","title":"View the visualization of the inference results under <code>../output/structure/2/show_0.jpg</code>, as shown below","text":""},{"location":"en/ppstructure/infer_deploy/index.html","title":"PP-OCR Deployment","text":""},{"location":"en/ppstructure/infer_deploy/index.html#paddle-deployment-introduction","title":"Paddle Deployment Introduction","text":"<p>Paddle provides a variety of deployment schemes to meet the deployment requirements of different scenarios. Please choose according to the actual situation:</p> <p></p> <p>PP-OCR has supported muti deployment schemes. Click the link to get the specific tutorial.</p> <ul> <li>Python Inference</li> <li>C++ Inference</li> <li>Serving (Python/C++)</li> <li>Paddle-Lite (ARM CPU/OpenCL ARM GPU)</li> <li>Paddle.js</li> <li>Jetson Inference</li> <li>Paddle2ONNX</li> </ul> <p>If you need the deployment tutorial of academic algorithm models other than PP-OCR, please directly enter the main page of corresponding algorithms, entrance\u3002</p>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html","title":"Server-side C++ Inference","text":"<p>This chapter introduces the C++ deployment steps of the PaddleOCR model. C++ is better than Python in terms of performance. Therefore, in CPU and GPU deployment scenarios, C++ deployment is mostly used. This section will introduce how to configure the C++ environment and deploy PaddleOCR in Linux (CPU\\GPU) environment. For Windows deployment please refer to Windows compilation guidelines.</p>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#1-prepare-the-environment","title":"1. Prepare the Environment","text":""},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#11-environment","title":"1.1 Environment","text":"<ul> <li>Linux, docker is recommended.</li> <li>Windows.</li> </ul>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#12-compile-opencv","title":"1.2 Compile OpenCV","text":"<ul> <li>First of all, you need to download the source code compiled package in the Linux environment from the OpenCV official website. Taking OpenCV 3.4.7 as an example, the download command is as follows.</li> </ul> <pre><code>cd deploy/cpp_infer\nwget https://paddleocr.bj.bcebos.com/libs/opencv/opencv-3.4.7.tar.gz\ntar -xf opencv-3.4.7.tar.gz\n</code></pre> <p>Finally, you will see the folder of <code>opencv-3.4.7/</code> in the current directory.</p> <ul> <li>Compile OpenCV, the OpenCV source path (<code>root_path</code>) and installation path (<code>install_path</code>) should be set by yourself. Enter the OpenCV source code path and compile it in the following way.</li> </ul> <pre><code>root_path=your_opencv_root_path\ninstall_path=${root_path}/opencv3\n\nrm -rf build\nmkdir build\ncd build\n\ncmake .. \\\n    -DCMAKE_INSTALL_PREFIX=${install_path} \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DWITH_IPP=OFF \\\n    -DBUILD_IPP_IW=OFF \\\n    -DWITH_LAPACK=OFF \\\n    -DWITH_EIGEN=OFF \\\n    -DCMAKE_INSTALL_LIBDIR=lib64 \\\n    -DWITH_ZLIB=ON \\\n    -DBUILD_ZLIB=ON \\\n    -DWITH_JPEG=ON \\\n    -DBUILD_JPEG=ON \\\n    -DWITH_PNG=ON \\\n    -DBUILD_PNG=ON \\\n    -DWITH_TIFF=ON \\\n    -DBUILD_TIFF=ON\n\nmake -j\nmake install\n</code></pre> <p>In the above commands, <code>root_path</code> is the downloaded OpenCV source code path, and <code>install_path</code> is the installation path of OpenCV. After <code>make install</code> is completed, the OpenCV header file and library file will be generated in this folder for later OCR source code compilation.</p> <p>The final file structure under the OpenCV installation path is as follows.</p> <pre><code>opencv3/\n|-- bin\n|-- include\n|-- lib\n|-- lib64\n|-- share\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#13-compile-or-download-or-the-paddle-inference-library","title":"1.3 Compile or Download or the Paddle Inference Library","text":"<ul> <li>There are 2 ways to obtain the Paddle inference library, described in detail below.</li> </ul>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#131-direct-download-and-installation","title":"1.3.1 Direct download and installation","text":"<p>Paddle inference library official website. You can review and select the appropriate version of the inference library on the official website.</p> <ul> <li>After downloading, use the following command to extract files.</li> </ul> <pre><code>tar -xf paddle_inference.tgz\n</code></pre> <p>Finally you will see the folder of <code>paddle_inference/</code> in the current path.</p>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#132-compile-the-inference-source-code","title":"1.3.2 Compile the inference source code","text":"<ul> <li> <p>If you want to get the latest Paddle inference library features, you can download the latest code from Paddle GitHub repository and compile the inference library from the source code. It is recommended to download the inference library with paddle version greater than or equal to 2.0.1.</p> </li> <li> <p>You can refer to Paddle inference library to get the Paddle source code from GitHub, and then compile To generate the latest inference library. The method of using git to access the code is as follows.</p> </li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/Paddle.git\ngit checkout develop\n</code></pre> <ul> <li>Enter the Paddle directory and run the following commands to compile the paddle inference library.</li> </ul> <pre><code>rm -rf build\nmkdir build\ncd build\n\ncmake  .. \\\n    -DWITH_CONTRIB=OFF \\\n    -DWITH_MKL=ON \\\n    -DWITH_MKLDNN=ON  \\\n    -DWITH_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DWITH_INFERENCE_API_TEST=OFF \\\n    -DON_INFER=ON \\\n    -DWITH_PYTHON=ON\nmake -j\nmake inference_lib_dist\n</code></pre> <p>For more compilation parameter options, please refer to the document.</p> <ul> <li>After the compilation process, you can see the following files in the folder of <code>build/paddle_inference_install_dir/</code>.</li> </ul> <pre><code>build/paddle_inference_install_dir/\n|-- CMakeCache.txt\n|-- paddle\n|-- third_party\n|-- version.txt\n</code></pre> <p><code>paddle</code> is the Paddle library required for C++ prediction later, and <code>version.txt</code> contains the version information of the current inference library.</p>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#2-compile-and-run-the-demo","title":"2. Compile and Run the Demo","text":""},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#21-export-the-inference-model","title":"2.1 Export the inference model","text":"<ul> <li>You can refer to Model inference and export the inference model. After the model is exported, assuming it is placed in the <code>inference</code> directory, the directory structure is as follows.</li> </ul> <pre><code>inference/\n|-- det_db\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- rec_rcnn\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- cls\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- table\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- layout\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#22-compile-paddleocr-c-inference-demo","title":"2.2 Compile PaddleOCR C++ inference demo","text":"<ul> <li>The compilation commands are as follows. The addresses of Paddle C++ inference library, opencv and other Dependencies need to be replaced with the actual addresses on your own machines.</li> </ul> <pre><code>sh tools/build.sh\n</code></pre> <p>Specifically, you should modify the paths in <code>tools/build.sh</code>. The related content is as follows.</p> <pre><code>OPENCV_DIR=your_opencv_dir\nLIB_DIR=your_paddle_inference_dir\nCUDA_LIB_DIR=your_cuda_lib_dir\nCUDNN_LIB_DIR=your_cudnn_lib_dir\n</code></pre> <p><code>OPENCV_DIR</code> is the OpenCV installation path; <code>LIB_DIR</code> is the download (<code>paddle_inference</code> folder) or the generated Paddle inference library path (<code>build/paddle_inference_install_dir</code> folder); <code>CUDA_LIB_DIR</code> is the CUDA library file path, in docker; it is <code>/usr/local/cuda/lib64</code>; <code>CUDNN_LIB_DIR</code> is the cuDNN library file path, in docker it is <code>/usr/lib/x86_64-linux-gnu/</code>.</p> <ul> <li>After the compilation is completed, an executable file named <code>ppocr</code> will be generated in the <code>build</code> folder.</li> </ul>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#23-run-the-demo","title":"2.3 Run the demo","text":"<p>Execute the built executable file:</p> <pre><code>./build/ppocr [--param1] [--param2] [...]\n</code></pre> <p>Note:ppocr uses the <code>PP-OCRv3</code> model by default, and the input shape used by the recognition model is <code>3, 48, 320</code>, if you want to use the old version model, you should add the parameter <code>--rec_img_h=32</code>.</p> <p>Specifically,</p>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#1-detclsrec","title":"1. det+cls+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=true \\\n    --det=true \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#2-detrec","title":"2. det+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=false \\\n    --det=true \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#3-det","title":"3. det","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --det=true \\\n    --rec=false\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#4-clsrec","title":"4. cls+rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#5-rec","title":"5. rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=false \\\n    --det=false \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#6-cls","title":"6. cls","text":"<pre><code>./build/ppocr --cls_model_dir=inference/cls \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=false \\\n    --cls=true \\\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#7-layouttable","title":"7. layout+table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --layout_model_dir=inference/layout \\\n    --type=structure \\\n    --table=true \\\n    --layout=true\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#8-layout","title":"8. layout","text":"<pre><code>./build/ppocr --layout_model_dir=inference/layout \\\n    --image_dir=../../ppstructure/docs/table/1.png \\\n    --type=structure \\\n    --table=false \\\n    --layout=true \\\n    --det=false \\\n    --rec=false\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#9-table","title":"9. table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --type=structure \\\n    --table=true\n</code></pre> <p>More parameters are as follows,</p> <ul> <li>Common parameters</li> </ul> parameter data type default meaning use_gpu bool false Whether to use GPU gpu_id int 0 GPU id when use_gpu is true gpu_mem int 4000 GPU memory requested cpu_math_library_num_threads int 10 Number of threads when using CPU inference. When machine cores is enough, the large the value, the faster the inference speed enable_mkldnn bool true Whether to use mkdlnn library output str ./output Path where visualization results are saved <ul> <li>forward</li> </ul> parameter data type default meaning det bool true Whether to perform text detection in the forward direction rec bool true Whether to perform text recognition in the forward direction cls bool false Whether to perform text direction classification in the forward direction <ul> <li>Detection related parameters</li> </ul> parameter data type default meaning det_model_dir string - Address of detection inference model max_side_len int 960 Limit the maximum image height and width to 960 det_db_thresh float 0.3 Used to filter the binarized image of DB prediction, setting 0.-0.3 has no obvious effect on the result det_db_box_thresh float 0.5 DB post-processing filter box threshold, if there is a missing box detected, it can be reduced as appropriate det_db_unclip_ratio float 1.6 Indicates the compactness of the text box, the smaller the value, the closer the text box to the text det_db_score_mode string slow slow: use polygon box to calculate bbox score, fast: use rectangle box to calculate. Use rectangular box to calculate faster, and polygonal box more accurate for curved text area. visualize bool true Whether to visualize the results\uff0cwhen it is set as true, the prediction results will be saved in the folder specified by the <code>output</code> field on an image with the same name as the input image. <ul> <li>Classifier related parameters</li> </ul> parameter data type default meaning use_angle_cls bool false Whether to use the direction classifier cls_model_dir string - Address of direction classifier inference model cls_thresh float 0.9 Score threshold of the  direction classifier cls_batch_num int 1 batch size of classifier <ul> <li>Recognition related parameters</li> </ul> parameter data type default meaning rec_model_dir string - Address of recognition inference model rec_char_dict_path string ../../ppocr/utils/ppocr_keys_v1.txt dictionary file rec_batch_num int 6 batch size of recognition rec_img_h int 48 image height of recognition rec_img_w int 320 image width of recognition <ul> <li>Layout related parameters</li> </ul> parameter data type default meaning layout_model_dir string - Address of layout inference model layout_dict_path string ../../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt dictionary file layout_score_threshold float 0.5 Threshold of score. layout_nms_threshold float 0.5 Threshold of nms. <ul> <li>Table recognition related parameters</li> </ul> parameter data type default meaning table_model_dir string - Address of table recognition inference model table_char_dict_path string ../../ppocr/utils/dict/table_structure_dict.txt dictionary file table_max_len int 488 The size of the long side of the input image of the table recognition model, the final input image size of the network is\uff08table_max_len\uff0ctable_max_len\uff09 merge_no_span_structure bool true Whether to merge  and  to &lt;/td <ul> <li>Multi-language inference is also supported in PaddleOCR, you can refer to recognition tutorial for more supported languages and models in PaddleOCR. Specifically, if you want to infer using multi-language models, you just need to modify values of <code>rec_char_dict_path</code> and <code>rec_model_dir</code>.</li> </ul> <p>The detection results will be shown on the screen, which is as follows.</p> <pre><code>predict img: ../../doc/imgs/12.jpg\n../../doc/imgs/12.jpg\n0       det boxes: [[74,553],[427,542],[428,571],[75,582]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b252935\u200b\u53f7\u200b rec score: 0.947724\n1       det boxes: [[23,507],[513,488],[515,529],[24,548]] rec text: \u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b rec score: 0.993728\n2       det boxes: [[187,456],[399,448],[400,480],[188,488]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b15\u200b\u53f7\u200b rec score: 0.964994\n3       det boxes: [[42,413],[483,391],[484,428],[43,450]] rec text: \u200b\u4e0a\u6d77\u200b\u65af\u683c\u5a01\u200b\u94c2\u200b\u5c14\u200b\u5927\u9152\u5e97\u200b rec score: 0.980086\nThe detection visualized image saved in ./output//12.jpg\n</code></pre> <ul> <li>layout+table</li> </ul> <pre><code>predict img: ../../ppstructure/docs/table/1.png\n0       type: text, region: [12,729,410,848], score: 0.781044, res: count of ocr result is : 7\n********** print ocr result **********\n0       det boxes: [[4,1],[79,1],[79,12],[4,12]] rec text: CTW1500. rec score: 0.769472\n...\n6       det boxes: [[4,99],[391,99],[391,112],[4,112]] rec text: sate-of-the-artmethods[12.34.36l.ourapproachachieves rec score: 0.90414\n********** end print ocr result **********\n1       type: text, region: [69,342,342,359], score: 0.703666, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[269,2],[269,13],[8,13]] rec text: Table6.Experimentalresults on CTW-1500 rec score: 0.890454\n********** end print ocr result **********\n2       type: text, region: [70,316,706,332], score: 0.659738, res: count of ocr result is : 2\n********** print ocr result **********\n0       det boxes: [[373,2],[630,2],[630,11],[373,11]] rec text: oroposals.andthegreencontoursarefinal rec score: 0.919729\n1       det boxes: [[8,3],[357,3],[357,11],[8,11]] rec text: Visualexperimentalresultshebluecontoursareboundar rec score: 0.915963\n********** end print ocr result **********\n3       type: text, region: [489,342,789,359], score: 0.630538, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[294,2],[294,14],[8,14]] rec text: Table7.Experimentalresults onMSRA-TD500 rec score: 0.942251\n********** end print ocr result **********\n4       type: text, region: [444,751,841,848], score: 0.607345, res: count of ocr result is : 5\n********** print ocr result **********\n0       det boxes: [[19,3],[389,3],[389,17],[19,17]] rec text: Inthispaper,weproposeanovel adaptivebound rec score: 0.941031\n1       det boxes: [[4,22],[390,22],[390,36],[4,36]] rec text: aryproposalnetworkforarbitraryshapetextdetection rec score: 0.960172\n2       det boxes: [[4,42],[392,42],[392,56],[4,56]] rec text: whichadoptanboundaryproposalmodeltogeneratecoarse rec score: 0.934647\n3       det boxes: [[4,61],[389,61],[389,75],[4,75]] rec text: ooundaryproposals,andthenadoptanadaptiveboundary rec score: 0.946296\n4       det boxes: [[5,80],[387,80],[387,93],[5,93]] rec text: leformationmodelcombinedwithGCNandRNNtoper rec score: 0.952401\n********** end print ocr result **********\n5       type: title, region: [444,705,564,724], score: 0.785429, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[6,2],[113,2],[113,14],[6,14]] rec text: 5.Conclusion rec score: 0.856903\n********** end print ocr result **********\n6       type: table, region: [14,360,402,711], score: 0.963643, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;Ext&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;85.3&lt;/td&gt;&lt;td&gt;67.9&lt;/td&gt;&lt;td&gt;75.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CSE [17]&lt;/td&gt;&lt;td&gt;MiLT&lt;/td&gt;&lt;td&gt;76.1&lt;/td&gt;&lt;td&gt;78.7&lt;/td&gt;&lt;td&gt;77.4&lt;/td&gt;&lt;td&gt;0.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LOMO[40]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;76.5&lt;/td&gt;&lt;td&gt;85.7&lt;/td&gt;&lt;td&gt;80.8&lt;/td&gt;&lt;td&gt;4.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;Sy-&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SegLink++ [28]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;81.4&lt;/td&gt;&lt;td&gt;6.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.0&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;81.5&lt;/td&gt;&lt;td&gt;4.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PSENet-1s [33]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;79.7&lt;/td&gt;&lt;td&gt;84.8&lt;/td&gt;&lt;td&gt;82.2&lt;/td&gt;&lt;td&gt;3.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB [12]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;86.9&lt;/td&gt;&lt;td&gt;83.4&lt;/td&gt;&lt;td&gt;22.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.1&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;83.5&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextDragon [5]&lt;/td&gt;&lt;td&gt;MLT+&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;84.5&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.2&lt;/td&gt;&lt;td&gt;86.4&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;39.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ContourNet [36]&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;83.9&lt;/td&gt;&lt;td&gt;4.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.02&lt;/td&gt;&lt;td&gt;85.93&lt;/td&gt;&lt;td&gt;84.45&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextPerception[23]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.9&lt;/td&gt;&lt;td&gt;87.5&lt;/td&gt;&lt;td&gt;84.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt; Syn&lt;/td&gt;&lt;td&gt;80.57&lt;/td&gt;&lt;td&gt;87.66&lt;/td&gt;&lt;td&gt;83.97&lt;/td&gt;&lt;td&gt;12.08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;81.45&lt;/td&gt;&lt;td&gt;87.81&lt;/td&gt;&lt;td&gt;84.51&lt;/td&gt;&lt;td&gt;12.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.60&lt;/td&gt;&lt;td&gt;86.45&lt;/td&gt;&lt;td&gt;85.00&lt;/td&gt;&lt;td&gt;12.21&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//6_1.png\n7       type: table, region: [462,359,820,657], score: 0.953917, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;SegLink [26]&lt;/td&gt;&lt;td&gt;70.0&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;77.0&lt;/td&gt;&lt;td&gt;8.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PixelLink [4]&lt;/td&gt;&lt;td&gt;73.2&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;77.8&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;73.9&lt;/td&gt;&lt;td&gt;83.2&lt;/td&gt;&lt;td&gt;78.3&lt;/td&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;75.9&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;5.2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;76.7&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FTSN[3]&lt;/td&gt;&lt;td&gt;77.1&lt;/td&gt;&lt;td&gt;87.6&lt;/td&gt;&lt;td&gt;82.0&lt;/td&gt;&lt;td&gt;:&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LSE[30]&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;84.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;78.2&lt;/td&gt;&lt;td&gt;88.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;8.6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MCN [16]&lt;/td&gt;&lt;td&gt;79&lt;/td&gt;&lt;td&gt;88&lt;/td&gt;&lt;td&gt;83&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;82.1&lt;/td&gt;&lt;td&gt;85.2&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;83.8&lt;/td&gt;&lt;td&gt;84.4&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;30.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB[12]&lt;/td&gt;&lt;td&gt;79.2&lt;/td&gt;&lt;td&gt;91.5&lt;/td&gt;&lt;td&gt;84.9&lt;/td&gt;&lt;td&gt;32.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;82.30&lt;/td&gt;&lt;td&gt;88.05&lt;/td&gt;&lt;td&gt;85.08&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (SynText)&lt;/td&gt;&lt;td&gt;80.68&lt;/td&gt;&lt;td&gt;85.40&lt;/td&gt;&lt;td&gt;82.97&lt;/td&gt;&lt;td&gt;12.68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (MLT-17)&lt;/td&gt;&lt;td&gt;84.54&lt;/td&gt;&lt;td&gt;86.62&lt;/td&gt;&lt;td&gt;85.57&lt;/td&gt;&lt;td&gt;12.31&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//7_1.png\n8       type: figure, region: [14,3,836,310], score: 0.969443, res: count of ocr result is : 26\n********** print ocr result **********\n0       det boxes: [[506,14],[539,15],[539,22],[506,21]] rec text: E rec score: 0.318073\n...\n25      det boxes: [[680,290],[759,288],[759,303],[680,305]] rec text: (d) CTW1500 rec score: 0.95911\n********** end print ocr result **********\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/cpp_infer.html#3-faq","title":"3. FAQ","text":"<ol> <li>Encountered the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.</code>, change the github address in <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to the https://gitee.com/Double_V/AutoLog address.</li> </ol>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html","title":"Sever Deployment","text":"<p>PaddleOCR provides 2 service deployment methods:</p> <ul> <li>Based on PaddleHub Serving: Code path is <code>./deploy/hubserving</code>. Please follow this tutorial.</li> <li>Based on PaddleServing: Code path is <code>./deploy/pdserving</code>. Please refer to the tutorial for usage.</li> </ul>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#service-deployment-based-on-paddlehub-serving","title":"Service deployment based on PaddleHub Serving","text":"<p>The hubserving service deployment directory includes seven service packages: text detection, text angle class, text recognition, text detection+text angle class+text recognition three-stage series connection, layout analysis, table recognition, and PP-Structure. Please select the corresponding service package to install and start the service according to your needs. The directory is as follows:</p> <pre><code>deploy/hubserving/\n  \u2514\u2500  ocr_det     text detection module service package\n  \u2514\u2500  ocr_cls     text angle class module service package\n  \u2514\u2500  ocr_rec     text recognition module service package\n  \u2514\u2500  ocr_system  text detection+text angle class+text recognition three-stage series connection service package\n  \u2514\u2500  structure_layout  layout analysis service package\n  \u2514\u2500  structure_table  table recognition service package\n  \u2514\u2500  structure_system  PP-Structure service package\n  \u2514\u2500  kie_ser  KIE(SER) service package\n  \u2514\u2500  kie_ser_re  KIE(SER+RE) service package\n</code></pre> <p>Each service pack contains 3 files. Take the 2-stage series connection service package as an example, the directory is as follows:</p> <pre><code>deploy/hubserving/ocr_system/\n  \u2514\u2500  __init__.py    Empty file, required\n  \u2514\u2500  config.json    Configuration file, optional, passed in as a parameter when using configuration to start the service\n  \u2514\u2500  module.py      Main module file, required, contains the complete logic of the service\n  \u2514\u2500  params.py      Parameter file, required, including parameters such as model path, pre and post-processing parameters\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#1-update","title":"1. Update","text":"<ul> <li>2022.10.09 add KIE services.</li> <li>2022.08.23 add layout analysis services.</li> <li>2022.03.30 add PP-Structure and table recognition services.</li> <li>2022.05.05 add PP-OCRv3 text detection and recognition services.</li> </ul>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#2-quick-start-service","title":"2. Quick start service","text":"<p>The following steps take the 2-stage series service as an example. If only the detection service or recognition service is needed, replace the corresponding file path.</p>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#21-install-paddlehub","title":"2.1 Install PaddleHub","text":"<pre><code>pip3 install paddlehub==2.1.0 --upgrade\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#22-download-inference-model","title":"2.2 Download inference model","text":"<p>Before installing the service module, you need to prepare the inference model and put it in the correct path. By default, the PP-OCRv3 models are used, and the default model path is:</p> Model Path text detection model ./inference/ch_PP-OCRv3_det_infer/ text recognition model ./inference/ch_PP-OCRv3_rec_infer/ text angle classifier ./inference/ch_ppocr_mobile_v2.0_cls_infer/ layout parse model ./inference/picodet_lcnet_x1_0_fgd_layout_infer/ tanle recognition ./inference/ch_ppstructure_mobile_v2.0_SLANet_infer/ KIE(SER) ./inference/ser_vi_layoutxlm_xfund_infer/ KIE(SER+RE) ./inference/re_vi_layoutxlm_xfund_infer/ <p>The model path can be found and modified in <code>params.py</code>. More models provided by PaddleOCR can be obtained from the model library. You can also use models trained by yourself.</p>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#23-install-service-module","title":"2.3 Install Service Module","text":"<p>PaddleOCR provides 5 kinds of service modules, install the required modules according to your needs.</p> <ul> <li>On the Linux platform(replace <code>/</code> with <code>\\</code> if using Windows), the examples are as the following table:</li> </ul> <p>| Service model | Command | | text detection | <code>hub install deploy/hubserving/ocr_det</code> | | text angle class: | <code>hub install deploy/hubserving/ocr_cls</code> | | text recognition: | <code>hub install deploy/hubserving/ocr_rec</code> | | 2-stage series: | <code>hub install deploy/hubserving/ocr_system</code> | | table recognition | <code>hub install deploy/hubserving/structure_table</code> | | PP-Structure | <code>hub install deploy/hubserving/structure_system</code> | | KIE(SER) | <code>hub install deploy/hubserving/kie_ser</code> | | KIE(SER+RE) | <code>hub install deploy/hubserving/kie_ser_re</code> |</p>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#24-start-service","title":"2.4 Start service","text":""},{"location":"en/ppstructure/infer_deploy/paddle_server.html#241-start-with-command-line-parameters-cpu-only","title":"2.4.1 Start with command line parameters (CPU only)","text":"<p>start command:</p> <pre><code>hub serving start --modules Module1==Version1, Module2==Version2, ... \\\n                  --port 8866 \\\n                  --use_multiprocess \\\n                  --workers \\\n</code></pre> <p>Parameters: |parameters|usage| |---|---| |<code>--modules</code>/<code>-m</code>|PaddleHub Serving pre-installed model, listed in the form of multiple Module==Version key-value pairsWhen Version is not specified, the latest version is selected by default| |<code>--port</code>/<code>-p</code>|Service port, default is 8866| |<code>--use_multiprocess</code>|Enable concurrent mode, by default using the single-process mode, this mode is recommended for multi-core CPU machinesWindows operating system only supports single-process mode| |<code>--workers</code>|The number of concurrent tasks specified in concurrent mode, the default is <code>2*cpu_count-1</code>, where <code>cpu_count</code> is the number of CPU cores|</p> <p>For example, start the 2-stage series service:</p> <pre><code>hub serving start -m ocr_system\n</code></pre> <p>This completes the deployment of a service API, using the default port number 8866.</p>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#242-start-with-configuration-filecpu-and-gpu","title":"2.4.2 Start with configuration file\uff08CPU and GPU\uff09","text":"<p>start command:</p> <pre><code>hub serving start --config/-c config.json\n</code></pre> <p>In which the format of <code>config.json</code> is as follows:</p> <pre><code>{\n    \"modules_info\": {\n        \"ocr_system\": {\n            \"init_args\": {\n                \"version\": \"1.0.0\",\n                \"use_gpu\": true\n            },\n            \"predict_args\": {\n            }\n        }\n    },\n    \"port\": 8868,\n    \"use_multiprocess\": false,\n    \"workers\": 2\n}\n</code></pre> <ul> <li>The configurable parameters in <code>init_args</code> are consistent with the <code>_initialize</code> function interface in <code>module.py</code>.</li> </ul> <p>When <code>use_gpu</code> is <code>true</code>, it means that the GPU is used to start the service. - The configurable parameters in <code>predict_args</code> are consistent with the <code>predict</code> function interface in <code>module.py</code>.</p> <p>Note:   - When using the configuration file to start the service, other parameters will be ignored.   - If you use GPU prediction (that is, <code>use_gpu</code> is set to <code>true</code>), you need to set the environment variable CUDA_VISIBLE_DEVICES before starting the service, such as:</p> <pre><code>```bash linenums=\"1\"\nexport CUDA_VISIBLE_DEVICES=0\n```\n</code></pre> <ul> <li><code>use_gpu</code> and <code>use_multiprocess</code> cannot be <code>true</code> at the same time.</li> </ul> <p>For example, use GPU card No. 3 to start the 2-stage series service:</p> <pre><code>export CUDA_VISIBLE_DEVICES=3\nhub serving start -c deploy/hubserving/ocr_system/config.json\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#3-send-prediction-requests","title":"3. Send prediction requests","text":"<p>After the service starts, you can use the following command to send a prediction request to obtain the prediction result:</p> <pre><code>python tools/test_hubserving.py --server_url=server_url --image_dir=image_path\n</code></pre> <p>Two parameters need to be passed to the script:</p> <ul> <li>server_url:service address, the format of which is   <code>http://[ip_address]:[port]/predict/[module_name]</code></li> </ul> <p>For example, if using the configuration file to start the text angle classification, text detection, text recognition, detection+classification+recognition 3 stages, table recognition and PP-Structure service,</p> <p>also modified the port for each service, then the <code>server_url</code> to send the request will be:</p> <pre><code>http://127.0.0.1:8865/predict/ocr_det\nhttp://127.0.0.1:8866/predict/ocr_cls\nhttp://127.0.0.1:8867/predict/ocr_rec\nhttp://127.0.0.1:8868/predict/ocr_system\nhttp://127.0.0.1:8869/predict/structure_table\nhttp://127.0.0.1:8870/predict/structure_system\nhttp://127.0.0.1:8870/predict/structure_layout\nhttp://127.0.0.1:8871/predict/kie_ser\nhttp://127.0.0.1:8872/predict/kie_ser_re\n</code></pre> <ul> <li>image_dir:Test image path, which can be a single image path or an image directory path</li> <li>visualize:Whether to visualize the results, the default value is False</li> <li>output:The folder to save the Visualization result, the default value is <code>./hubserving_result</code></li> </ul> <p>Example:</p> <pre><code>python tools/test_hubserving.py --server_url=http://127.0.0.1:8868/predict/ocr_system --image_dir=./doc/imgs/ --visualize=false`\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#4-returned-result-format","title":"4. Returned result format","text":"<p>The returned result is a list. Each item in the list is a dictionary which may contain three fields. The information is as follows:</p> field name data type description angle str angle text str text content confidence float text recognition confidence text_region list text location coordinates html str table HTML string regions list The result of layout analysis + table recognition + OCR, each item is a listincluding <code>bbox</code> indicating area coordinates, <code>type</code> of area type and <code>res</code> of area results layout list The result of layout analysis, each item is a dict, including <code>bbox</code> indicating area coordinates, <code>label</code> of area type <p>The fields returned by different modules are different. For example, the results returned by the text recognition service module do not contain <code>text_region</code>, detailed table is as follows:</p> field name/module name ocr_det ocr_cls ocr_rec ocr_system structure_table structure_system structure_layout kie_ser kie_re angle \u2714 \u2714 text \u2714 \u2714 \u2714 \u2714 \u2714 confidence \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 text_region \u2714 \u2714 \u2714 \u2714 \u2714 html \u2714 \u2714 regions \u2714 \u2714 layout \u2714 ser_res \u2714 re_res \u2714 <p>Note: If you need to add, delete or modify the returned fields, you can modify the file <code>module.py</code> of the corresponding module. For the complete process, refer to the user-defined modification service module in the next section.</p>"},{"location":"en/ppstructure/infer_deploy/paddle_server.html#5-user-defined-service-module-modification","title":"5. User-defined service module modification","text":"<p>If you need to modify the service logic, the following steps are generally required (take the modification of <code>deploy/hubserving/ocr_system</code> for example):</p> <ol> <li>Stop service:</li> </ol> <pre><code>hub serving stop --port/-p XXXX\n</code></pre> <ol> <li>Modify the code in the corresponding files under <code>deploy/hubserving/ocr_system</code>, such as <code>module.py</code> and <code>params.py</code>, to your actual needs.</li> </ol> <p>For example, if you need to replace the model used by the deployed service, you need to modify model path parameters <code>det_model_dir</code> and <code>rec_model_dir</code> in <code>params.py</code>. If you want to turn off the text direction classifier, set the parameter <code>use_angle_cls</code> to <code>False</code>.</p> <p>Of course, other related parameters may need to be modified at the same time. Please modify and debug according to the actual situation.</p> <p>It is suggested to run <code>module.py</code> directly for debugging after modification before starting the service test.</p> <p>Note The image input shape used by the PPOCR-v3 recognition model is <code>3, 48, 320</code>, so you need to modify <code>cfg.rec_image_shape = \"3, 48, 320\"</code> in <code>params.py</code>, if you do not use the PPOCR-v3 recognition model, then there is no need to modify this parameter. 3. (Optional) If you want to rename the module, the following lines should be modified:    - <code>ocr_system</code> within <code>from deploy.hubserving.ocr_system.params import read_params</code>    - <code>ocr_system</code> within <code>name=\"ocr_system\",</code> 4. (Optional) It may require you to delete the directory <code>__pycache__</code> to force flush build cache of CPython:</p> <pre><code>find deploy/hubserving/ocr_system -name '__pycache__' -exec rm -r {} \\;\n</code></pre> <ol> <li>Install modified service module:</li> </ol> <pre><code>hub install deploy/hubserving/ocr_system/\n</code></pre> <ol> <li>Restart service:</li> </ol> <pre><code>hub serving start -m ocr_system\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/python_infer.html","title":"Python Inference","text":""},{"location":"en/ppstructure/infer_deploy/python_infer.html#1-layout-structured-analysis","title":"1. Layout Structured Analysis","text":"<p>Go to the <code>ppstructure</code> directory</p> <pre><code>cd ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the PP-StructureV2 layout analysis model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_layout_infer.tar &amp;&amp; tar xf picodet_lcnet_x1_0_layout_infer.tar\n# Download the PP-OCRv3 text detection model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_det_infer.tar\n# Download the PP-OCRv3 text recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_rec_infer.tar\n# Download the PP-StructureV2 form recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/ch_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n</code></pre>"},{"location":"en/ppstructure/infer_deploy/python_infer.html#11-layout-analysis-table-recognition","title":"1.1 layout analysis + table recognition","text":"<pre><code>python3 predict_system.py --det_model_dir=inference/ch_PP-OCRv3_det_infer \\\n                          --rec_model_dir=inference/ch_PP-OCRv3_rec_infer \\\n                          --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n                          --layout_model_dir=inference/picodet_lcnet_x1_0_layout_infer \\\n                          --image_dir=./docs/table/1.png \\\n                          --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n                          --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n                          --output=../output \\\n                          --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel, and the picture area will be cropped and saved. The filename of excel and picture is their coordinates in the image. Detailed results are stored in the <code>res.txt</code> file.</p>"},{"location":"en/ppstructure/infer_deploy/python_infer.html#12-layout-analysis","title":"1.2 layout analysis","text":"<pre><code>python3 predict_system.py --layout_model_dir=inference/picodet_lcnet_x1_0_layout_infer \\\n                          --image_dir=./docs/table/1.png \\\n                          --output=../output \\\n                          --table=false \\\n                          --ocr=false\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each picture in image will be cropped and saved. The filename of picture area is their coordinates in the image. Layout analysis results will be stored in the <code>res.txt</code> file</p>"},{"location":"en/ppstructure/infer_deploy/python_infer.html#13-table-recognition","title":"1.3 table recognition","text":"<pre><code>python3 predict_system.py --det_model_dir=inference/ch_PP-OCRv3_det_infer \\\n                          --rec_model_dir=inference/ch_PP-OCRv3_rec_infer \\\n                          --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n                          --image_dir=./docs/table/table.jpg \\\n                          --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n                          --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n                          --output=../output \\\n                          --vis_font_path=../doc/fonts/simfang.ttf \\\n                          --layout=false\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel. The filename of excel is their coordinates in the image.</p>"},{"location":"en/ppstructure/infer_deploy/python_infer.html#2-key-information-extraction","title":"2. Key Information Extraction","text":""},{"location":"en/ppstructure/infer_deploy/python_infer.html#21-ser","title":"2.1 SER","text":"<pre><code>cd ppstructure\n\nmkdir inference &amp;&amp; cd inference\n# download model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\ncd ..\npython3 predict_system.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=./inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../ppocr/utils/dict/kie_dict/xfund_class_list.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\" \\\n  --mode=kie\n</code></pre> <p>After the operation is completed, each image will store the visualized image in the <code>kie</code> directory under the directory specified by the <code>output</code> field, and the image name is the same as the input image name.</p>"},{"location":"en/ppstructure/infer_deploy/python_infer.html#22-reser","title":"2.2 RE+SER","text":"<pre><code>cd ppstructure\n\nmkdir inference &amp;&amp; cd inference\n# download model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_infer.tar\ncd ..\n\npython3 predict_system.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=./inference/re_vi_layoutxlm_xfund_infer \\\n  --ser_model_dir=./inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../ppocr/utils/dict/kie_dict/xfund_class_list.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\" \\\n  --mode=kie\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>kie</code> directory under the directory specified by the <code>output</code> field, where the visual images and prediction results are stored.</p>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html","title":"Layout Recovery","text":""},{"location":"en/ppstructure/model_train/recovery_to_doc.html#1-introduction","title":"1. Introduction","text":"<p>The layout recovery module is used to restore the image or pdf to an editable Word file consistent with the original image layout.</p> <p>Two layout recovery methods are provided, you can choose by PDF format:</p> <ul> <li> <p>Standard PDF parse(the input is standard PDF): Python based PDF to word library pdf2docx is optimized, the method extracts data from PDF with PyMuPDF, then parse layout with rule, finally, generate docx with python-docx.</p> </li> <li> <p>Image format PDF parse(the input can be standard PDF or image format PDF): Layout recovery combines layout analysis\u3001table recognition to better recover images, tables, titles, etc. supports input files in PDF and document image formats in Chinese and English.</p> </li> </ul> <p>The input formats and application scenarios of the two methods are as follows:</p> method input formats application scenarios/problem Standard PDF parse pdf Advantages: Better recovery for non-paper documents, each page remains on the same page after restorationDisadvantages: English characters in some Chinese documents are garbled, some contents are still beyond the current page, the whole page content is restored to the table format, and the recovery effect of some pictures is not good Image format PDF parse( pdf\u3001picture Advantages: More suitable for paper document content recovery,  OCR recognition effect is more goodDisadvantages: Currently, the recovery is based on rules, the effect of content typesetting (spacing, fonts, etc.) need to be further improved, and the effect of layout recovery depends on layout analysis <p>The following figure shows the effect of restoring the layout of documents by using PDF parse:</p> <p></p> <p>The following figures show the effect of restoring the layout of English and Chinese documents by using OCR technique:</p> <p></p> <p></p>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#2-install","title":"2. Install","text":""},{"location":"en/ppstructure/model_train/recovery_to_doc.html#21-install-paddlepaddle","title":"2.1 Install PaddlePaddle","text":"<pre><code>python3 -m pip install --upgrade pip\n\n# If you have cuda9 or cuda10 installed on your machine, please run the following command to install\npython3 -m pip install \"paddlepaddle-gpu\" -i https://mirror.baidu.com/pypi/simple\n\n# CPU installation\npython3 -m pip install \"paddlepaddle\" -i https://mirror.baidu.com/pypi/simple\n````\n\nFor more requirements, please refer to the instructions in [Installation Documentation](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/macos-pip_en.html).\n\n### 2.2 Install PaddleOCR\n\n- **(1) Download source code**\n\n```bash linenums=\"1\"\n[Recommended] git clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If the pull cannot be successful due to network problems, you can also choose to use the hosting on the code cloud:\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: Code cloud hosting code may not be able to synchronize the update of this github project in real time, there is a delay of 3 to 5 days, please use the recommended method first.\n````\n\n- **(2) Install recovery `requirements`**\n\nThe layout restoration is exported as docx files, so python-docx API need to be installed, and PyMuPDF api([requires Python &gt;= 3.7](https://pypi.org/project/PyMuPDF/)) need to be installed to process the input files in pdf format.\n\nInstall all the libraries by running the following command:\n\n```bash linenums=\"1\"\npython3 -m pip install -r ppstructure/recovery/requirements.txt\n````\n\n And if using pdf parse method, we need to install pdf2docx api.\n\n```bash linenums=\"1\"\nwget https://paddleocr.bj.bcebos.com/whl/pdf2docx-0.0.0-py3-none-any.whl\npip3 install pdf2docx-0.0.0-py3-none-any.whl\n</code></pre>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#3-quick-start-using-standard-pdf-parse","title":"3. Quick Start using standard PDF parse","text":"<p><code>use_pdf2docx_api</code> use PDF parse for layout recovery, The whl package is also provided  for quick use, follow the above code, for more infomation please refer to quickstart for details.</p> <pre><code># install paddleocr\npip3 install \"paddleocr&gt;=2.6\"\npaddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --use_pdf2docx_api=true\n</code></pre> <p>Command line:</p> <pre><code>python3 predict_system.py \\\n    --image_dir=ppstructure/docs/recovery/UnrealText.pdf \\\n    --recovery=True \\\n    --use_pdf2docx_api=True \\\n    --output=../output/\n</code></pre>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#4-quick-start-using-image-format-pdf-parse","title":"4. Quick Start using image format PDF parse","text":"<p>Through layout analysis, we divided the image/PDF documents into regions, located the key regions, such as text, table, picture, etc., and recorded the location, category, and regional pixel value information of each region. Different regions are processed separately, where:</p> <ul> <li> <p>OCR detection and recognition is performed in the text area, and the coordinates of the OCR detection box and the text content information are added on the basis of the previous information</p> </li> <li> <p>The table area identifies tables and records html and text information of tables</p> </li> <li>Save the image directly</li> </ul> <p>We can restore the test picture through the layout information, OCR detection and recognition structure, table information, and saved pictures.</p> <p>The whl package is also provided for quick use, follow the above code, for more infomation please refer to quickstart for details.</p> <pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --recovery=true --lang='en'\n</code></pre>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#41-download-models","title":"4.1 Download models","text":"<p>If input is English document, download English models:</p> <pre><code>cd PaddleOCR/ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the detection model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar\n# Download the recognition model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar &amp;&amp; tar xf en_PP-OCRv3_rec_infer.tar\n# Download the ultra-lightweight English table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\n# Download the layout model of publaynet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_infer.tar\ncd ..\n</code></pre> <p>If input is Chinese document\uff0cdownload Chinese models: Chinese and English ultra-lightweight PP-OCRv3 model</p>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#42-layout-recovery","title":"4.2 Layout recovery","text":"<pre><code>python3 predict_system.py \\\n    --image_dir=./docs/table/1.png \\\n    --det_model_dir=inference/en_PP-OCRv3_det_infer \\\n    --rec_model_dir=inference/en_PP-OCRv3_rec_infer \\\n    --rec_char_dict_path=../ppocr/utils/en_dict.txt \\\n    --table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_infer \\\n    --layout_dict_path=../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt \\\n    --vis_font_path=../doc/fonts/simfang.ttf \\\n    --recovery=True \\\n    --output=../output/\n</code></pre> <p>After running, the docx of each picture will be saved in the directory specified by the output field</p> <p>Field\uff1a</p> <ul> <li>image_dir\uff1atest file\uff0c can be picture, picture directory, pdf file, pdf file directory</li> <li>det_model_dir\uff1aOCR detection model path</li> <li>rec_model_dir\uff1aOCR recognition model path</li> <li>rec_char_dict_path\uff1aOCR recognition dict path. If the Chinese model is used, change to \"../ppocr/utils/ppocr_keys_v1.txt\". And if you trained the model on your own dataset, change to the trained dictionary</li> <li>table_model_dir\uff1atabel recognition model path</li> <li>table_char_dict_path\uff1atabel recognition dict path. If the Chinese model is used, no need to change</li> <li>layout_model_dir\uff1alayout analysis model path</li> <li>layout_dict_path\uff1alayout analysis dict path. If the Chinese model is used, change to \"../ppocr/utils/dict/layout_dict/layout_cdla_dict.txt\"</li> <li>recovery\uff1awhether to enable layout of recovery, default False</li> <li>output\uff1asave the recovery result path</li> </ul>"},{"location":"en/ppstructure/model_train/recovery_to_doc.html#5-more","title":"5. More","text":"<p>For training, evaluation and inference tutorial for text detection models, please refer to text detection doc.</p> <p>For training, evaluation and inference tutorial for text recognition models, please refer to text recognition doc.</p> <p>For training, evaluation and inference tutorial for layout analysis models, please refer to layout analysis doc</p> <p>For training, evaluation and inference tutorial for table recognition models, please refer to table recognition doc</p>"},{"location":"en/ppstructure/model_train/train_kie.html","title":"Key Information Extraction (KIE)","text":""},{"location":"en/ppstructure/model_train/train_kie.html#1-introduction","title":"1. Introduction","text":"<p>Key information extraction (KIE) refers to extracting key information from text or images. As downstream task of OCR, the key information extraction task of document image has many practical application scenarios, such as form recognition, ticket information extraction, ID card information extraction, etc.</p> <p>PP-Structure conducts research based on the LayoutXLM multi-modal, and proposes the VI-LayoutXLM, which gets rid of visual features when finetuning the downstream tasks. An textline sorting method is also utilized to fit in reading order. What's more, UDML knowledge distillation is used for higher accuracy. Finally, the accuracy and inference speed of VI-LayoutXLM surpass those of LayoutXLM.</p> <p>The main features of the key information extraction module in PP-Structure are as follows.</p> <ul> <li>Integrate multi-modal methods such as LayoutXLM, VI-LayoutXLM, and PP-OCR inference engine.</li> <li>Supports Semantic Entity Recognition (SER) and Relation Extraction (RE) tasks based on multimodal methods. Based on the SER task, the text recognition and classification in the image can be completed; based on the RE task, the relationship extraction of the text content in the image can be completed, such as judging the problem pair (pair).</li> <li>Supports custom training for SER tasks and RE tasks.</li> <li>Supports end-to-end system prediction and evaluation of OCR+SER.</li> <li>Supports end-to-end system prediction of OCR+SER+RE.</li> <li>Support SER model export and inference using PaddleInference.</li> </ul>"},{"location":"en/ppstructure/model_train/train_kie.html#2-performance","title":"2. Performance","text":"<p>We evaluate the methods on the Chinese dataset of XFUND, and the performance is as follows</p> Model Backbone Task Config file Hmean Inference time (ms) Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% 15.49 trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% 19.49 trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% 15.49 trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% 19.49 trained model <ul> <li>Note\uff1aInference environment\uff1aV100 GPU + cuda10.2 + cudnn8.1.1 + TensorRT 7.2.3.4\uff0ctested using fp16.</li> </ul> <p>For more KIE models in PaddleOCR, please refer to KIE model zoo.</p>"},{"location":"en/ppstructure/model_train/train_kie.html#3-visualization","title":"3. Visualization","text":"<p>There are two main solutions to the key information extraction task based on VI-LayoutXLM series model.</p> <p>(1) Text detection + text recognition + semantic entity recognition (SER)</p> <p>(2) Text detection + text recognition + semantic entity recognition (SER) + relationship extraction (RE)</p> <p>The following images are demo results of the SER and RE models. For more detailed introduction to the above solutions, please refer to KIE Guide.</p>"},{"location":"en/ppstructure/model_train/train_kie.html#31-ser","title":"3.1 SER","text":"<p>Demo results for SER task are as follows.</p> <p></p> <p></p> <p></p> <p></p> <p>Note: test pictures are from xfund dataset, invoice dataset and a composite ID card dataset.</p> <p>Boxes of different colors in the image represent different categories.</p> <p>The invoice and application form images have three categories: <code>request</code>, <code>answer</code> and <code>header</code>. The <code>question</code> and <code>answer</code> can be used to extract the relationship.</p> <p>For the ID card image, the model can directly identify the key information such as <code>name</code>, <code>gender</code>, <code>nationality</code>, so that the subsequent relationship extraction process is not required, and the key information extraction task can be completed using only one model.</p>"},{"location":"en/ppstructure/model_train/train_kie.html#32-re","title":"3.2 RE","text":"<p>Demo results for RE task are as follows.</p> <p></p> <p></p> <p></p> <p>Red boxes are questions, blue boxes are answers. The green lines means the two connected objects are a pair.</p>"},{"location":"en/ppstructure/model_train/train_kie.html#4-usage","title":"4. Usage","text":""},{"location":"en/ppstructure/model_train/train_kie.html#41-prepare-for-the-environment","title":"4.1 Prepare for the environment","text":"<p>Use the following command to install KIE dependencies.</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npip install -r requirements.txt\npip install -r ppstructure/kie/requirements.txt\n# \u200b\u5b89\u88c5\u200bPaddleOCR\u200b\u5f15\u64ce\u200b\u7528\u4e8e\u200b\u9884\u6d4b\u200b\npip install paddleocr -U\n</code></pre> <p>The visualized results of SER are saved in the <code>./output</code> folder by default. Examples of results are as follows.</p> <p></p>"},{"location":"en/ppstructure/model_train/train_kie.html#42-quick-start","title":"4.2 Quick start","text":"<p>Here we use XFUND dataset to quickly experience the SER model and RE model.</p>"},{"location":"en/ppstructure/model_train/train_kie.html#421-prepare-for-the-dataset","title":"4.2.1 Prepare for the dataset","text":"<pre><code>mkdir train_data\ncd train_data\n# download and uncompress the dataset\nwget https://paddleocr.bj.bcebos.com/ppstructure/dataset/XFUND.tar &amp;&amp; tar -xf XFUND.tar\ncd ..\n</code></pre>"},{"location":"en/ppstructure/model_train/train_kie.html#422-predict-images-using-the-trained-model","title":"4.2.2 Predict images using the trained model","text":"<p>Use the following command to download the models.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# download and uncompress the SER trained model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_pretrained.tar\n\n# download and uncompress the RE trained model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_pretrained.tar\n</code></pre> <p>If you want to use OCR engine to obtain end-to-end prediction results, you can use the following command to predict.</p> <pre><code># just predict using SER trained model\npython3 tools/infer_kie_token_ser.py \\\n  -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./ppstructure/docs/kie/input/zh_val_42.jpg\n\n# predict using SER and RE trained model at the same time\npython3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/image/zh_val_42.jpg \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n</code></pre> <p>The visual result images and the predicted text file will be saved in the <code>Global.save_res_path</code> directory.</p> <p>If you want to use a custom ocr model, you can set it through the following fields</p> <ul> <li><code>Global.kie_det_model_dir</code>: the detection inference model path</li> <li><code>Global.kie_rec_model_dir</code>: the recognition inference model path</li> </ul> <p>If you want to load the text detection and recognition results collected before, you can use the following command to predict.</p> <pre><code># just predict using SER trained model\npython3 tools/infer_kie_token_ser.py \\\n  -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False\n\n# predict using SER and RE trained model at the same time\npython3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n</code></pre>"},{"location":"en/ppstructure/model_train/train_kie.html#423-inference-using-paddleinference","title":"4.2.3 Inference using PaddleInference","text":"<p>Firstly, download the inference SER inference model.</p> <pre><code>mkdir inference\ncd inference\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_infer.tar\ncd ..\n</code></pre> <ul> <li>SER</li> </ul> <p>Use the following command for inference.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visual results and text file will be saved in directory <code>output</code>.</p> <ul> <li>RE</li> </ul> <p>Use the following command for inference.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm_xfund_infer \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_xfund_infer \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visual results and text file will be saved in directory <code>output</code>.</p> <p>If you want to use a custom ocr model, you can set it through the following fields</p> <ul> <li><code>--det_model_dir</code>: the detection inference model path</li> <li><code>--rec_model_dir</code>: the recognition inference model path</li> </ul>"},{"location":"en/ppstructure/model_train/train_kie.html#43-more","title":"4.3 More","text":"<p>For training, evaluation and inference tutorial for KIE models, please refer to KIE doc.</p> <p>For training, evaluation and inference tutorial for text detection models, please refer to text detection doc.</p> <p>For training, evaluation and inference tutorial for text recognition models, please refer to text recognition doc.</p> <p>To complete the key information extraction task in your own scenario from data preparation to model selection, please refer to: Guide to End-to-end KIE\u3002</p>"},{"location":"en/ppstructure/model_train/train_kie.html#5-reference","title":"5. Reference","text":"<ul> <li>LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding, https://arxiv.org/pdf/2104.08836.pdf</li> <li>microsoft/unilm/layoutxlm, https://github.com/microsoft/unilm/tree/master/layoutxlm</li> <li>XFUND dataset, https://github.com/doc-analysis/XFUND</li> </ul>"},{"location":"en/ppstructure/model_train/train_kie.html#6-license","title":"6. License","text":"<p>The content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</p>"},{"location":"en/ppstructure/model_train/train_layout.html","title":"Layout analysis","text":""},{"location":"en/ppstructure/model_train/train_layout.html#1-introduction","title":"1. Introduction","text":"<p>Layout analysis refers to the regional division of documents in the form of pictures and the positioning of key areas, such as text, title, table, picture, etc. The layout analysis algorithm is based on the lightweight model PP-picodet of PaddleDetection, including English layout analysis, Chinese layout analysis and table layout analysis models.  English layout analysis models can detect document layout elements such as text, title, table, figure, list. Chinese layout analysis models can detect document layout elements such as text, figure, figure caption, table, table caption, header, footer, reference, and equation. Table layout analysis models can detect table regions.</p> <p></p>"},{"location":"en/ppstructure/model_train/train_layout.html#2-quick-start","title":"2. Quick start","text":"<p>PP-Structure currently provides layout analysis models in Chinese, English and table documents. For the model link, see models_list. The whl package is also provided for quick use, see quickstart for details.</p>"},{"location":"en/ppstructure/model_train/train_layout.html#3-install","title":"3. Install","text":""},{"location":"en/ppstructure/model_train/train_layout.html#31-install-paddlepaddle","title":"3.1. Install PaddlePaddle","text":"<ul> <li>\uff081) Install PaddlePaddle</li> </ul> <pre><code>python3 -m pip install --upgrade pip\n\n# GPU Install\npython3 -m pip install \"paddlepaddle-gpu&gt;=2.3\" -i https://mirror.baidu.com/pypi/simple\n\n# CPU Install\npython3 -m pip install \"paddlepaddle&gt;=2.3\" -i https://mirror.baidu.com/pypi/simple\n</code></pre> <p>For more requirements, please refer to the instructions in the Install file\u3002</p>"},{"location":"en/ppstructure/model_train/train_layout.html#32-install-paddledetection","title":"3.2. Install PaddleDetection","text":"<ul> <li>\uff081\uff09Download PaddleDetection Source code</li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/PaddleDetection.git\n</code></pre> <ul> <li>\uff082\uff09Install third-party libraries</li> </ul> <pre><code>cd PaddleDetection\npython3 -m pip install -r requirements.txt\n</code></pre>"},{"location":"en/ppstructure/model_train/train_layout.html#4-data-preparation","title":"4. Data preparation","text":"<p>If you want to experience the prediction process directly, you can skip data preparation and download the pre-training model.</p>"},{"location":"en/ppstructure/model_train/train_layout.html#41-english-data-set","title":"4.1. English data set","text":"<p>Download document analysis data set PubLayNet\uff08Dataset 96G\uff09\uff0ccontains 5 classes\uff1a<code>{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}</code></p> <pre><code># Download data\nwget https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz\n# Decompress data\ntar -xvf publaynet.tar.gz\n</code></pre> <p>Uncompressed directory structure\uff1a</p> <pre><code>|-publaynet\n  |- test\n     |- PMC1277013_00004.jpg\n     |- PMC1291385_00002.jpg\n     | ...\n  |- train.json\n  |- train\n     |- PMC1291385_00002.jpg\n     |- PMC1277013_00004.jpg\n     | ...\n  |- val.json\n  |- val\n     |- PMC538274_00004.jpg\n     |- PMC539300_00004.jpg\n     | ...\n</code></pre> <p>data distribution\uff1a</p> File or Folder Description num <code>train/</code> Training set pictures 335,703 <code>val/</code> Verification set pictures 11,245 <code>test/</code> Test set pictures 11,405 <code>train.json</code> Training set annotation files - <code>val.json</code> Validation set dimension files - <p>Data Annotation</p> <p>The JSON file contains the annotations of all images, and the data is stored in a dictionary nested manner.Contains the following keys\uff1a</p> <ul> <li> <p>info\uff0crepresents the dimension file info\u3002</p> </li> <li> <p>licenses\uff0crepresents the dimension file licenses\u3002</p> </li> <li> <p>images\uff0crepresents the list of image information in the annotation file\uff0ceach element is the information of an image\u3002The information of one of the images is as follows:</p> </li> </ul> <pre><code>{\n    'file_name': 'PMC4055390_00006.jpg',    # file_name\n    'height': 601,                      # image height\n    'width': 792,                       # image width\n    'id': 341427                        # image id\n}\n</code></pre> <ul> <li>annotations\uff0c represents the list of annotation information of the target object in the annotation file\uff0ceach element is the annotation information of a target object\u3002The following is the annotation information of one of the target objects:</li> </ul> <pre><code>{\n\n    'segmentation':             # Segmentation annotation of objects\n    'area': 60518.099043117836, # Area of object\n    'iscrowd': 0,               # iscrowd\n    'image_id': 341427,         # image id\n    'bbox': [50.58, 490.86, 240.15, 252.16], # bbox [x1,y1,w,h]\n    'category_id': 1,           # category_id\n    'id': 3322348               # image id\n}\n</code></pre>"},{"location":"en/ppstructure/model_train/train_layout.html#42-more-datasets","title":"4.2. More datasets","text":"<p>We provide CDLA(Chinese layout analysis), TableBank(Table layout analysis)etc. data set download links\uff0cprocess to the JSON format of the above annotation file\uff0cthat is, the training can be conducted in the same way\u3002</p> dataset \u200b\u7b80\u4ecb\u200b cTDaR2019_cTDaR For form detection (TRACKA) and form identification (TRACKB).Image types include historical data sets (beginning with cTDaR_t0, such as CTDAR_T00872.jpg) and modern data sets (beginning with cTDaR_t1, CTDAR_T10482.jpg). IIIT-AR-13K Data sets constructed by manually annotating figures or pages from publicly available annual reports, containing 5 categories:table, figure, natural image, logo, and signature. TableBank For table detection and recognition of large datasets, including Word and Latex document formats CDLA Chinese document layout analysis data set, for Chinese literature (paper) scenarios, including 10 categories:Text, Title, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation DocBank Large-scale dataset (500K document pages) constructed using weakly supervised methods for document layout analysis, containing 12 categories:Author, Caption, Date, Equation, Figure, Footer, List, Paragraph, Reference, Section, Table, Title"},{"location":"en/ppstructure/model_train/train_layout.html#5-start-training","title":"5. Start training","text":"<p>Training scripts, evaluation scripts, and prediction scripts are provided, and the PubLayNet pre-training model is used as an example in this section.</p> <p>If you do not want training and directly experience the following process of model evaluation, prediction, motion to static, and inference, you can download the provided pre-trained model (PubLayNet dataset) and skip this part.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# Download PubLayNet pre-training model\uff08Direct experience model evaluates, predicts, and turns static\uff09\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout.pdparams\n# Download the PubLaynet inference model\uff08Direct experience model reasoning\uff09\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\n</code></pre> <p>If the test image is Chinese, the pre-trained model of Chinese CDLA dataset can be downloaded to identify 10 types of document regions\uff1aTable, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation\uff0cDownload the training model and inference model of Model 'picodet_lcnet_x1_0_fgd_layout_cdla' in layout analysis model\u3002If only the table area in the image is detected, you can download the pre-trained model of the table dataset, and download the training model and inference model of the 'picodet_LCnet_x1_0_FGd_layout_table' model in Layout Analysis model</p>"},{"location":"en/ppstructure/model_train/train_layout.html#51-train","title":"5.1. Train","text":"<p>Start training with the PaddleDetection layout analysis profile</p> <ul> <li>Modify Profile</li> </ul> <p>If you want to train your own data set, you need to modify the data configuration and the number of categories in the configuration file.</p> <p>Using 'configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml' as an example, the change is as follows:</p> <pre><code>metric: COCO\n# Number of categories\nnum_classes: 5\n\nTrainDataset:\n  !COCODataSet\n    # Modify to your own training data directory\n    image_dir: train\n    # Modify to your own training data label file\n    anno_path: train.json\n    # Modify to your own training data root directory\n    dataset_dir: /root/publaynet/\n    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']\n\nEvalDataset:\n  !COCODataSet\n    # Modify to your own validation data directory\n    image_dir: val\n    # Modify to your own validation data label file\n    anno_path: val.json\n    # Modify to your own validation data root\n    dataset_dir: /root/publaynet/\n\nTestDataset:\n  !ImageFolder\n    # Modify to your own test data label file\n    anno_path: /root/publaynet/val.json\n</code></pre> <ul> <li>Start training. During training, PP picodet pre training model will be downloaded by default. There is no need to download in advance.</li> </ul> <pre><code># GPU training supports single-card and multi-card training\n# The training log is automatically saved to the log directory\n\n# Single card training\nexport CUDA_VISIBLE_DEVICES=0\npython3 tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --eval\n\n# Multi-card training, with the -- GPUS parameter specifying the card number\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --eval\n</code></pre> <p>Attention\uff1aIf the video memory is out during training, adjust Batch_size in TrainReader and base_LR in LearningRate. The published config is obtained by 8-card training. If the number of GPU cards is changed to 1, then the base_LR needs to be reduced by 8 times.</p> <p>After starting training normally, you will see the following log output:</p> <pre><code>[08/15 04:02:30] ppdet.utils.checkpoint INFO: Finish loading model weights: /root/.cache/paddle/weights/LCNet_x1_0_pretrained.pdparams\n[08/15 04:02:46] ppdet.engine INFO: Epoch: [0] [   0/1929] learning_rate: 0.040000 loss_vfl: 1.216707 loss_bbox: 1.142163 loss_dfl: 0.544196 loss: 2.903065 eta: 17 days, 13:50:26 batch_cost: 15.7452 data_cost: 2.9112 ips: 1.5243 images/s\n[08/15 04:03:19] ppdet.engine INFO: Epoch: [0] [  20/1929] learning_rate: 0.064000 loss_vfl: 1.180627 loss_bbox: 0.939552 loss_dfl: 0.442436 loss: 2.628206 eta: 2 days, 12:18:53 batch_cost: 1.5770 data_cost: 0.0008 ips: 15.2184 images/s\n[08/15 04:03:47] ppdet.engine INFO: Epoch: [0] [  40/1929] learning_rate: 0.088000 loss_vfl: 0.543321 loss_bbox: 1.071401 loss_dfl: 0.457817 loss: 2.057003 eta: 2 days, 0:07:03 batch_cost: 1.3190 data_cost: 0.0007 ips: 18.1954 images/s\n[08/15 04:04:12] ppdet.engine INFO: Epoch: [0] [  60/1929] learning_rate: 0.112000 loss_vfl: 0.630989 loss_bbox: 0.859183 loss_dfl: 0.384702 loss: 1.883143 eta: 1 day, 19:01:29 batch_cost: 1.2177 data_cost: 0.0006 ips: 19.7087 images/s\n</code></pre> <ul> <li><code>--eval</code> indicates that the best model is saved as <code>output/picodet_lcnet_x1_0_layout/best_accuracy</code>  by default during the evaluation process \u3002</li> </ul> <p>Note that the configuration file for prediction / evaluation must be consistent with the training.</p>"},{"location":"en/ppstructure/model_train/train_layout.html#52-fgd-distillation-training","title":"5.2. FGD Distillation Training","text":"<p>PaddleDetection supports FGD-based Focal and Global Knowledge Distillation for Detectors  The training process of the target detection model of distillation, FGD distillation is divided into two parts <code>Focal</code> and <code>Global</code>.     <code>Focal</code> Distillation separates the foreground and background of the image, allowing the student model to focus on the key pixels of the foreground and background features of the teacher model respectively;<code>Global</code>Distillation section reconstructs the relationships between different pixels and transfers them from the teacher to the student to compensate for the global information lost in <code>Focal</code>Distillation.</p> <p>Change the dataset and modify the data configuration and number of categories in the [TODO] configuration, referring to 4.1. Start training:</p> <pre><code># Single Card Training\nexport CUDA_VISIBLE_DEVICES=0\npython3 tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    --eval\n</code></pre> <ul> <li><code>-c</code>: Specify the model configuration file.</li> <li><code>--slim_config</code>:  Specify the compression policy profile.</li> </ul>"},{"location":"en/ppstructure/model_train/train_layout.html#6-model-evaluation-and-prediction","title":"6. Model evaluation and prediction","text":""},{"location":"en/ppstructure/model_train/train_layout.html#61-indicator-evaluation","title":"6.1. Indicator evaluation","text":"<p>Model parameters in training are saved by default in <code>output/picodet_ Lcnet_ X1_ 0_ Under the layout</code> directory. When evaluating indicators, you need to set <code>weights</code> to point to the saved parameter file.Assessment datasets can be accessed via <code>configs/picodet/legacy_ Model/application/layout_ Analysis/picodet_ Lcnet_ X1_ 0_ Layout. Yml</code> . Modify <code>EvalDataset</code>  : <code>img_dir</code>,<code>anno_ Path</code>and<code>dataset_dir</code> setting.</p> <pre><code># GPU evaluation, weights as weights to be measured\npython3 tools/eval.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights=./output/picodet_lcnet_x1_0_layout/best_model\n</code></pre> <p>The following information will be printed out, such as mAP, AP0.5, etc.</p> <pre><code> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.935\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.956\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.404\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.782\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.969\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.539\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.938\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.949\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.495\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.818\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.978\n[08/15 07:07:09] ppdet.engine INFO: Total sample number: 11245, averge FPS: 24.405059207157436\n[08/15 07:07:09] ppdet.engine INFO: Best test bbox ap is 0.935.\n</code></pre> <p>If you use the provided pre-training model for evaluation or the FGD distillation training model, replace the <code>weights</code> model path and execute the following command for evaluation:</p> <pre><code>python3 tools/eval.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights=output/picodet_lcnet_x2_5_layout/best_model\n</code></pre> <ul> <li><code>-c</code>: Specify the model configuration file.</li> <li><code>--slim_config</code>:  Specify the distillation policy profile.</li> <li><code>-o weights</code>: Specify the model path trained by the distillation algorithm.</li> </ul>"},{"location":"en/ppstructure/model_train/train_layout.html#62-test-layout-analysis-results","title":"6.2. Test Layout Analysis Results","text":"<p>The profile predicted to be used must be consistent with the training, for example, if you pass <code>python3 tools/train'. Py-c configs/picodet/legacy_ Model/application/layout_ Analysis/picodet_ Lcnet_ X1_ 0_ Layout. Yml</code> completed the training process for the model.</p> <p>With  trained PaddleDetection model, you can use the following commands to make model predictions.</p> <pre><code>python3 tools/infer.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights='output/picodet_lcnet_x1_0_layout/best_model.pdparams' \\\n    --infer_img='docs/images/layout.jpg' \\\n    --output_dir=output_dir/ \\\n    --draw_threshold=0.5\n</code></pre> <ul> <li><code>--infer_img</code>:  Reasoning for a single picture can also be done via <code>--infer_ Dir</code>Inform all pictures in the file.</li> <li><code>--output_dir</code>:  Specify the path to save the visualization results.</li> <li><code>--draw_threshold</code>:Specify the NMS threshold for drawing the result box.</li> </ul> <p>If you use the provided pre-training model for prediction or the FGD distillation training model, change the <code>weights</code> model path and execute the following command to make the prediction:</p> <pre><code>python3 tools/infer.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights='output/picodet_lcnet_x2_5_layout/best_model.pdparams' \\\n    --infer_img='docs/images/layout.jpg' \\\n    --output_dir=output_dir/ \\\n    --draw_threshold=0.5\n</code></pre>"},{"location":"en/ppstructure/model_train/train_layout.html#7-model-export-and-inference","title":"7. Model Export and Inference","text":""},{"location":"en/ppstructure/model_train/train_layout.html#71-model-export","title":"7.1 Model Export","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>Layout analysis model to inference model steps are as follows\uff1a</p> <pre><code>python3 tools/export_model.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights=output/picodet_lcnet_x1_0_layout/best_model \\\n    --output_dir=output_inference/\n</code></pre> <ul> <li>If no post-export processing is required, specify\uff1a<code>-o export.benchmark=True</code>\uff08If -o already exists, delete -o here\uff09</li> <li>If you do not need to export NMS, specify\uff1a<code>-o export.nms=False</code></li> </ul> <p>After successful conversion, there are three files in the directory:</p> <pre><code>output_inference/picodet_lcnet_x1_0_layout/\n    \u251c\u2500\u2500 model.pdiparams         # inference Parameter file for model\n    \u251c\u2500\u2500 model.pdiparams.info    # inference Model parameter information, ignorable\n    \u2514\u2500\u2500 model.pdmodel           # inference Model Structure File for Model\n</code></pre> <p>If you change the <code>weights</code> model path using the provided pre-training model to the Inference model, or using the FGD distillation training model, the model to inference model steps are as follows:</p> <pre><code>python3 tools/export_model.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights=./output/picodet_lcnet_x2_5_layout/best_model \\\n    --output_dir=output_inference/\n</code></pre>"},{"location":"en/ppstructure/model_train/train_layout.html#72-model-inference","title":"7.2 Model inference","text":"<p>Replace model_with the provided inference training model for inference or the FGD distillation training <code>model_dir</code>Inference model path, execute the following commands for inference:</p> <pre><code>python3 deploy/python/infer.py \\\n    --model_dir=output_inference/picodet_lcnet_x1_0_layout/ \\\n    --image_file=docs/images/layout.jpg \\\n    --device=CPU\n</code></pre> <ul> <li>--device\uff1aSpecify the GPU or CPU device</li> </ul> <p>When model inference is complete, you will see the following log output:</p> <pre><code>------------------------------------------\n-----------  Model Configuration -----------\nModel Arch: PicoDet\nTransform Order:\n--transform op: Resize\n--transform op: NormalizeImage\n--transform op: Permute\n--transform op: PadStride\n--------------------------------------------\nclass_id:0, confidence:0.9921, left_top:[20.18,35.66],right_bottom:[341.58,600.99]\nclass_id:0, confidence:0.9914, left_top:[19.77,611.42],right_bottom:[341.48,901.82]\nclass_id:0, confidence:0.9904, left_top:[369.36,375.10],right_bottom:[691.29,600.59]\nclass_id:0, confidence:0.9835, left_top:[369.60,608.60],right_bottom:[691.38,736.72]\nclass_id:0, confidence:0.9830, left_top:[369.58,805.38],right_bottom:[690.97,901.80]\nclass_id:0, confidence:0.9716, left_top:[383.68,271.44],right_bottom:[688.93,335.39]\nclass_id:0, confidence:0.9452, left_top:[370.82,34.48],right_bottom:[688.10,63.54]\nclass_id:1, confidence:0.8712, left_top:[370.84,771.03],right_bottom:[519.30,789.13]\nclass_id:3, confidence:0.9856, left_top:[371.28,67.85],right_bottom:[685.73,267.72]\nsave result to: output/layout.jpg\nTest iter 0\n------------------ Inference Time Info ----------------------\ntotal_time(ms): 2196.0, img_num: 1\naverage latency time(ms): 2196.00, QPS: 0.455373\npreprocess_time(ms): 2172.50, inference_time(ms): 11.90, postprocess_time(ms): 11.60\n</code></pre> <ul> <li>Model\uff1amodel structure</li> <li>Transform Order\uff1aPreprocessing operation</li> <li>class_id, confidence, left_top, right_bottom\uff1aIndicates category id, confidence level, upper left coordinate, lower right coordinate, respectively</li> <li>save result to\uff1aSave path of visual layout analysis results, default save to ./output folder</li> <li>inference time info\uff1aInference time, where preprocess_time represents the preprocessing time, Inference_time represents the model prediction time, and postprocess_time represents the post-processing time</li> </ul> <p>The result of visualization layout is shown in the following figure</p> <p></p>"},{"location":"en/ppstructure/model_train/train_layout.html#citations","title":"Citations","text":"<pre><code>@inproceedings{zhong2019publaynet,\n  title={PubLayNet: largest dataset ever for document layout analysis},\n  author={Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},\n  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},\n  year={2019},\n  volume={},\n  number={},\n  pages={1015-1022},\n  doi={10.1109/ICDAR.2019.00166},\n  ISSN={1520-5363},\n  month={Sep.},\n  organization={IEEE}\n}\n\n@inproceedings{yang2022focal,\n  title={Focal and global knowledge distillation for detectors},\n  author={Yang, Zhendong and Li, Zhe and Jiang, Xiaohu and Gong, Yuan and Yuan, Zehuan and Zhao, Danpei and Yuan, Chun},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={4643--4652},\n  year={2022}\n}\n</code></pre>"},{"location":"en/ppstructure/model_train/train_table.html","title":"Table Recognition","text":""},{"location":"en/ppstructure/model_train/train_table.html#1-pipeline","title":"1. pipeline","text":"<p>The table recognition mainly contains three models</p> <ol> <li>Single line text detection-DB</li> <li>Single line text recognition-CRNN</li> <li>Table structure and cell coordinate prediction-SLANet</li> </ol> <p>The table recognition flow chart is as follows</p> <p></p> <ol> <li>The coordinates of single-line text is detected by DB model, and then sends it to the recognition model to get the recognition result.</li> <li>The table structure and cell coordinates is predicted by SLANet model.</li> <li>The recognition result of the cell is combined by the coordinates, recognition result of the single line and the coordinates of the cell.</li> <li>The cell recognition result and the table structure together construct the html string of the table.</li> </ol>"},{"location":"en/ppstructure/model_train/train_table.html#2-performance","title":"2. Performance","text":"<p>We evaluated the algorithm on the PubTabNet<sup>[1]</sup> eval dataset, and the performance is as follows:</p> Method Acc TEDS(Tree-Edit-Distance-based Similarity) Speed EDD<sup>[2]</sup> x 88.30% x TableRec-RARE(ours) 71.73% 93.88% 779ms SLANet(ours) 76.31% 95.89% 766ms <p>The performance indicators are explained as follows:</p> <ul> <li>Acc: The accuracy of the table structure in each image, a wrong token is considered an error.</li> <li>TEDS: The accuracy of the model's restoration of table information. This indicator evaluates not only the table structure, but also the text content in the table.</li> <li>Speed: The inference speed of a single image when the model runs on the CPU machine and MKL is enabled.</li> </ul>"},{"location":"en/ppstructure/model_train/train_table.html#3-result","title":"3. Result","text":""},{"location":"en/ppstructure/model_train/train_table.html#4-how-to-use","title":"4. How to use","text":""},{"location":"en/ppstructure/model_train/train_table.html#41-quick-start","title":"4.1 Quick start","text":"<p>PP-Structure currently provides table recognition models in both Chinese and English. For the model link, see models_list. The whl package is also provided for quick use, see quickstart for details.</p> <p>The following takes the Chinese table recognition model as an example to introduce how to recognize a table.</p> <p>Use the following commands to quickly complete the identification of a table.</p> <pre><code>cd PaddleOCR/ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the PP-OCRv3 text detection model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_det_infer.tar\n# Download the PP-OCRv3 text recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar &amp;&amp; tar xf ch_PP-OCRv3_rec_infer.tar\n# Download the PP-StructureV2 form recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/ch_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n# run\npython3.7 table/predict_table.py \\\n    --det_model_dir=inference/ch_PP-OCRv3_det_infer \\\n    --rec_model_dir=inference/ch_PP-OCRv3_rec_infer  \\\n    --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n    --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n    --image_dir=docs/table/table.jpg \\\n    --output=../output/table\n</code></pre> <p>After the operation is completed, the excel table of each image will be saved to the directory specified by the output field, and an html file will be produced in the directory to visually view the cell coordinates and the recognized table.</p> <p>NOTE</p> <ol> <li>If you want to use the English table recognition model, you need to download the English text detection and recognition model and the English table recognition model in models_list, and replace <code>table_structure_dict_ch.txt</code> with <code>table_structure_dict.txt</code>.</li> <li>To use the TableRec-RARE model, you need to replace <code>table_structure_dict_ch.txt</code> with <code>table_structure_dict.txt</code>, and add parameter <code>--merge_no_span_structure=False</code></li> </ol>"},{"location":"en/ppstructure/model_train/train_table.html#42-training-evaluation-and-inference","title":"4.2 Training, Evaluation and Inference","text":"<p>The training, evaluation and inference process of the text detection model can be referred to detection</p> <p>The training, evaluation and inference process of the text recognition model can be referred to recognition</p> <p>The training, evaluation and inference process of the table recognition model can be referred to table_recognition</p>"},{"location":"en/ppstructure/model_train/train_table.html#43-calculate-teds","title":"4.3 Calculate TEDS","text":"<p>The table uses TEDS(Tree-Edit-Distance-based Similarity) as the evaluation metric of the model. Before the model evaluation, the three models in the pipeline need to be exported as inference models (we have provided them), and the gt for evaluation needs to be prepared. Examples of gt are as follows:</p> <pre><code>PMC5755158_010_01.png    &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Weaning&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Week 15&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Off-test&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Weaning&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Week 15&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;0.17 \u00b1 0.08&lt;/td&gt;&lt;td&gt;0.16 \u00b1 0.03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Off-test&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;0.80 \u00b1 0.24&lt;/td&gt;&lt;td&gt;0.19 \u00b1 0.09&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>Each line in gt consists of the file name and the html string of the table. The file name and the html string of the table are separated by <code>\\t</code>.</p> <p>You can also use the following command to generate an evaluation gt file from the annotation file:</p> <pre><code>python3 ppstructure/table/convert_label2html.py --ori_gt_path /path/to/your_label_file --save_path /path/to/save_file\n</code></pre> <p>Use the following command to evaluate. After the evaluation is completed, the teds indicator will be output.</p> <pre><code>python3 table/eval_table.py \\\n    --det_model_dir=path/to/det_model_dir \\\n    --rec_model_dir=path/to/rec_model_dir \\\n    --table_model_dir=path/to/table_model_dir \\\n    --image_dir=docs/table/table.jpg \\\n    --rec_char_dict_path=../ppocr/utils/dict/table_dict.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --det_limit_side_len=736 \\\n    --det_limit_type=min \\\n    --gt_path=path/to/gt.txt\n</code></pre> <p>Evaluate on the PubLatNet dataset using the English model</p> <pre><code>cd PaddleOCR/ppstructure\n# Download the model\nmkdir inference &amp;&amp; cd inference\n# Download the text detection model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_det_infer.tar &amp;&amp; tar xf en_ppocr_mobile_v2.0_table_det_infer.tar\n# Download the text recognition model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_rec_infer.tar &amp;&amp; tar xf en_ppocr_mobile_v2.0_table_rec_infer.tar\n# Download the table recognition model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n\npython3 table/eval_table.py \\\n    --det_model_dir=inference/en_ppocr_mobile_v2.0_table_det_infer \\\n    --rec_model_dir=inference/en_ppocr_mobile_v2.0_table_rec_infer \\\n    --table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n    --image_dir=train_data/table/pubtabnet/val/ \\\n    --rec_char_dict_path=../ppocr/utils/dict/table_dict.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --det_limit_side_len=736 \\\n    --det_limit_type=min \\\n    --rec_image_shape=3,32,320 \\\n    --gt_path=path/to/gt.txt\n</code></pre> <p>output is</p> <pre><code>teds: 95.89\n</code></pre>"},{"location":"en/ppstructure/model_train/train_table.html#5-reference","title":"5. Reference","text":"<ol> <li>https://github.com/ibm-aur-nlp/PubTabNet</li> <li>https://arxiv.org/pdf/1911.10683</li> </ol>"},{"location":"en/ppstructure/model_train/training.html","title":"Model Training","text":"<p>This article will introduce the basic concepts that is necessary for model training and tuning.</p> <p>At the same time, it will briefly introduce the structure of the training data and how to prepare the data to fine-tune model in vertical scenes.</p>"},{"location":"en/ppstructure/model_train/training.html#1-yml-configuration","title":"1. Yml Configuration","text":"<p>The PaddleOCR uses configuration files to control network training and evaluation parameters. In the configuration file, you can set the model, optimizer, loss function, and pre- and post-processing parameters of the model. PaddleOCR reads these parameters from the configuration file, and then builds a complete training process to train the model. Fine-tuning can also be completed by modifying the parameters in the configuration file, which is simple and convenient.</p> <p>For the complete configuration file description, please refer to Configuration File</p>"},{"location":"en/ppstructure/model_train/training.html#2-basic-concepts","title":"2. Basic Concepts","text":"<p>During the model training process, some hyper-parameters can be manually specified to obtain the optimal result at the least cost. Different data volumes may require different hyper-parameters. When you want to fine-tune the model based on your own data, there are several parameter adjustment strategies for reference:</p>"},{"location":"en/ppstructure/model_train/training.html#21-learning-rate","title":"2.1 Learning Rate","text":"<p>The learning rate is one of the most important hyper-parameters for training neural networks. It represents the step length of the gradient moving towards the optimal solution of the loss function in each iteration. A variety of learning rate update strategies are provided by PaddleOCR, which can be specified in configuration files. For example,</p> <pre><code>Optimizer:\n  ...\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]\n    warmup_epoch: 5\n</code></pre> <p><code>Piecewise</code> stands for piece-wise constant attenuation. Different learning rates are specified in different learning stages, and the learning rate stay the same in each stage.</p> <p><code>warmup_epoch</code> means that in the first 5 epochs, the learning rate will be increased gradually from 0 to base_lr. For all strategies, please refer to the code learning_rate.py.</p>"},{"location":"en/ppstructure/model_train/training.html#22-regularization","title":"2.2 Regularization","text":"<p>Regularization can effectively avoid algorithm over-fitting. PaddleOCR provides L1 and L2 regularization methods. L1 and L2 regularization are the most widely used regularization methods. L1 regularization adds a regularization term to the objective function to reduce the sum of absolute values of the parameters; while in L2 regularization, the purpose of adding a regularization term is to reduce the sum of squared parameters. The configuration method is as follows:</p> <pre><code>Optimizer:\n  ...\n  regularizer:\n    name: L2\n    factor: 2.0e-05\n</code></pre>"},{"location":"en/ppstructure/model_train/training.html#23-evaluation-indicators","title":"2.3 Evaluation Indicators","text":"<p>(1) Detection stage: First, evaluate according to the IOU of the detection frame and the labeled frame. If the IOU is greater than a certain threshold, it is judged that the detection is accurate. Here, the detection frame and the label frame are different from the general general target detection frame, and they are represented by polygons. Detection accuracy: the percentage of the correct detection frame number in all detection frames is mainly used to judge the detection index. Detection recall rate: the percentage of correct detection frames in all marked frames, which is mainly an indicator of missed detection.</p> <p>(2) Recognition stage: Character recognition accuracy, that is, the ratio of correctly recognized text lines to the number of marked text lines. Only the entire line of text recognition pairs can be regarded as correct recognition.</p> <p>(3) End-to-end statistics: End-to-end recall rate: accurately detect and correctly identify the proportion of text lines in all labeled text lines; End-to-end accuracy rate: accurately detect and correctly identify the number of text lines in the detected text lines The standard for accurate detection is that the IOU of the detection box and the labeled box is greater than a certain threshold, and the text in the correctly identified detection box is the same as the labeled text.</p>"},{"location":"en/ppstructure/model_train/training.html#3-data-and-vertical-scenes","title":"3. Data and Vertical Scenes","text":""},{"location":"en/ppstructure/model_train/training.html#31-training-data","title":"3.1 Training Data","text":"<p>The current open source models, data sets and magnitudes are as follows:</p> <ul> <li>Detection:</li> <li>English data set, ICDAR2015</li> <li> <p>Chinese data set, LSVT street view data set training data 3w pictures</p> </li> <li> <p>Identification:</p> </li> <li>English data set, MJSynth and SynthText synthetic data, the data volume is tens of millions.</li> <li>Chinese data set, LSVT street view data set crops the image according to the truth value, and performs position calibration, a total of 30w images. In addition, based on the LSVT corpus, 500w of synthesized data.</li> <li>Small language data set, using different corpora and fonts, respectively generated 100w synthetic data set, and using ICDAR-MLT as the verification set.</li> </ul> <p>Among them, the public data sets are all open source, users can search and download by themselves, or refer to Chinese data set, synthetic data is not open source, users can use open source synthesis tools to synthesize by themselves. Synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator etc.</p>"},{"location":"en/ppstructure/model_train/training.html#32-vertical-scene","title":"3.2 Vertical Scene","text":"<p>PaddleOCR mainly focuses on general OCR. If you have vertical requirements, you can use PaddleOCR + vertical data to train yourself; If there is a lack of labeled data, or if you do not want to invest in research and development costs, it is recommended to directly call the open API, which covers some of the more common vertical categories.</p>"},{"location":"en/ppstructure/model_train/training.html#33-build-your-own-dataset","title":"3.3 Build Your Own Dataset","text":"<p>There are several experiences for reference when constructing the data set:</p> <p>(1) The amount of data in the training set:</p> <p>a. The data required for detection is relatively small. For Fine-tune based on the PaddleOCR model, 500 sheets are generally required to achieve good results.</p> <p>b. Recognition is divided into English and Chinese. Generally, English scenarios require hundreds of thousands of data to achieve good results, while Chinese requires several million or more.</p> <p>(2) When the amount of training data is small, you can try the following three ways to get more data:</p> <p>a. Manually collect more training data, the most direct and effective way.</p> <p>b. Basic image processing or transformation based on PIL and opencv. For example, the three modules of ImageFont, Image, ImageDraw in PIL write text into the background, opencv's rotating affine transformation, Gaussian filtering and so on.</p> <p>c. Use data generation algorithms to synthesize data, such as algorithms such as pix2pix.</p>"},{"location":"en/ppstructure/model_train/training.html#4-faq","title":"4. FAQ","text":"<p>Q: How to choose a suitable network input shape when training CRNN recognition?</p> <pre><code>A: The general height is 32, the longest width is selected, there are two methods:\n\n(1) Calculate the aspect ratio distribution of training sample images. The selection of the maximum aspect ratio considers 80% of the training samples.\n\n(2) Count the number of texts in training samples. The selection of the longest number of characters considers the training sample that satisfies 80%. Then the aspect ratio of Chinese characters is approximately considered to be 1, and that of English is 3:1, and the longest width is estimated.\n</code></pre> <p>Q: During the recognition training, the accuracy of the training set has reached 90, but the accuracy of the verification set has been kept at 70, what should I do?</p> <pre><code>A: If the accuracy of the training set is 90 and the test set is more than 70, it should be over-fitting. There are two methods to try:\n\n(1) Add more augmentation methods or increase the [probability] of augmented prob (https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppocr/data/imaug/rec_img_aug.py#L341), The default is 0.4.\n\n(2) Increase the [l2 dcay value] of the system (https://github.com/PaddlePaddle/PaddleOCR/blob/a501603d54ff5513fc4fc760319472e59da25424/configs/rec/ch_ppocr_v1.1/rec_chinese_lite_train_v1.1.yml#L47)\n</code></pre> <p>Q: When the recognition model is trained, loss can drop normally, but acc is always 0</p> <pre><code>A: It is normal for the acc to be 0 at the beginning of the recognition model training, and the indicator will come up after a longer training period.\n</code></pre> <p>Click the following links for detailed training tutorial:</p> <ul> <li>text detection model training</li> <li>text recognition model training</li> <li>text direction classification model training</li> </ul>"}]}